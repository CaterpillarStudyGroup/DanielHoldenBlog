<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>ImportantArticles</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The note of Important Articles">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/pagetoc.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="index.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="ReadPapers.html"><strong aria-hidden="true">2.</strong> å¦‚ä½•é«˜æ•ˆè¯»è®ºæ–‡</a></li><li class="chapter-item expanded affix "><li class="part-title">CVPR Tutorial - Denoising Diffusion Models: A Generative Learning Big Bang</li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/Introduction.html"><strong aria-hidden="true">3.</strong> Introduction</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">4.</strong> Fundamentals</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="diffusion-tutorial-part/Fundamentals/DenoisingDiffusionProbabilisticModels.html"><strong aria-hidden="true">4.1.</strong> Denoising Diffusion Probabilistic Models</a></li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/Fundamentals/Score-basedGenerativeModelingwithDifferentialEquations.html"><strong aria-hidden="true">4.2.</strong> Score-based Generative Modeling with Differential Equations</a></li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/Fundamentals/AcceleratedSampling.html"><strong aria-hidden="true">4.3.</strong> Accelerated Sampling</a></li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/Fundamentals/ConditionalGenerationandGuidance.html"><strong aria-hidden="true">4.4.</strong> Conditional Generation and Guidance</a></li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/Fundamentals/Summary.html"><strong aria-hidden="true">4.5.</strong> Summary</a></li></ol></li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/Architecture.html"><strong aria-hidden="true">5.</strong> T2I åŸºæ¨¡å‹</a></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.</strong> Image Applications Based on åŸºæ¨¡å‹</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="diffusion-tutorial-part/ApplicationOnImage/ImageEditing.html"><strong aria-hidden="true">6.1.</strong> å›¾åƒç¼–è¾‘</a></li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/Applicationsonotherdomains/InverseProblems.html"><strong aria-hidden="true">6.2.</strong> å›¾åƒå»å™ª/å›¾åƒè¶…åˆ†/å›¾åƒè¡¥å…¨</a></li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/ApplicationOnImage/LargeContents.html"><strong aria-hidden="true">6.3.</strong> å¤§å›¾ç”Ÿæˆ</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.</strong> 3D Applications Based on Diffusion</div></li><li><ol class="section"><li class="chapter-item expanded "><a href="diffusion-tutorial-part/ApplicationsOn3D/3D.html"><strong aria-hidden="true">7.1.</strong> 3Dè¡¨ç¤º</a></li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/ApplicationsOn3D/2Ddiffusionmodelsfor3Dgeneration.html"><strong aria-hidden="true">7.2.</strong> 3Dç”Ÿæˆ</a></li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/ApplicationsOn3D/Diffusionmodelsforviewsynthesis.html"><strong aria-hidden="true">7.3.</strong> æ–°è§†è§’åˆæˆ</a></li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/ApplicationsOn3D/3Dreconstruction.html"><strong aria-hidden="true">7.4.</strong> 3Dé‡å»º</a></li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/ApplicationsOn3D/Inverseproblems.html"><strong aria-hidden="true">7.5.</strong> 3Dç¼–è¾‘</a></li></ol></li><li class="chapter-item expanded "><a href="diffusion-tutorial-part/ApplicationsOn3D/Safetyandlimitationsofdiffusionmodels.html"><strong aria-hidden="true">8.</strong> Safety and limitations of diffusion models</a></li><li class="chapter-item expanded affix "><li class="part-title">Video Diffusion Models</li><li class="chapter-item expanded "><a href="VideoDiffusionModels/Introduction.html"><strong aria-hidden="true">9.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoGeneration/VideoGeneration.html"><strong aria-hidden="true">10.</strong> Video Generation</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoGeneration/Pioneeringearlyworks.html"><strong aria-hidden="true">10.1.</strong> é—­æºT2Vå¤§æ¨¡å‹</a></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoGeneration/Open-sourcebasemodels.html"><strong aria-hidden="true">10.2.</strong> å¼€æºT2VåŸºæ¨¡å‹</a></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoGeneration/WorksBasedOnT2I.html"><strong aria-hidden="true">10.3.</strong> Works Based on T2I åŸºæ¨¡å‹</a></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoGeneration/WorksBasedOnT2V.html"><strong aria-hidden="true">10.4.</strong> Works Based on T2V åŸºæ¨¡å‹</a></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoGeneration/Storyboard.html"><strong aria-hidden="true">10.5.</strong> Storyboard</a></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoGeneration/Longvideogeneration.html"><strong aria-hidden="true">10.6.</strong> Long video generation</a></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoGeneration/Multimodal-guidedgeneration.html"><strong aria-hidden="true">10.7.</strong> Multimodal-guided generation</a></li></ol></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoEditing.html"><strong aria-hidden="true">11.</strong> Video Editing</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoEditing/Tuning-based.html"><strong aria-hidden="true">11.1.</strong> Tuning-based</a></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoEditing/Training-free.html"><strong aria-hidden="true">11.2.</strong> Training-free</a></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoEditing/ControlledEdifng.html"><strong aria-hidden="true">11.3.</strong> Controlled Edifng</a></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoEditing/3D-Aware.html"><strong aria-hidden="true">11.4.</strong> 3D-Aware</a></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/VideoEditing/OtherGuidance.html"><strong aria-hidden="true">11.5.</strong> Other Guidance</a></li></ol></li><li class="chapter-item expanded "><a href="VideoDiffusionModels/EvaluationMetrics.html"><strong aria-hidden="true">12.</strong> è¯„ä»·æŒ‡æ ‡</a></li><li class="chapter-item expanded affix "><li class="part-title">Others</li><li class="chapter-item expanded "><a href="LargeMultimodalModelsNotesonCVPR2023Tutorial.html"><strong aria-hidden="true">13.</strong> Large Multimodal Models Notes on CVPR 2023 Tutorial</a></li><li class="chapter-item expanded "><a href="HPE_HMR_Summary.html"><strong aria-hidden="true">14.</strong> Deep Learning for 3D Human Pose Estimation and Mesh Recovery: A Survey</a></li><li class="chapter-item expanded "><a href="3D_Gaussian_Splatting.html"><strong aria-hidden="true">15.</strong> A Survey on 3D Gaussian Splatting</a></li><li class="chapter-item expanded "><a href="HumanMotionGenerationSummary.html"><strong aria-hidden="true">16.</strong> Human Motion Generation: A Survey</a></li><li class="chapter-item expanded "><a href="HumanVideoGeneration.html"><strong aria-hidden="true">17.</strong> Human Video Generation</a></li><li class="chapter-item expanded "><a href="æ•°æ®é›†.html"><strong aria-hidden="true">18.</strong> æ•°æ®é›†</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">ImportantArticles</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/CaterpillarStudyGroup/ImportantArticles" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <h1 id="reference"><a class="header" href="#reference">Reference</a></h1>
<ol>
<li>ä¸ªäººåšå®¢</li>
</ol>
<ul>
<li>Daniel Holden's blog: https://www.daniel-holden.com/page/all</li>
</ul>
<ol start="2">
<li>CVPR Tutorl</li>
</ol>
<ul>
<li>https://cvpr2023-tutorial-diffusion-models.github.io/</li>
</ul>
<ol start="3">
<li>Bç«™åˆ†äº«</li>
</ol>
<ul>
<li>https://sites.google.com/view/showlab/tutorial</li>
</ul>
<ol start="4">
<li>å¤§å­¦è¯¾ç¨‹</li>
</ol>
<h1 id="æœªå½’æ¡£è®ºæ–‡"><a class="header" href="#æœªå½’æ¡£è®ºæ–‡">æœªå½’æ¡£è®ºæ–‡</a></h1>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>39</td><td>2024</td><td>Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</td><td>æ–‡ç”Ÿ3D Mesh + æ–‡ç”Ÿæˆ3DåŠ¨ä½œ + é‡å®šå‘ = 3DåŠ¨ç‰©è¿åŠ¨åºåˆ—</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/39.html">link</a></td></tr>
<tr><td>35</td><td></td><td>MagicPony: Learning Articulated 3D Animals in the Wild</td><td>å›¾åƒç”Ÿæˆ3DåŠ¨ç‰©Meshå¹¶ç»‘å®šï¼Œå›¾åƒç”Ÿæˆ3DåŠ¨ä½œ</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/35.html">link</a></td></tr>
<tr><td>32</td><td></td><td>Artemis: Articulated Neural Pets with Appearance and Motion Synthesis</td><td>NGI åŠ¨ç‰©çš„é«˜åº¦é€¼çœŸæ¸²æŸ“</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/32.html">link</a></td></tr>
<tr><td>30</td><td>2024</td><td>CAT3D: Create Anything in 3D with Multi-View Diffusion Models</td><td>åŸºäºDiffusionçš„3Dé‡å»º</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/30.html">link</a></td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><p>è½¬è½½å‡ºå¤„ï¼šGAMES åœ¨çº¿æŠ¥å‘Š227</p>
<p><img src="./assets/ReadPaperSummary.PNG" alt="" /></p>
<h1 id="æ‰¾è®ºæ–‡"><a class="header" href="#æ‰¾è®ºæ–‡">æ‰¾è®ºæ–‡</a></h1>
<ol>
<li>æ‰¾æ–¹å‘</li>
</ol>
<ul>
<li>ä¸»æµä¼šè®®çš„paper session</li>
</ul>
<ol start="2">
<li>æ‰¾ç»¼è¿°ï¼Œç²¾è¯»</li>
</ol>
<ul>
<li>æœç´¢â€œæ–¹å‘â€ç›¸å…³çš„å…³é”®è¯</li>
</ul>
<ol start="3">
<li>æ‰¾ç»å…¸è®ºæ–‡ï¼Œç²¾è¯»</li>
</ol>
<ul>
<li>ç»¼è¿°é‡Œé¢è¢«highlightçš„è®ºæ–‡</li>
<li>å¼•ç”¨éå¸¸é«˜çš„è®ºæ–‡</li>
<li>è·å¥–è®ºæ–‡</li>
</ul>
<ol start="4">
<li>å»¶ä¼¸</li>
</ol>
<ul>
<li>referenceï¼šç»¼è¿°é‡Œï¼ˆç¬¬äºŒç« ï¼‰è¢«metionçš„è®ºæ–‡æ¬¡ä¹‹</li>
<li>citationï¼šåœ¨googleé‡Œç‚¹å‡»â€œè¢«å¼•ç”¨â€</li>
</ul>
<ol start="5">
<li>ä¹ æƒ¯</li>
</ol>
<ul>
<li>ç»´æŒä¸€ä¸ªpaper queueï¼Œç»å…¸è®ºæ–‡æé«˜ä¼˜å…ˆçº§</li>
<li>ä»é˜Ÿé¦–é€‰ä¸€ç¯‡</li>
<li>æŠŠreferenceå’Œcitationï¼ˆç­›å»ä¸ç›¸å…³ï¼‰æ”¾å…¥é˜Ÿå°¾</li>
</ul>
<ol start="6">
<li>å¤šäº¤æµ</li>
</ol>
<h1 id="è¯»è®ºæ–‡"><a class="header" href="#è¯»è®ºæ–‡">è¯»è®ºæ–‡</a></h1>
<ol>
<li>Quick skim</li>
</ol>
<p>å†…å®¹ï¼šçœ‹å›¾ï¼Œçœ‹è§†é¢‘ï¼ˆä¼šè®®çš„ presentation viewï¼‰
ç›®çš„ï¼šå¯¹æ–‡ç« äº†è§£å¤§æ¦‚
æ—¶é—´ï¼šååˆ†é’Ÿ</p>
<ol start="2">
<li>Critical reading</li>
</ol>
<p>å†…å®¹ï¼š</p>
<ul>
<li>Title</li>
<li>Abstract</li>
<li>Introduction</li>
<li>discussion/limitation</li>
</ul>
<p>ç›®çš„ï¼š</p>
<ul>
<li>æ ¸å¿ƒé—®é¢˜æ˜¯ä»€ä¹ˆ</li>
<li>æ ¸å¿ƒè´¡çŒ®æ˜¯ä»€ä¹ˆï¼Ÿ</li>
<li>å¤§è‡´æ–¹æ³•æ˜¯ä»€ä¹ˆï¼Ÿæœ‰æ•ˆï¼Ÿç¼ºé™·ï¼ŸéªŒè¯ï¼Ÿ</li>
<li>å¯å‘</li>
</ul>
<h1 id="è®°ç¬”è®°"><a class="header" href="#è®°ç¬”è®°">è®°ç¬”è®°</a></h1>
<ol>
<li>Summary</li>
<li>Fact.</li>
</ol>
<ul>
<li>motivafion</li>
<li>contribution</li>
<li>method</li>
<li>evaluation</li>
</ul>
<ol start="3">
<li>Crifical thinking</li>
</ol>
<ul>
<li>æ‰¹åˆ¤ä¼˜ç¼ºç‚¹</li>
</ul>
<ol start="4">
<li>Creative thinking</li>
</ol>
<p>å¦‚ä½•å¯å‘äº†æˆ‘</p>
<ul>
<li>æƒ³åˆ°ä»€ä¹ˆidea</li>
<li>æ€ä¹ˆimprove</li>
<li>æ€ä¹ˆgeneralize</li>
</ul>
<ol start="5">
<li>what I have learned</li>
</ol>
<p>ç²¾è¯»ï¼š1å‘¨1ï½2ç¯‡</p>
<h1 id="æ±‚åé¦ˆ"><a class="header" href="#æ±‚åé¦ˆ">æ±‚åé¦ˆ</a></h1>
<ul>
<li>å¯¹ç…§ä½œè€…videoçš„é‡ç‚¹</li>
<li>å¯¹ç…§ä½œè€…æ€»ç»“çš„limitaton</li>
<li>åˆ†äº«ï¼Œäº¤æµ</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="diffusion-modelæ€ç»´å¯¼å›¾"><a class="header" href="#diffusion-modelæ€ç»´å¯¼å›¾">Diffusion Modelæ€ç»´å¯¼å›¾</a></h1>
<ul>
<li>åŸºç¡€
<ul>
<li>DDPM
<ul>
<li>åŸç†ï¼šForward/Reverse</li>
<li>è®­ç»ƒä¸æ¨æ–­</li>
<li>æ•°å­¦åŸç†</li>
</ul>
</li>
<li>Scored-based 
<ul>
<li>ä»DDPMåˆ°SDE</li>
<li>ç”¨SDEæè¿°DDPMçš„æ­£å‘è¿‡ç¨‹å’Œé€†å‘è¿‡ç¨‹</li>
<li>æ‹Ÿåˆscore function</li>
<li>ä»SDEåˆ°ODE</li>
</ul>
</li>
<li>Pipeline
<ul>
<li>æ¡ä»¶ç”Ÿæˆ</li>
<li>æŸå¤±å‡½æ•°</li>
<li>é‡‡æ ·</li>
</ul>
</li>
</ul>
</li>
<li>T2IåŸºæ¨¡å‹</li>
<li>åŸºäºT2IåŸºæ¨¡å‹çš„å›¾åƒåº”ç”¨
<ul>
<li>å›¾åƒç”Ÿæˆ</li>
<li>å›¾åƒç¼–è¾‘</li>
<li>ç‰¹å®šå¯¹è±¡çš„å›¾åƒç”Ÿæˆ</li>
<li>å¤šä¸ªç‰¹å®šå¯¹è±¡çš„ç”Ÿæˆ</li>
</ul>
</li>
<li>åŸºäºT2IåŸºæ¨¡å‹çš„3Dåº”ç”¨
<ul>
<li>3Dè¡¨ç¤º</li>
<li>3Dç”Ÿæˆ</li>
<li>æ–°è§†è§’ç”Ÿæˆ</li>
<li>3Dé‡å»º</li>
<li>3Dç¼–è¾‘</li>
</ul>
</li>
<li>åŸºäºT2IåŸºæ¨¡å‹çš„è§†é¢‘ç”Ÿæˆï¼ˆè§å¦å¤–ä¸€ä¸ªç³»åˆ—ï¼‰</li>
<li>ç®—æ³•æŠ€å·§
<ul>
<li>åŠ é€Ÿ</li>
<li>æå‡è´¨é‡</li>
<li>æå‡ç¨³å®šæ€§</li>
</ul>
</li>
</ul>
<h1 id="èƒŒæ™¯"><a class="header" href="#èƒŒæ™¯">èƒŒæ™¯</a></h1>
<h2 id="generative-adversarial-networks"><a class="header" href="#generative-adversarial-networks">Generative Adversarial Networks</a></h2>
<p>Restricted Boltzmann Machines<br />
Bayesian Networks<br />
Variational Autoencoders<br />
Normalizing Flows<br />
Energy-based Models<br />
Autoregressive Models<br />
Denoising Diffusion Models</p>
<p>P6</p>
<p><img src="diffusion-tutorial-part/../assets/D1-6.png" alt="" /> </p>
<p>Disclaimer: We rely on paper titles for counting the number of papers in each topic. Our statistics are likely to be biased.</p>
<h1 id="å‚è€ƒææ–™"><a class="header" href="#å‚è€ƒææ–™">å‚è€ƒææ–™</a></h1>
<h2 id="denoising-diffusion-models-a-generative-learning-big-bang"><a class="header" href="#denoising-diffusion-models-a-generative-learning-big-bang">Denoising Diffusion Models: A Generative Learning Big Bangã€€ã€€ã€€</a></h2>
<p>Jiaming Songã€€ã€€ã€€Chenlin Meng ã€€ã€€ã€€Arash Vahdatã€€ã€€ã€€</p>
<p>CVPR  #18546</p>
<p>https://www.youtube.com/watch?v=1d4r19GEVos</p>
<p><a href="https://cvpr2023-tutorial-diffusion-models.github.io/">https://cvpr2023-tutorial-diffusion-models.github.io/</a></p>
<p><img src="diffusion-tutorial-part/../assets/D1-7.png" alt="" /> </p>
<h2 id="æå®æ¯…dmè¯¾ç¨‹"><a class="header" href="#æå®æ¯…dmè¯¾ç¨‹">æå®æ¯…DMè¯¾ç¨‹</a></h2>
<h2 id="å…¶å®ƒå‚è€ƒææ–™"><a class="header" href="#å…¶å®ƒå‚è€ƒææ–™">å…¶å®ƒå‚è€ƒææ–™</a></h2>
<p>P4</p>
<div style="break-before: page; page-break-before: always;"></div><p>P12</p>
<h1 id="diffusion-model-æ˜¯å¦‚ä½•è¿ä½œçš„"><a class="header" href="#diffusion-model-æ˜¯å¦‚ä½•è¿ä½œçš„">Diffusion Model æ˜¯å¦‚ä½•è¿ä½œçš„ï¼Ÿ</a></h1>
<p>P13</p>
<p>Denoising diffusion models consist of two processes:</p>
<ul>
<li>Forward diffusion process that gradually adds noise to input</li>
<li>Reverse denoising process that learns to generate data by denoising</li>
</ul>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-13.png" alt="" /> </p>
<p>P14</p>
<h2 id="forward-diffusion-process"><a class="header" href="#forward-diffusion-process">Forward Diffusion Process</a></h2>
<p>The formal definition of the forward process in T steps:</p>
<h3 id="ç›´è§‚ç†è§£"><a class="header" href="#ç›´è§‚ç†è§£">ç›´è§‚ç†è§£</a></h3>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy1-8-1.png" alt="" /></p>
<blockquote>
<p>çœŸæ­£çš„åŠ å™ªè¿‡ç¨‹ï¼Œ<strong>ä¸æ˜¯ç›´æ¥çš„image + noise</strong>ã€‚</p>
</blockquote>
<h3 id="ä»æ•°å­¦ä¸Šç†è§£"><a class="header" href="#ä»æ•°å­¦ä¸Šç†è§£">ä»æ•°å­¦ä¸Šç†è§£</a></h3>
<blockquote>
<p>âœ… ä»ç¬¬ä¸€å¼ å›¾åƒåˆ°æœ€åçš„çº¯å™ªå£°ï¼Œå®é™…ä¸Šæ˜¯åˆ†å¸ƒçš„æ”¹å˜ã€‚</p>
</blockquote>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-14.png" alt="" /> </p>
<p>é€šè¿‡é€æ­¥çš„ scale down è®©å‡å€¼è¶‹è¿‘äº 0ã€‚é€šè¿‡å¼•å…¥å™ªå£°ä½¿æ–¹å·®è¶‹è¿‘äº 1ã€‚ä½¿å¾—åŸå§‹åˆ†å¸ƒé€æ­¥é€¼è¿‘ \(\mathcal{N} (0,1 )\)åˆ†å¸ƒï¼Œ</p>
<blockquote>
<p>â“ æ±‚è”åˆåˆ†å¸ƒæœ‰ä»€ä¹ˆç”¨?</p>
</blockquote>
<h3 id="ä»æ“ä½œå±‚é¢ç†è§£"><a class="header" href="#ä»æ“ä½œå±‚é¢ç†è§£">ä»æ“ä½œå±‚é¢ç†è§£</a></h3>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-15.png" alt="" /> </p>
<blockquote>
<p>âœ… å®é™…ä¸Šï¼Œåœ¨ç»™å®šä¸€å¼ å›¾åƒx0æ—¶ï¼Œæƒ³è¦è·å¾—ç¬¬tå¼ åŠ å™ªå›¾åƒæ—¶ï¼Œä¸éœ€è¦çœŸçš„é€šè¿‡å…¬å¼\(q(x_t|x_{t-1})\)ä» \(\mathbf{x} _{t-1}\)åˆ° \(\mathbf{x} _{t}\)ä¸€æ­¥ä¸€æ­¥è®¡ç®—å‡ºæ¥ï¼Œå¯ä»¥ç›´æ¥ä» \(\mathbf{x}_0\)ç”Ÿæˆä»»æ„çš„ \(\mathbf{x}_t\)ã€‚ </p>
</blockquote>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-15-1.png" alt="" /> </p>
<p>ä»æ•°å­¦ä¸Šå¯ä»¥è¯æ˜ï¼Œä»x0é€æ­¥è®¡ç®—åˆ°xtå’Œä»x0ç›´æ¥è®¡ç®—åˆ°xtï¼Œè¿™ä¸¤ç§è¡Œä¸ºæ˜¯ç­‰ä»·çš„ã€‚</p>
<p>æ ¹æ®å…¬å¼ \(\mathbf{x} _t=\sqrt{\bar{a} _t}   \mathbf{x} _0+\sqrt{(1-\bar{a} _t) }  \varepsilon  \)å¯çŸ¥ï¼Œå½“ \(\bar{a} _T  â†’ 0\)ï¼Œåˆ†å¸ƒ\(q(x_T)\)çš„å‡å€¼è¶‹äº0ï¼Œæ–¹å·®è¶‹äº1ï¼Œå˜æˆçº¯é«˜æ–¯å™ªå£°ã€‚</p>
<p>P16</p>
<h3 id="è¿›ä¸€æ­¥ç†è§£"><a class="header" href="#è¿›ä¸€æ­¥ç†è§£">è¿›ä¸€æ­¥ç†è§£</a></h3>
<p>So far, we discussed the diffusion kernel \(q(\mathbf{x} _t|\mathbf{x} _0)\) but what about \(q(\mathbf{x}_t)\)?</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-16-1.png" alt="" /> </p>
<p>The diffusion kernel is Gaussian convolution.</p>
<blockquote>
<p>âœ… convolution æ˜¯ä¸€ç§ä¿¡å·å¹³æ»‘æ–¹æ³•ã€‚<br />
âœ… \(q(\mathbf{x} _ t|\mathbf{x} _ 0)\) æ˜¯æ ‡å‡†é«˜æ–¯åˆ†å¸ƒï¼Œå› æ­¤ \(q(\mathbf{x} _ t)\) æ˜¯ä»¥é«˜æ–¯åˆ†å¸ƒä¸ºæƒé‡çš„çœŸå®æ•°æ®çš„åŠ æƒå¹³å‡ã€‚</p>
</blockquote>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-16-2.png" alt="" /> </p>
<p>We can sample \(\mathbf{x}_t \sim q(\mathbf{x}_t)\) by first sampling \(\mathbf{x}_0\) and then sampling \(\mathbf{x}_t \sim q(\mathbf{x}_t|\mathbf{x}_0)\) (i.e., ancestral sampling).</p>
<blockquote>
<p>âœ… å®é™…ä¸Šï¼Œæ²¡æœ‰ä»»æ„ä¸€ä¸ªæ—¶é—´æ­¥çš„ \(q(\mathbf{x})\) çš„çœŸå®åˆ†å¸ƒï¼Œåªæœ‰è¿™äº›åˆ†å¸ƒçš„ sample.</p>
</blockquote>
<h2 id="reverse-denoising-process"><a class="header" href="#reverse-denoising-process">Reverse Denoising Process</a></h2>
<p>P17</p>
<h3 id="ç›´è§‚ç†è§£-1"><a class="header" href="#ç›´è§‚ç†è§£-1">ç›´è§‚ç†è§£</a></h3>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy1-2.png" alt="" /></p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy1-4-1.png" alt="" /></p>
<p>Denoiseæ˜¯ä¸€ä¸ªç½‘ç»œæ¨¡å—ï¼Œé€šè¿‡Denoiseæ¨¡å—å­¦ä¹ æ¯ä¸ªæ—¶é—´æ­¥çš„å»å™ªè¿‡ç¨‹ã€‚</p>
<blockquote>
<p>âœ… æŠŠ \(\mathbf{x}_0\) åŠ å™ªä¸º init-noiseï¼Œå†ä» init-noise æ¢å¤å‡º \(\mathbf{x}_0\)ï¼Œè¿™ä¸ªæ“ä½œæ˜¯ä¸å¯è¡Œçš„ã€‚<br />
âœ… å› ä¸ºï¼Œæ ¹æ®å…¬å¼ \(\mathbf{x} _t=\sqrt{\bar{a} _t}   \mathbf{x} _0+\sqrt{(1-\bar{a} _t) }  \varepsilon  \), ä¸” \(\bar{a} _T  â†’ 0\)ï¼Œé‚£ä¹ˆç»è¿‡ \(T\) æ­¥åŠ å™ªåï¼Œ\(\mathbf{x} _t\approx \varepsilon \). è€Œæ˜¯ \(\varepsilon \) æ˜¯ä¸€ä¸ªä¸ \(\mathbf{x} _ 0\) æ²¡æœ‰ä»»åŠ¡å…³ç³»çš„å™ªå£°ï¼Œæ‰€ä»¥ä¸å¯èƒ½ä»ä¸­æ¢å¤å‡º \(\mathbf{x} _ 0\).</p>
</blockquote>
<h3 id="ä»æ•°å­¦ä¸Šç†è§£-1"><a class="header" href="#ä»æ•°å­¦ä¸Šç†è§£-1">ä»æ•°å­¦ä¸Šç†è§£</a></h3>
<p>ä»xTåˆ°x0çš„è¿‡ç¨‹ï¼Œä¹Ÿæ˜¯åˆ†å¸ƒçš„æ”¹å˜ã€‚ä»\(\mathcal{N}(\mathbf{x}_Tï¼›\mathbf{0,I})\)wåˆ†å¸ƒå˜æˆçœŸå®åˆ†å¸ƒçš„è¿‡ç¨‹ã€‚</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-17-2.png" alt="" /> </p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-17-1.png" alt="" /> </p>
<p>ä¸Forwardä¸åŒçš„æ˜¯ï¼Œ\(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)æ²¡æœ‰ä¸€ä¸ªå‡†ç¡®çš„æ•°å­¦å…¬å¼æ¥è¡¨è¾¾ã€‚</p>
<p>Can we approximate \(q(\mathbf{x}_{t-1}|\mathbf{x}_t)\)? Yes, we can use a <strong>Normal distribution</strong> if \(\beta _t\) is small in each forward diffusion step.</p>
<blockquote>
<p>âœ… Nomal distribution æ˜¯ç‰¹å®šå‡å€¼å’Œæ–¹å·®çš„é«˜æ–¯åˆ†å¸ƒï¼Œä¸ä¸€å®šæ˜¯ std é«˜æ–¯ã€‚</p>
</blockquote>
<p>P18</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-18.png" alt="" /> </p>
<p>å‡è®¾\(p(\mathbf{x} _ T)\)å’Œ\(p(\mathbf{x}_{t-1}|\mathbf{x}<em>t)\)åˆ†åˆ«ç¬¦åˆä»¥ä¸Šåˆ†å¸ƒã€‚<br />
ä»ç¬¬1ä¸ªåˆ†å¸ƒä¸­sampleå‡º\(x_T\)ï¼ŒæŠŠå®ƒä»£å…¥ç¬¬äºŒä¸ªåˆ†å¸ƒï¼Œå°±å¯ä»¥sampleå‡º\(x</em>{T-1}\)ï¼Œç›´åˆ°æœ€ç»ˆsampleå‡º\(x_0\)</p>
<p><strong>ç”±äºä»¥ä¸Šæˆªå›¾æ¥è‡ªä¸åŒçš„ææ–™ï¼Œå­˜åœ¨på’Œqæ··æœ‰çš„æƒ…å†µï¼Œéœ€æ³¨æ„åŒºåˆ†ã€‚</strong></p>
<p>P19</p>
<h3 id="learning-denoising-model"><a class="header" href="#learning-denoising-model">Learning Denoising Model</a></h3>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-19-1.png" alt="" /> </p>
<blockquote>
<p>âœ… ä»¥ä¸Šæ˜¯å»å™ªæ¨¡å‹çš„å…¬å¼ï¼Œä¸‹é¢æœ‰å…³äºè¿™äº›å…¬å¼çš„è¯¦ç»†è§£é‡Šã€‚</p>
</blockquote>
<p>P20</p>
<h1 id="è®­ç»ƒä¸æ¨æ–­"><a class="header" href="#è®­ç»ƒä¸æ¨æ–­">è®­ç»ƒä¸æ¨æ–­</a></h1>
<p>ä½¿ç”¨Forwardæµç¨‹å¯¹çœŸå®æ•°æ®åŠ å™ªï¼Œä»¥æ„é€ pair dataã€‚<br />
ä½¿ç”¨ä½¿ç”¨Denoiseæ¨¡å—å­¦ä¹ å»å™ªåˆ†å¸ƒï¼Œå®Œæˆå»å™ªè¿‡ç¨‹ã€‚</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy1-5.png" alt="" /></p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-20.png" alt="" /> </p>
<p>P21</p>
<h2 id="implementation-considerations"><a class="header" href="#implementation-considerations">Implementation Considerations</a></h2>
<p>Diffusion models often use U-Net architectures with ResNet blocks and self-attention layers to represent \(\epsilon _\theta (\mathbf{x}_t,t)\).</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-21.png" alt="" /> </p>
<p>Time representation: sinusoidal positional embeddings or random Fourier features.</p>
<p>Time features are fed to the residual blocks using either simple spatial addition or using adaptive group normalization layers. (see <u>Dharivwal and Nichol NeurIPS 2021</u>).</p>
<blockquote>
<p>âœ… \(\sigma \) æ˜¯æ€ä¹ˆå®šä¹‰çš„ï¼Ÿ</p>
</blockquote>
<h1 id="æ•°å­¦åŸç†"><a class="header" href="#æ•°å­¦åŸç†">æ•°å­¦åŸç†</a></h1>
<p>P10</p>
<h2 id="ç”Ÿæˆæ¨¡å‹æœ¬è´¨ä¸Šçš„å…±åŒç›®æ ‡"><a class="header" href="#ç”Ÿæˆæ¨¡å‹æœ¬è´¨ä¸Šçš„å…±åŒç›®æ ‡">ç”Ÿæˆæ¨¡å‹æœ¬è´¨ä¸Šçš„å…±åŒç›®æ ‡</a></h2>
<h3 id="ç›®æ ‡æ˜¯è¦å­¦ä¸€ä¸ªåˆ†å¸ƒ"><a class="header" href="#ç›®æ ‡æ˜¯è¦å­¦ä¸€ä¸ªåˆ†å¸ƒ">ç›®æ ‡æ˜¯è¦å­¦ä¸€ä¸ªåˆ†å¸ƒ</a></h3>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-10.png" alt="" /> </p>
<p>ç”Ÿæˆæ¨¡å‹çš„æœ¬è´¨æ˜¯è¦å­¦åˆ°çœŸå®æ•°æ®çš„åˆ†å¸ƒï¼Œä»¥åŠä»æŸä¸ªå·²ç»åˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯æ­£æ€åˆ†å¸ƒï¼‰åˆ°è¿™ä¸ªçœŸå®æ•°æ®åˆ†å¸ƒçš„æ˜ å°„ã€‚</p>
<blockquote>
<p>âœ… å®é™…ä½¿ç”¨ä¸­è¿˜ä¼šåŠ ä¸€ä¸ª conditionï¼Œä½†æ•´ä½“ä¸Šæ²¡æœ‰æœ¬è´¨å·®å¼‚ï¼Œå› æ­¤åé¢æ¨å¯¼ä¸­ä¸è€ƒè™‘ condition.</p>
</blockquote>
<p>P11</p>
<h3 id="å®šä¹‰ç›®æ ‡å‡½æ•°"><a class="header" href="#å®šä¹‰ç›®æ ‡å‡½æ•°">å®šä¹‰ç›®æ ‡å‡½æ•°</a></h3>
<h4 id="ä»¥minimize-kl-divergenceä½œä¸ºç›®æ ‡å‡½æ•°"><a class="header" href="#ä»¥minimize-kl-divergenceä½œä¸ºç›®æ ‡å‡½æ•°">ä»¥Minimize KL Divergenceä½œä¸ºç›®æ ‡å‡½æ•°</a></h4>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-11.png" alt="" /> </p>
<p>ç›®æ ‡æ˜¯è®©ç”Ÿæˆæ•°æ®çš„åˆ†å¸ƒä¸çœŸå®æ•°æ®çš„åˆ†å¸ƒå°½é‡çš„æ¥è¿‘ï¼Œä½†æ˜¯æ€æ ·è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒæ˜¯å¦æ¥è¿‘ï¼Ÿ</p>
<blockquote>
<p>âœ… å¸¸ç”¨KL Divergenceæ¥è¡¡é‡é¢„æµ‹åˆ†å¸ƒä¸GTåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚</p>
</blockquote>
<h4 id="ä»¥maximum-likelihood-estimation"><a class="header" href="#ä»¥maximum-likelihood-estimation">ä»¥Maximum Likelihood Estimation</a></h4>
<p>\(P_{data}\) ä»£è¡¨çœŸå®åˆ†å¸ƒï¼Œä»åˆ†å¸ƒä¸­ Sample å‡ºæ¥çš„ \(x\) å³è®­ç»ƒé›†<br />
\(x_i\)æ˜¯æ•°æ®é›†é‡Œçš„ä¸€ä¸ªæ•°æ®ï¼Œä¹Ÿæ˜¯çœŸå®æ•°æ®åˆ†å¸ƒé‡Œçš„ä¸€ä¸ªé‡‡æ ·ã€‚\(P_\theta (x^i)\) ä»£è¡¨ \(P_\theta\) ç”Ÿæˆ \(x^i\) çš„æ¦‚ç‡ã€‚</p>
<blockquote>
<p>âœ… ç”±äº \(P_\theta\) éå¸¸å¤æ‚ï¼Œç®—ä¸å‡ºè¿™ä¸ªæ¦‚ç‡ï¼Œä½†æ­¤å¤„å‡è®¾ \(P_\theta (x^i)\) å·²çŸ¥ã€‚</p>
</blockquote>
<p>äºæ˜¯å¯ä»¥å°†å®šä¹‰ç›®æ ‡å‡½æ•°ä¸ºï¼šæ‰¾å‡ºè®©çœŸå® \(x^i\) è¢«ç”Ÿæˆå‡ºæ¥çš„æ¦‚ç‡æœ€é«˜çš„\(\theta \).</p>
<p>\begin{align*} \theta ^\ast =\text{arg } \max_{\theta } \prod_{i=1}^{m} P_\theta (x^i) \end{align*}</p>
<h4 id="ä¸¤ä¸ªç›®æ ‡å‡½æ•°æ˜¯ç­‰ä»·çš„"><a class="header" href="#ä¸¤ä¸ªç›®æ ‡å‡½æ•°æ˜¯ç­‰ä»·çš„">ä¸¤ä¸ªç›®æ ‡å‡½æ•°æ˜¯ç­‰ä»·çš„</a></h4>
<p>å¯é€šè¿‡æ•°æ®æ¨å¯¼è¯æ˜ï¼Œè¿™é‡Œæåˆ°çš„ä¸¤ä¸ªç›®æ ‡ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€è‡´çš„ã€‚è¯æ˜è¿‡ç¨‹å¦‚ä¸‹ï¼š</p>
<p>P12</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-12.png" alt="" /> </p>
<p>Maximum Likelihood = Minimize KL Divergence</p>
<blockquote>
<p>âœ… ç»“è®ºï¼šè®©çœŸå®æ•°æ®çš„æ¦‚ç‡æœ€å¤§ï¼Œä¸è®©ä¸¤ä¸ªåˆ†å¸ƒå°½é‡æ¥è¿‘ï¼Œåœ¨æ•°å­¦ä¸Šæ˜¯ä¸€è‡´çš„ã€‚<br />
âœ… VAEã€diffusionã€flow based ç­‰ç”Ÿæˆæ¨¡å‹ï¼Œéƒ½æ˜¯ä»¥æœ€å¤§åŒ– Likelihood ä¸ºç›®æ ‡ã€‚GAN æ˜¯æœ€å°åŒ– JS Divergence ä¸ºç›®æ ‡ã€‚</p>
</blockquote>
<p>P13</p>
<h2 id="compute-ğ‘ƒ_thetax"><a class="header" href="#compute-ğ‘ƒ_thetax">Compute \(ğ‘ƒ_\theta(x)\)</a></h2>
<h3 id="è®¡ç®—ğ‘ƒ_thetaxçš„å¸¸ç”¨æŠ€å·§"><a class="header" href="#è®¡ç®—ğ‘ƒ_thetaxçš„å¸¸ç”¨æŠ€å·§">è®¡ç®—\(ğ‘ƒ_\theta(x)\)çš„å¸¸ç”¨æŠ€å·§</a></h3>
<blockquote>
<p>âœ… VAE å’Œ diffusion éå¸¸ç›¸ä¼¼ï¼Œè®¸å¤šå…¬å¼æ˜¯é€šç”¨çš„ã€‚</p>
</blockquote>
<h4 id="æŠ€å·§ä¸€ä¸æ¨æ–­ç”Ÿæˆç»“æœè€Œæ˜¯æ¨æ–­ç”Ÿæˆç»“æœåˆ†å¸ƒçš„å‡å€¼"><a class="header" href="#æŠ€å·§ä¸€ä¸æ¨æ–­ç”Ÿæˆç»“æœè€Œæ˜¯æ¨æ–­ç”Ÿæˆç»“æœåˆ†å¸ƒçš„å‡å€¼">æŠ€å·§ä¸€ï¼šä¸æ¨æ–­ç”Ÿæˆç»“æœï¼Œè€Œæ˜¯æ¨æ–­ç”Ÿæˆç»“æœåˆ†å¸ƒçš„å‡å€¼</a></h4>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-13-1.png" alt="" /></td><td><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-13-2.png" alt="" /></td></tr>
<tr><td><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-13-3.png" alt="" /></td><td><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-13-4.png" alt="" /></td></tr>
</tbody></table>
<blockquote>
<p>âœ… \(Gï¼ˆzï¼‰\) ä¸ä»£è¡¨æŸä¸ªç”Ÿæˆç»“æœï¼Œè€Œæ˜¯ä¸€ä¸ªé«˜æ–¯çš„å‡å€¼ï¼Œç„¶åè®¡ç®— \(x\) åœ¨è¿™ä¸ªåˆ†å¸ƒä¸­çš„æ¦‚ç‡ã€‚</p>
</blockquote>
<p>P14</p>
<h4 id="æŠ€å·§äºŒä¸æ±‚ğ‘ƒ_thetaxè€Œæ˜¯æ±‚lower-bound-of-log-px"><a class="header" href="#æŠ€å·§äºŒä¸æ±‚ğ‘ƒ_thetaxè€Œæ˜¯æ±‚lower-bound-of-log-px">æŠ€å·§äºŒï¼šä¸æ±‚\(ğ‘ƒ_\theta(x)\)ï¼Œè€Œæ˜¯æ±‚Lower bound of \(log P(x)\)</a></h4>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-14.png" alt="" /></p>
<blockquote>
<p>âœ… é€šå¸¸æ— æ³•æœ€å¤§åŒ– \(Pï¼ˆxï¼‰\)ï¼Œè€Œæ˜¯æœ€å¤§åŒ– \(log P(x)\) çš„ä¸‹ç•Œã€‚<br />
âœ… ä»¥ä¸Šå…¬å¼æ¨å¯¼ä¸­çœç•¥å‚æ•° \( \theta\)ã€‚</p>
</blockquote>
<p>P15</p>
<h3 id="ddpm-compute-ğ‘ƒ_thetax"><a class="header" href="#ddpm-compute-ğ‘ƒ_thetax">DDPM: Compute \(ğ‘ƒ_\theta(x)\)</a></h3>
<p>å¯¹äº diffusion modelï¼Œå‡è®¾æ¯æ¬¡ denoise å‡ºçš„æ˜¯ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒçš„å‡å€¼ã€‚</p>
<blockquote>
<p>â“ é—®ï¼šä¸ºä»€ä¹ˆå‡è®¾\(G(x_t)\) æ˜¯é«˜æ–¯åˆ†å¸ƒçš„ meanï¼Ÿ<br />
âœ… ç­”ï¼šæœ‰äººå°è¯•è¿‡å…¶å®ƒå‡è®¾ï¼Œæ•ˆæœæ²¡æœ‰å˜å¥½ï¼Œä¸”é«˜æ–¯åˆ†å¸ƒä¾¿äºè®¡ç®—ã€‚</p>
</blockquote>
<p>é€šè¿‡é“¾å¼æ³•åˆ™ï¼Œå¯ä»¥å¾—å‡º \(x_0\) åœ¨æœ€ç»ˆåˆ†å¸ƒä¸­çš„æ¦‚ç‡ä¸ºï¼š</p>
<p>$$
P_ \theta (x_0)=\int\limits _ {x_1:x_T}^{} P(x_T)P_ \theta (x_{T-1}|x_T) \dots P_ \theta (x_ {t-1}|x_t) \dots P_ \theta(x_0|x_1)dx_1:x_T<br />
$$</p>
<p>P16</p>
<h3 id="ddpm-lower-bound-of-log-px"><a class="header" href="#ddpm-lower-bound-of-log-px">DDPM: Lower bound of \(log P(x)\)</a></h3>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-16-1.png" alt="" /></p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-16-2.png" alt="" /></p>
<h3 id="è®¡ç®—lower-bound-of-log-px"><a class="header" href="#è®¡ç®—lower-bound-of-log-px">è®¡ç®—Lower bound of \(log P(x)\)</a></h3>
<h4 id="è®¡ç®—qx_tx_t-1"><a class="header" href="#è®¡ç®—qx_tx_t-1">è®¡ç®—\(qï¼ˆx_tï½œx_{t-1}ï¼‰\)</a></h4>
<p>P17<br />
<img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-17.png" alt="" /></p>
<blockquote>
<p>âœ… æå‰å®šå¥½ä¸€ç»„ \(\beta \)ï¼ä»£è¡¨ noise è¦åŠ å¤šå¤§ã€‚<br />
âœ… \(qï¼ˆx_tï½œx_{t-1}ï¼‰\) ä»ç„¶å±äºé«˜æ–¯åˆ†å¸ƒï¼Œå…¶å‡å€¼ä¸º \(\sqrt{1-\beta _t} \cdot x_t\)ï¼Œæ–¹å·®ä¸º \(\beta _t\).</p>
</blockquote>
<h4 id="è®¡ç®—qx_tx_0"><a class="header" href="#è®¡ç®—qx_tx_0">è®¡ç®—\(qï¼ˆx_tï½œx_{0}ï¼‰\)</a></h4>
<p>P18<br />
<img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-18.png" alt="" /></p>
<p>P19<br />
<img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-19.png" alt="" /></p>
<blockquote>
<p>âœ… ç”±äºä¸¤æ¬¡ sample å‡ºçš„ noise æ˜¯ç‹¬ç«‹åŒåˆ†å¸ƒï¼Œä¸¤ä¸ª noise ä»¥è¿™ç§å½¢å¼ç›¸åŠ çš„ç»“æœï¼Œä¹Ÿç¬¦åˆæŸä¸ªç‰¹å®šçš„é«˜æ–¯åˆ†å¸ƒã€‚</p>
</blockquote>
<p>P20</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-20.png" alt="" /></p>
<blockquote>
<p>âœ… ç»“è®ºï¼š\(qï¼ˆx_tï½œx_{0}ï¼‰\)ä¹Ÿç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œå…¶å‡å€¼ä¸º\(\bar{\alpha }_t\)ï¼Œæ–¹å·®ä¸º\({1-\bar{\alpha }_t}\).</p>
</blockquote>
<h2 id="å®šä¹‰æŸå¤±å‡½æ•°"><a class="header" href="#å®šä¹‰æŸå¤±å‡½æ•°">å®šä¹‰æŸå¤±å‡½æ•°</a></h2>
<p>å¦‚ä½•å®šä¹‰æŸå¤±å‡½æ•°ï¼Œå¯ä»¥è¾¾åˆ°æœ€å¤§åŒ–\(\log P_{\theta}(x_0)\)çš„ç›®çš„</p>
<h3 id="æŸå¤±å‡½æ•°ä¸ç›®æ ‡å‡½æ•°"><a class="header" href="#æŸå¤±å‡½æ•°ä¸ç›®æ ‡å‡½æ•°">æŸå¤±å‡½æ•°ä¸ç›®æ ‡å‡½æ•°</a></h3>
<blockquote>
<p>ç›®æ ‡å‡½æ•°æ˜¯æ ¹æ®å®é™…æ„ä¹‰æ¨å¯¼å‡ºæ¥çš„ä¼˜åŒ–ç›®æ ‡ã€‚æŸå¤±å‡½æ•°æ˜¯èƒ½å¼•å¯¼å­¦ä¹ æ”¶æ•›åˆ°ç›®æ ‡çŠ¶æ€çš„å‡½æ•°ï¼Œå¯ä»¥æ²¡æœ‰å®é™…æ„ä¹‰ï¼Œä¹Ÿå¯ä»¥è·Ÿç›®æ ‡å‡½æ•°ä¸ä¸€æ ·ã€‚<br />
è™½ç„¶ç›®æ ‡å‡½æ•°å¾ˆæ˜ç¡®ï¼Œä½†æ˜¯æŸå¤±å‡½æ•°ä¸ä¸€å®šè¦è·Ÿç›®æ ‡å‡½æ•°ä¸€æ ·ã€‚å¯ä»¥ä»ç›®æ ‡å‡½æ•°ä¸­æå–å‡ºå½±å“ç»“æœçš„å…³é”®å› ç´ æ¥å¼•å¯¼å­¦ä¹ è¿‡ç¨‹ã€‚</p>
</blockquote>
<h3 id="æ¨å¯¼ä¸ç®€åŒ–ç›®æ ‡å‡½æ•°log-px"><a class="header" href="#æ¨å¯¼ä¸ç®€åŒ–ç›®æ ‡å‡½æ•°log-px">æ¨å¯¼ä¸ç®€åŒ–ç›®æ ‡å‡½æ•°\(log P(x)\)</a></h3>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-16-2.png" alt="" /></p>
<p>P21<br />
<img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-21.png" alt="" /></p>
<p>P22</p>
<p>æœ€åç®€åŒ–ä¸ºä»¥ä¸‹ä¸‰é¡¹ï¼š</p>
<p>\begin{align*} E_{q(x_1|x_0)}[log P(x_0|x_1)]-KL(q(x_T|x_0)||P(x_T))
-\sum_{t=2}^{T}E_{q(x_t|x_0)}[KL(q(x_{t-1}|x_t,x_0)||P(x_{t-1}|x_t))]   \end{align*}</p>
<h3 id="åˆ†æç›®æ ‡å‡½æ•°ä¸­ä¸ä¼˜åŒ–ç›¸å…³çš„å…³é”®å› ç´ "><a class="header" href="#åˆ†æç›®æ ‡å‡½æ•°ä¸­ä¸ä¼˜åŒ–ç›¸å…³çš„å…³é”®å› ç´ ">åˆ†æç›®æ ‡å‡½æ•°ä¸­ä¸ä¼˜åŒ–ç›¸å…³çš„å…³é”®å› ç´ </a></h3>
<h4 id="ç»“è®º"><a class="header" href="#ç»“è®º">ç»“è®º</a></h4>
<blockquote>
<p>âœ… ç›®æ ‡æ˜¯è¦ä¼˜åŒ– \( \theta\)ï¼Œç¬¬äºŒé¡¹ä¸\( \theta\)æ— å…³ï¼Œå¯ä»¥ç•¥æ‰ã€‚<br />
âœ… ç¬¬ä¸‰é¡¹çš„ KL Divrgence æ¶‰åŠåˆ°ä¸¤ä¸ªåˆ†å¸ƒï¼Œåˆ†å¸ƒ1æ˜¯å›ºå®šçš„ï¼Œå¯ä»¥é€šè¿‡è®¡ç®—å¾—åˆ°ï¼Œåˆ†å¸ƒ2æ˜¯ç”± \( \theta\) å†³å®šçš„ï¼Œæ˜¯è¦ä¼˜åŒ–çš„å¯¹è±¡ã€‚</p>
</blockquote>
<p>P23</p>
<h4 id="å…³äºç¬¬ä¸‰é¡¹åˆ†å¸ƒ1çš„æ¨å¯¼è¿‡ç¨‹"><a class="header" href="#å…³äºç¬¬ä¸‰é¡¹åˆ†å¸ƒ1çš„æ¨å¯¼è¿‡ç¨‹">å…³äºç¬¬ä¸‰é¡¹åˆ†å¸ƒ1çš„æ¨å¯¼è¿‡ç¨‹</a></h4>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-23-1.png" alt="" /></p>
<p>å·²çŸ¥ \(q (x_t\mid x_0)\)ï¼Œ\(q (x_{t-1} \mid x_0)\) å’Œ \(q (x_t \mid x_{t-1})\)ä¸ºï¼š</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-23-2.png" alt="" /></p>
<p>æ±‚ \(q (x_{t-1} \mid x_t,x_0)\).</p>
<blockquote>
<p>âœ… \((q(x_{t-1}|x_t,x_0)\)çš„æ•°æ®å«ä¹‰ä¸ºï¼šå·²çŸ¥\(x_0\) å’Œ \(x_t\)ï¼Œæ±‚ \(x_{t-1}\) çš„åˆ†å¸ƒã€‚</p>
</blockquote>
<p>P24<br />
<img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-24.png" alt="" /></p>
<p>P25<br />
<img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-25.png" alt="" /></p>
<blockquote>
<p>https://arxiv.org/pdf/2208.11970.pdf</p>
</blockquote>
<p>P26<br />
<img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-26.png" alt="" /></p>
<blockquote>
<p>âœ… ç»“è®ºï¼š\(q(x_{t-1}|x_t,x_0)\) ä¹Ÿæ˜¯é«˜æ–¯åˆ†å¸ƒï¼Œä¸”å…¶å‡å€¼ä¸æ–¹å·®æ˜¯ä¸\(\theta\)æ— å…³çš„å›ºå®šçš„å€¼ã€‚</p>
</blockquote>
<h4 id="åŒ–ç®€åçš„ç›®æ ‡å‡½æ•°"><a class="header" href="#åŒ–ç®€åçš„ç›®æ ‡å‡½æ•°">åŒ–ç®€åçš„ç›®æ ‡å‡½æ•°</a></h4>
<p>æ ¹æ®ä»¥ä¸Šæ¨å¯¼ï¼Œç›®æ ‡å‡½æ•°å¯ç®€åŒ–ä¸ºæœ€å°åŒ–åŸç›®æ ‡å‡½æ•°ç¬¬ä¸‰é¡¹ä¸­åˆ†å¸ƒ1ä¸åˆ†å¸ƒ2çš„KL Divergenceã€‚</p>
<p>\begin{align*} E_{q(x_1|x_0)}[log P(x_0|x_1)]-KL(q(x_T|x_0)||P(x_T))
-\sum_{t=2}^{T}E_{q(x_t|x_0)}[KL(q(x_{t-1}|x_t,x_0)||P(x_{t-1}|x_t))]   \end{align*}</p>
<p>å…¶ä¸­åˆ†å¸ƒ1ä¸ºä¸\(\theta\)æ— å…³çš„å›ºå®šï¼Œåˆ†å¸ƒ2ä¸ºä¸\(\theta\)æœ‰å…³çš„å¾…ä¼˜åŒ–åˆ†å¸ƒã€‚</p>
<h4 id="how-to-minimize-kl-divergence"><a class="header" href="#how-to-minimize-kl-divergence">How to minimize KL divergence?</a></h4>
<h5 id="æ–¹å¼ä¸€ç›´æ¥å¥—å…¬å¼"><a class="header" href="#æ–¹å¼ä¸€ç›´æ¥å¥—å…¬å¼">æ–¹å¼ä¸€ï¼šç›´æ¥å¥—å…¬å¼</a></h5>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-27-3.png" alt="" /></p>
<blockquote>
<p>âœ… ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒçš„ KLD æœ‰å…¬å¼è§£ï¼Œä½†æ­¤å¤„ä¸ç”¨å…¬å¼è§£ï¼Œå› ä¸º  \( \theta\) åªèƒ½å½±å“åˆ†å¸ƒ2çš„å‡å€¼ã€‚</p>
</blockquote>
<h5 id="æ–¹å¼äºŒ"><a class="header" href="#æ–¹å¼äºŒ">æ–¹å¼äºŒ</a></h5>
<p>åˆ†å¸ƒ1çš„å‡å€¼å’Œæ–¹å·®æ˜¯å›ºå®šçš„ã€‚åˆ†å¸ƒ2çš„å‡å€¼æ˜¯å¾…ä¼˜åŒ–çš„ï¼Œæ–¹å·®æ˜¯å›ºå®šçš„ã€‚</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-27-2.png" alt="" /></p>
<blockquote>
<p>âœ… å› æ­¤å‡å° KLD çš„æ–¹æ³•æ˜¯è®©åˆ†å¸ƒ2çš„å‡å€¼æ¥è¿‘åˆ†å¸ƒ1çš„å‡å€¼ã€‚</p>
</blockquote>
<h3 id="å®šä¹‰æŸå¤±å‡½æ•°-1"><a class="header" href="#å®šä¹‰æŸå¤±å‡½æ•°-1">å®šä¹‰æŸå¤±å‡½æ•°</a></h3>
<blockquote>
<p>âœ… åˆ†å¸ƒ1çš„å‡å€¼å¯ä»¥çœ‹ä½œæ˜¯ \(x_{t-1}\) çš„ GT äº†ã€‚å…¶è®¡ç®—å…¬å¼ä¸ºï¼š</p>
</blockquote>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-28-2.png" alt="" /></p>
<p>\(x_{t-1}\)çš„GTçš„è®¡ç®—å…¬å¼ä¸­åŒ…å«äº†x0å’Œxtï¼ŒæŠŠx0å’Œxtéƒ½è½¬åŒ–ä¸ºxtçš„è¡¨ç¤ºï¼Œå¾—ï¼š</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/lhy3-31.png" alt="" /></p>
<blockquote>
<p>âœ… å¯ä»¥å‘ç° \(x_t\) ä¸ \(x_{t-1}\)å’ŒGT ä¹‹é—´ï¼Œå”¯ä¸€æœªçŸ¥çš„éƒ¨åˆ†å°±æ˜¯ noise \(\varepsilon \). å› æ­¤ç”¨ç½‘ç»œå­¦ä¹ è¿™ä¸ªnoiseã€‚</p>
</blockquote>
<p>æœ€ç»ˆå®šä¹‰æŸå¤±å‡½æ•°ä¸ºç½‘ç»œè¾“å‡º(é¢„æµ‹çš„noise)ä¸GTï¼ˆæ„é€ è®­ç»ƒæ•°æ®æ—¶æ‰€ç”Ÿæˆçš„noiseï¼‰ä¹‹é—´çš„L2è·ç¦»ã€‚</p>
<h2 id="å…¶å®ƒé—®é¢˜"><a class="header" href="#å…¶å®ƒé—®é¢˜">å…¶å®ƒé—®é¢˜</a></h2>
<h3 id="å…³äºalpha-"><a class="header" href="#å…³äºalpha-">å…³äº\(\alpha \)</a></h3>
<blockquote>
<p>âœ… \(\alpha \) æ˜¯é¢„å®šä¹‰çš„è¶…å‚ï¼ŒDDPM è¯•å›¾å­¦ä¹  \(\alpha \)ï¼Œå‘ç°æ²¡æœ‰æå‡ã€‚</p>
</blockquote>
<h1 id="ç›¸å…³è®ºæ–‡"><a class="header" href="#ç›¸å…³è®ºæ–‡">ç›¸å…³è®ºæ–‡</a></h1>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2015</td><td>Deep Unsupervised Learning using Nonequilibrium Thermodynamics</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2020</td><td>Denoising Diffusion Probabilistic Models</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2021</td><td>Score-Based Generative Modeling through Stochastic Differential Equations</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/8.html">link</a></td></tr>
</tbody></table>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P22</p>
<h1 id="score-based-generative-modeling-with-differential-equations"><a class="header" href="#score-based-generative-modeling-with-differential-equations">Score-based Generative Modeling with Differential Equations</a></h1>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2021</td><td>Score-Based Generative Modeling through Stochastic Differential Equations</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/8.html">link</a></td></tr>
</tbody></table>
<p>P26</p>
<h2 id="ddpm-vs-stochastic-differential-equation"><a class="header" href="#ddpm-vs-stochastic-differential-equation">DDPM VS Stochastic Differential Equation</a></h2>
<blockquote>
<p>ğŸ” <a href="https://caterpillarstudygroup.github.io/mathematics_basic_for_ML/NumericalComputation/ODE_SDE.html">SDE</a>
<img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-26.png" alt="" /> </p>
</blockquote>
<blockquote>
<p>âœ… DDPM æ˜¯åœ¨æ—¶é—´ä¸Šåšäº†ç¦»æ•£åŒ–çš„ SDEï¼</p>
</blockquote>
<p>P27</p>
<h3 id="forward-diffusion-process-as-stochastic-differential-equation"><a class="header" href="#forward-diffusion-process-as-stochastic-differential-equation">Forward Diffusion Process as Stochastic Differential Equation</a></h3>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-27.png" alt="" /> </p>
<blockquote>
<p>âœ… drift term ä½¿ \( \mathbf{x} _ t\) è¶‹å‘äº Origin.<br />
âœ… Origin æˆ‘ç†è§£ä¸º \( \vec{0} \) å‘é‡çš„æ„æ€ã€‚<br />
âœ… \( \mathbf{x} _ t\) æœ€ç»ˆè¶‹å‘äº std normal.</p>
</blockquote>
<p>P29</p>
<h3 id="the-generative-reverse-stochastic-differential-equation"><a class="header" href="#the-generative-reverse-stochastic-differential-equation">The Generative Reverse Stochastic Differential Equation</a></h3>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-29.png" alt="" /> </p>
<blockquote>
<p>ğŸ” <u>Anderson, in Stochastic Processes and their Applications, 1982</u></p>
</blockquote>
<blockquote>
<p>âœ… \(q _ t(\cdot )\) æè¿° \(t\) æ—¶åˆ»çš„åˆ†å¸ƒã€‚<br />
âœ… \(q _ t(\mathbf{x} _ t)\) ä¸º \(\mathbf{x} _ t\) åœ¨ \(q _ t\) åˆ†å¸ƒä¸­çš„æ¦‚ç‡ã€‚<br />
âœ… Generative çš„å…³é”®æ˜¯æ‹Ÿåˆ score funchonï¼</p>
</blockquote>
<p><strong>But how to get the score function</strong> \(\nabla \mathbf{x} _t \log q_t(\mathbf{x} _t)\)?</p>
<p>P32</p>
<h2 id="score-matching"><a class="header" href="#score-matching">Score Matching</a></h2>
<p>NaÃ¯ve idea, learn model for the score function by direct regression?</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-32.png" alt="" /> </p>
<blockquote>
<p>âœ… ç›´æ¥ç”¨ä¸€ä¸ªç½‘ç»œæ‹Ÿåˆ score functionï¼</p>
</blockquote>
<p><strong>But</strong> \(\nabla \mathbf{x} _t \log q_t(\mathbf{x} _t)\) <strong>(score of the</strong> <em><strong>marginal diffused density</strong></em> \(q_t(\mathbf{x} _t)\)<strong>) is not tractable!</strong></p>
<blockquote>
<p>âœ… å­˜åœ¨çš„é—®é¢˜ï¼šåªèƒ½ sample from \(q_t\)ï¼Œä½†æ²¡æœ‰ \(q_t\) çš„ close form.</p>
</blockquote>
<p><u>Vincent, â€œA Connection Between Score Matching and Denoising Autoencodersâ€, Neural Computation, 2011</u></p>
<p><u>Song and Ermon, â€œGenerative Modeling by Estimating Gradients of the Data Distributionâ€, NeurIPS, 2019</u></p>
<p>P33</p>
<h3 id="denoising-score-matching"><a class="header" href="#denoising-score-matching">Denoising Score Matching</a></h3>
<p>Instead, diffuse individual data points \(\mathbf{x}_0\). Diffused \(q_t(\mathbf{x}_t|\mathbf{x}_0)\) <em><strong>is</strong></em> tractable!</p>
<blockquote>
<p>ğŸ” <u>Vincent, in Neural Computation, 2011</u></p>
</blockquote>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-33-1.png" alt="" /> </p>
<blockquote>
<p>â“ \(\gamma _ t\) å’Œ \(\sigma\) æ€ä¹ˆå®šä¹‰ï¼Ÿ ç­”ï¼šè§ä¸Šä¸€é¡µDDPMçš„æ¨å¯¼ã€‚</p>
</blockquote>
<p>å› æ­¤<strong>Denoising Score Matching</strong>çš„ç›®æ ‡å‡½æ•°å˜ä¸º:</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-33-2.png" alt="" /> </p>
<p><strong>After expectations</strong>, \(\mathbf{s} _ \theta (\mathbf{x} _ t,t)\approx \nabla _ {\mathbf{x} _ t}\log q _ t(\mathbf{x} _ t)\)<strong>!</strong></p>
<blockquote>
<p>ğŸ” <u>Song and Ermon, NeurIPS, 2019</u></p>
</blockquote>
<blockquote>
<p>âœ… æœ€å \(\mathbf{s} _ \theta (\mathbf{x} _ t,t)\) å­¦åˆ°çš„æ˜¯æ‰€æœ‰ \(\mathbf{x} _ 0\) å¯¹åº”çš„ score çš„å‡å€¼ã€‚</p>
</blockquote>
<blockquote>
<p>âœ… ç»“æœå‘ç°æ—¶é—´ç¦»æ•£çš„ diffusion model(DDPM) å’Œæ—¶é—´è¿ç»­çš„ diffusion model(SDE),å…¶ç›®æ ‡å‡½æ•°æ˜¯ä¸€è‡´çš„ï¼Œä¸”ä¸¤ä¸ªç‰ˆæœ¬å¯ä»¥äº’ç›¸è½¬åŒ–ã€‚</p>
</blockquote>
<p>$$
\min_ {\mathbf{\theta}  } \mathbb{E} _ {t\sim u(0,T)}\mathbb{E} _ {\mathbf{x} _ 0\sim q_ 0(\mathbf{x} _ 0)}\mathbb{E} _{\epsilon \sim \mathcal{N}(\mathbf{0,I} ) }\frac{1}{\sigma ^2_t} ||\epsilon -\epsilon _ \theta (\mathbf{x} _ t,t)||^2_2 
$$</p>
<p>P35</p>
<h2 id="different-parameterizations"><a class="header" href="#different-parameterizations">Different Parameterizations</a></h2>
<blockquote>
<p>ğŸ” Karras et al., <u>&quot;Elucidating the Design Space of Diffusion-Based Generative Models&quot;,</u> NeurIPS 2022 <a href="https://caterpillarstudygroup.github.io/ReadPapers/9.html">link</a></p>
</blockquote>
<blockquote>
<p>âœ… è°ƒå‚å¯¹ç”Ÿæˆè´¨é‡å½±å“å¾ˆå¤§ã€‚</p>
</blockquote>
<p>P36</p>
<h2 id="synthesis-with-sde-vs-ode"><a class="header" href="#synthesis-with-sde-vs-ode">Synthesis with SDE vs. ODE</a></h2>
<p><strong>Generative Reverse Diffusion SDE (stochastic):</strong></p>
<p>$$
d\mathbf{x} _ t=-\frac{1}{2} \beta (t)[\mathbf{x} _ t+2s_ \theta (\mathbf{x} _ t,t)]dt+\sqrt{\beta (t)} d\varpi _ t
$$</p>
<p><strong>Generative Probability Flow ODE (deterministic):</strong></p>
<p>$$
d\mathbf{x} _ t=-\frac{1}{2} \beta (t)[\mathbf{x} _ t+s_ \theta (\mathbf{x} _ t,t)]dt
$$</p>
<blockquote>
<p>âœ… <a href="https://caterpillarstudygroup.github.io/ReadPapers/9.html">Song et al., ICLR, 2021</a>è¡¨æ˜ï¼Œå¯ä»¥æŠŠ SDE æ¨¡å‹è½¬æ¢ä¸ºODEæ¨¡å‹ã€‚åªéœ€è¦å¯¹sampleè¿‡ç¨‹è¿›è¡Œå…¬å¼ä¿®æ”¹å³å¯ã€‚æ¯ä¸ªå™ªå£°å¯¹åº”ç‰¹å®šçš„è¾“å‡ºã€‚</p>
</blockquote>
<p>P37</p>
<h4 id="diffusion-models-as-neural-odes"><a class="header" href="#diffusion-models-as-neural-odes">Diffusion Models as Neural ODEs</a></h4>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-37.png" alt="" /></p>
<p>ä½¿ç”¨ODEçš„sampleå…¬å¼æœ‰ä»¥ä¸‹å¥½å¤„ï¼š</p>
<ul>
<li>ODE æ¨æ–­ï¼Œå¯ä»¥ä½¿ç”¨æˆç†Ÿçš„ ODE solver è¿›è¡Œ sample åŠ é€Ÿã€‚</li>
<li><strong>Deterministic encoding and generation</strong> (semantic image interpolation, etc.)</li>
<li><strong>Log-likelihood computation</strong> (instantaneous change of variables):</li>
</ul>
<blockquote>
<p>â“ ç¬¬ä¸‰æ¡æ²¡å¬æ‡‚ï¼ŒæŠŠ model å½“æˆåŸºäºæ•°æ®çš„ ODE æ¥ç”¨ï¼Ÿ</p>
</blockquote>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P38</p>
<h1 id="accelerated-sampling"><a class="header" href="#accelerated-sampling">Accelerated Sampling</a></h1>
<p>P39</p>
<h2 id="the-generative-learning-trilemma"><a class="header" href="#the-generative-learning-trilemma">The generative learning trilemma</a></h2>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-39.png" alt="" /> </p>
<blockquote>
<p>ğŸ” <u>Tackling the Generative Learning Trilemma with Denoising Diffusion GANs, ICLR 2022</u> </p>
</blockquote>
<p>å…¶ä¸­Diffusion basedç”Ÿæˆæ¨¡å‹çš„ä¸»è¦é—®é¢˜æ˜¯ç”Ÿæˆé€Ÿåº¦æ…¢ï¼Œå› æ­¤éœ€è¦åœ¨ä¿æŒé«˜é‡‡æ ·è´¨é‡å’Œå¤šæ ·æ€§çš„å‰æä¸‹ï¼Œé’ˆå¯¹é‡‡æ ·é€Ÿåº¦æ…¢çš„é—®é¢˜è¿›è¡ŒåŠ é€Ÿã€‚</p>
<p>P41</p>
<h2 id="acceleration-techniques"><a class="header" href="#acceleration-techniques">Acceleration Techniques</a></h2>
<ul>
<li>Advanced ODE/SDE Solvers</li>
<li>Distillation Techniques</li>
<li>Low-dim. Diffusion Processes</li>
<li>Advanced Diffusion Processes</li>
</ul>
<p>P42</p>
<h1 id="advanced-odesde-solvers"><a class="header" href="#advanced-odesde-solvers">Advanced ODE/SDE Solvers</a></h1>
<blockquote>
<p>âœ… ODE å®ç° std normal åˆ†å¸ƒä¸çœŸå®æ•°æ®åˆ†å¸ƒä¹‹é—´çš„æ˜ å°„ã€‚</p>
</blockquote>
<p>P43</p>
<h2 id="generative-odes"><a class="header" href="#generative-odes">Generative ODEs</a></h2>
<p>Solve ODEs with as little function evaluations as possible</p>
<p>$$
dx=\epsilon _\theta (x,t)dt
$$</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-43.png" alt="" /> </p>
<ul>
<li>ä¸€é˜¶æ–¹æ³•ï¼ˆEuler æ–¹æ³•ï¼‰ï¼šæ¯ä¸ªæ—¶é—´æ­¥ç®€åŒ–ä¸ºçº¿æ€§è¿‡ç¨‹ã€‚å½“ step è¾ƒå¤§æ—¶ï¼Œä¼šä¸ GT æœ‰è¾ƒå¤§çš„åç¦»ã€‚</li>
</ul>
<p>P44</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-44.png" alt="" /> </p>
<ul>
<li>é«˜é˜¶æ–¹æ³•
P45<br />
<img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-45.png" alt="" /> </li>
</ul>
<p>P46</p>
<h2 id="æ‰©æ•£æ¨¡å‹-odesde-æ±‚è§£å™¨çš„ç›¸å…³å·¥ä½œ"><a class="header" href="#æ‰©æ•£æ¨¡å‹-odesde-æ±‚è§£å™¨çš„ç›¸å…³å·¥ä½œ">æ‰©æ•£æ¨¡å‹ ODE/SDE æ±‚è§£å™¨çš„ç›¸å…³å·¥ä½œ</a></h2>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/08-14.png" alt="" /></p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td><strong>2</strong></td><td>2021</td><td>Denoising Diffusion Implicit Models (DDIM)</td><td>âœ… DDIMï¼šå¯ä»¥ç›´æ¥ä» \(t_2\) å»å™ªåˆ° \(t_1\). <br> âœ… æŠŠ \(x_t\) å»æ‰ä¸€ä¸ª nolse ä¹‹åï¼Œä¸æ˜¯ sample å¦ä¸€ä¸ªnoiseï¼Œè€Œæ˜¯æŠŠåŸæ¥çš„ noise ä¹˜ä»¥ä¸€ä¸ªç³»æ•°å†åŠ å›å»ã€‚</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/2.html">link</a></td></tr>
<tr><td></td><td>2021</td><td>Score-Based Generative Modeling through Stochastic Differential Equations</td><td>Runge-Kutta adaptive step-size ODE solver</td><td></td><td></td></tr>
<tr><td></td><td>2021</td><td>Gotta Go Fast When Generating Data with Score-Based Models</td><td>Higher-Order adaptive step-size SDE solver</td><td></td><td></td></tr>
<tr><td></td><td>2021</td><td>Denoising Diffusion Implicit Models</td><td>Reparametrized, smoother ODE</td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>gDDIM: Generalized denoising diffusion implicit models</td><td>Reparametrized, smoother ODE</td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>Pseudo Numerical Methods for Diffusion Models on Manifolds</td><td>Higher-Order ODE solver with linear multistepping</td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>Fast Sampling of Diffusion Models with Exponential Integrator</td><td>Exponential ODE Integrators</td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps</td><td>Exponential ODE Integrators</td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models</td><td>Exponential ODE Integrators</td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>Elucidating the Design Space of Diffusion-Based Generative Models</td><td>Higher-Order ODE solver with Heunâ€™s Method</td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>Parallel Sampling of Diffusion Model</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>A Geometric Perspective on Diffusion Models</td><td></td><td></td><td></td></tr>
</tbody></table>
<blockquote>
<p>âœ… è¿™äº›solverså¯ä»¥ä»¥plug-inçš„æ–¹å¼ä½¿ç”¨ï¼Œä¸”é€šå¸¸èƒ½æ¯”DDPMæ›´å¿«æ”¶æ•›ã€‚</p>
</blockquote>
<h1 id="distillation-techniques"><a class="header" href="#distillation-techniques">Distillation Techniques</a></h1>
<p>P48</p>
<h2 id="ode-distillation"><a class="header" href="#ode-distillation">ODE Distillation</a></h2>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-48.png" alt="" /> </p>
<p>Can we train a neural network to directly predict \(\mathbf{x} _{{t}'} \) given \(\mathbf{x} _t\)?</p>
<blockquote>
<p>âœ… \(\mathbf{x} _{{t}'} \)ä¸\(\mathbf{x} _t\)çš„å…³ç³»æ˜¯ç¡®å®šçš„ã€‚</p>
</blockquote>
<p>P49</p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2022</td><td><strong>Progressive distillation</strong> for fast sampling of diffusion models</td><td>è’¸é¦</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/1.html">link</a></td></tr>
<tr><td></td><td>2023</td><td>On Distillation of Guided Diffusion Models</td><td><strong>Award Candidate</strong></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/3.html">link</a></td></tr>
<tr><td></td><td>2023</td><td><strong>Consistency Models</strong></td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/7.html">link</a></td></tr>
</tbody></table>
<p>P52</p>
<h2 id="sde-distillation"><a class="header" href="#sde-distillation">SDE Distillation</a></h2>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-52.png" alt="" /> </p>
<p>Can we train a neural network to directly predict <strong>distribution of</strong> \(\mathbf{x} _ {{t}'} \) given \(\mathbf{x} _ t \) ?</p>
<blockquote>
<p>âœ… \(\mathbf{x} _ t\) ä¸ \( \mathbf{x} _ {{t}' }\) æ²¡æœ‰å¿…ç„¶çš„è”ç³»ï¼Œå¾—åˆ°çš„æ˜¯ \( \mathbf{x} _ {{t}' }\) çš„åˆ†å¸ƒã€‚</p>
</blockquote>
<p>ä½†Normal assumption in denoising distribution holds only for small step</p>
<blockquote>
<p>âœ… ä» \(t\) ä¸ \({t}'\) çš„å·®è·è¿‡å¤§æ—¶ï¼Œnormal åˆ†å¸ƒä¸è¶³ä»¥è¡¨è¾¾ \(q(\mathbf{x} _ {{t}'}ï½œ\mathbf{x} _ t)\).</p>
</blockquote>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-53.png" alt="" /> </p>
<p>å› æ­¤<strong>Requires more complicated functional approximators!</strong>ï¼Œä¾‹å¦‚GANæˆ–energy-basedã€‚</p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2022</td><td>Tackling the Generative Learning Trilemma with Denoising Diffusion GANs</td><td>GAN</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/10.html">link</a></td></tr>
<tr><td></td><td>2021</td><td>Learning energy-based models by diffusion recovery likelihood</td><td>Energy-based models</td><td></td><td></td></tr>
</tbody></table>
<p>P54</p>
<h2 id="training-based-sampling-techniques"><a class="header" href="#training-based-sampling-techniques">Training-based Sampling Techniques</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2021</td><td>Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed</td><td>Knowledge distillation</td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality</td><td>Learned Samplers</td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>Fast Sampling of Diffusion Models via Operator Learning</td><td>Neural Operators</td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>Wavelet Diffusion Models Are Fast and Scalable Image Generators</td><td>Wavelet Diffusion Models</td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>GENIE: Higher-Order Denoising Diffusion Solvers</td><td>Distilled ODE Solvers</td><td></td><td></td></tr>
</tbody></table>
<p>P56</p>
<h1 id="low-dim-diffusion-process"><a class="header" href="#low-dim-diffusion-process">Low-dim Diffusion Process</a></h1>
<h2 id="cascaded-generation"><a class="header" href="#cascaded-generation">Cascaded Generation</a></h2>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-56.png" alt="" /> </p>
<p>Cascaded Diffusion Models outperform Big-GAN in FID and IS and VQ-VAE2 in Classification Accuracy Score.</p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2021</td><td>Cascaded Diffusion Models for High Fidelity Image Generation</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/3.html">link</a></td></tr>
<tr><td></td><td>2022</td><td>Hierarchical Text-Conditional Image Generation with CLIP Latents</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding</td><td></td><td></td><td></td></tr>
</tbody></table>
<p>P57</p>
<h2 id="latent-diffusion-models"><a class="header" href="#latent-diffusion-models">Latent Diffusion Models</a></h2>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/08-21.png" alt="" /> </p>
<h3 id="main-idea"><a class="header" href="#main-idea">Main Ideaï¼š</a></h3>
<p><strong>Variational autoencoder + score-based prior</strong></p>
<p>Encoder maps the input data to an embedding space<br />
Denoising diffusion models are applied in the latent space</p>
<p>P58</p>
<h3 id="advantages"><a class="header" href="#advantages">Advantages:</a></h3>
<p>(1) The distribution of latent embeddings close to Normal distribution \(\to \) <em><strong>Simpler denoising, Faster synthesis</strong></em>!<br />
(2) Latent space \(\to \) <em><strong>More expressivity and flexibility in design!</strong></em><br />
(3) Tailored Autoencoders \(\to \) <em><strong>More expressivity, Application to any data type (graphs, text, 3D data, etc.)!</strong></em></p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2021</td><td>Score-based generative modeling in latent space</td><td><strong>End-to-End</strong> Training objective<br><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-59.png" alt="" /> <br> âœ… è¿™ç¯‡æ–‡ç« å¯¹ VAE å’Œ diffusion ä¸€èµ·è¿›è¡Œè®­ç»ƒï¼Œæ–‡ç« çš„åˆ›æ–°ç‚¹æ˜¯ï¼Œåˆ©ç”¨ score matching ä¸­çš„ä¿¡æ¯æ¥è®¡ç®— cross entropy.</td><td></td><td></td></tr>
<tr><td>45</td><td>2022</td><td>High-Resolution Image Synthesis with Latent Diffusion Models</td><td><strong>Two-stage</strong> Trainingï¼Œå…ˆè®­E&amp;Dï¼Œå†è®­diffusionã€‚æ¯æ¬¡éœ€è¦è®­ç»ƒçš„ç½‘ç»œéƒ½ä¸å¤§ã€‚</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/45.html">link</a></td></tr>
<tr><td></td><td>2021</td><td>D2C: Diffusion-Denoising Models for Few-shot Conditional Generation</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>Score-Guided Intermediate Layer Optimization: Fast Langevin Mixing for Inverse Problems</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>Dimensionality-Varying Diffusion Process</td><td></td><td></td><td></td></tr>
</tbody></table>
<p>The efficiency and expressivity of latent diffusion models + open-source access fueled a large body of work in the community</p>
<h1 id="advanced-diffusion-models"><a class="header" href="#advanced-diffusion-models">Advanced Diffusion Models</a></h1>
<blockquote>
<p>âœ… è¿™ä¸€éƒ¨åˆ†æ²¡æœ‰è®²</p>
</blockquote>
<p>P63</p>
<h2 id="ode-interpretation"><a class="header" href="#ode-interpretation">ODE interpretation</a></h2>
<p>æŠŠODEçœ‹ä½œæ˜¯Deterministic generative process</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-63.png" alt="" /> </p>
<ul>
<li>DDIM sampler can be considered as an integration rule of the following ODE:</li>
</ul>
<p>$$
d\mathbf{\bar{x} } (t)=\epsilon ^{(t)} _ \theta(\frac{\mathbf{\bar{x} } (t)}{\sqrt{\eta ^2+1}} )d\eta (t); \mathbf{\bar{x} } =\mathbf{x} / \sqrt{\bar{a} },\eta = \sqrt{1-\bar{a}} / \sqrt{\bar{a } }
$$</p>
<ul>
<li>
<p>Karras et al. argue that the ODE of DDIM is favored, as the tangent of the solution trajectory always points 
towards the denoiser output.</p>
</li>
<li>
<p>This leads to largely linear solution trajectories with low curvature Ã  Low curvature means less truncation 
errors accumulated over the trajectories. </p>
</li>
</ul>
<blockquote>
<p>ğŸ” <u>Song et al., â€œDenoising Diffusion Implicit Modelsâ€, ICLR 2021.</u><br />
ğŸ” <u>Karras et al., â€œElucidating the Design Space of Diffusion-Based Generative Modelsâ€, arXiv 2022.</u></p>
</blockquote>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2022</td><td><strong>Progressive distillation</strong> for fast sampling of diffusion models</td><td>é€šè¿‡ä¿®æ”¹å‚æ•°åŒ–æ–¹å¼æ¥æå‡â€œå‡å°‘sampling stepsâ€çš„ç¨³å®šæ€§ã€‚</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/1.html">link</a></td></tr>
</tbody></table>
<p>P64</p>
<h2 id="momentum-based-diffusion"><a class="header" href="#momentum-based-diffusion">â€œMomentum-basedâ€ diffusion</a></h2>
<h5 id="introduce-a-velocity-variable-and-run-diffusion-in-extended-space"><a class="header" href="#introduce-a-velocity-variable-and-run-diffusion-in-extended-space">Introduce a velocity variable and run diffusion in extended space</a></h5>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-64.png" alt="" /> </p>
<p><u>Dockhorn et al., â€œScore-Based Generative Modeling with Critically-Damped Langevin Diffusionâ€, ICLR 2022.</u></p>
<p>P65</p>
<h2 id="additional-reading"><a class="header" href="#additional-reading">Additional Reading</a></h2>
<ul>
<li>SchrÃ¶dinger Bridge:</li>
</ul>
<blockquote>
<p>ğŸ” Bortoli et al., <u>&quot;Diffusion SchrÃ¶dinger Bridge&quot;,</u> NeurIPS 2021<br />
ğŸ” Chen et al., <u>â€œLikelihood Training of SchrÃ¶dinger Bridge using Forward-Backward SDEs Theoryâ€, </u>ICLR 2022</p>
</blockquote>
<ul>
<li>Diffusion Processes on Manifolds:</li>
</ul>
<blockquote>
<p>ğŸ” Bortoli et al., <u>&quot;Riemannian Score-Based Generative Modelling&quot;, </u>NeurIPS 2022</p>
</blockquote>
<ul>
<li>Cold Diffusion:</li>
</ul>
<blockquote>
<p>ğŸ” Bansal et al., <u>&quot;Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise&quot;, </u>arXiv 2022</p>
</blockquote>
<ul>
<li>Diffusion for Corrupted Data:</li>
</ul>
<blockquote>
<p>ğŸ” Daras et al., <u>&quot;Soft Diffusion: Score Matching for General Corruptions&quot;, </u>TMLR 2023<br />
ğŸ” Delbracio and Milanfar, <u>&quot;Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration&quot;, </u>arXiv 2023<br />
ğŸ” Luo et al., <u>&quot;Image Restoration with Mean-Reverting Stochastic Differential Equations&quot;, </u>ICML 2023<br />
ğŸ” Liu et al., <u>â€œI2SB: Image-to-Image SchrÃ¶dinger Bridgeâ€, </u>ICML 2023</p>
</blockquote>
<ul>
<li>Blurring Diffusion Process:</li>
</ul>
<blockquote>
<p>ğŸ” Hoogeboom and Salimans, <u>&quot;Blurring Diffusion Models&quot;, </u>ICLR 2023<br />
ğŸ” Rissanen et al, <u>â€œGenerative Modelling With Inverse Heat Dissipationâ€, </u>ICLR 2023</p>
</blockquote>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P66</p>
<h1 id="conditional-generation-and-guidance"><a class="header" href="#conditional-generation-and-guidance">Conditional Generation and Guidance</a></h1>
<p>P67</p>
<blockquote>
<p>âœ… é€šå¸¸éœ€è¦çš„æ˜¯ç‰¹å®šçš„ç”Ÿæˆï¼Œè€Œä¸æ˜¯éšæ„çš„ç”Ÿæˆã€‚å› æ­¤éœ€è¦é€šè¿‡controlå¼•å…¥ç‰¹å®šçš„éœ€æ±‚ã€‚</p>
</blockquote>
<p>ä»¥ä¸‹æ˜¯æ–‡ç”Ÿå›¾çš„ä¾‹å­ï¼š</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-67.png" alt="" /> </p>
<p><u>Ramesh et al., â€œHierarchical Text-Conditional Image Generation with CLIP Latentsâ€, arXiv 2022.</u><br />
<u>Saharia et al., â€œPhotorealistic Text-to-Image Diffusion Models with Deep Language Understandingâ€, arXiv 2022.</u></p>
<p>P68</p>
<h2 id="conditioning-and-guidance-techniques"><a class="header" href="#conditioning-and-guidance-techniques">Conditioning and Guidance Techniques</a></h2>
<p>Explicit Conditions<br />
Classifier Guidance<br />
Classifier-free Guidance</p>
<p>P69</p>
<h1 id="explicit-conditions"><a class="header" href="#explicit-conditions">Explicit Conditions</a></h1>
<p>P70<br />
Conditional sampling can be considered as training \(p(\mathbf{x} |\mathbf{y} )\) where \(\mathbf{y}\) is the input conditioning (e.g., text) and \(\mathbf{x}\) is generated output (e.g., image)</p>
<p>Train the score model for \(\mathbf{x}\) conditioned on \(\mathbf{y}\) using:</p>
<p>$$
\mathbb{E} _ {(\mathbf{x,y} )\sim P\mathrm{data} (\mathbf{x,y} )}\mathbb{E} _ {\epsilon \sim \mathcal{N}(\mathbf{0,I} ) }\mathbb{E} _{t\sim u[0,T]}||\epsilon _ \theta (\mathbf{x} _ t,t;\mathbf{y} )- \epsilon ||^2_2 
$$</p>
<p>The conditional score is simply a U-Net with \(\mathbf{x}_t\) and \(\mathbf{y}\) together in the input.</p>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-70.png" alt="" /> </p>
<blockquote>
<p>âœ… éœ€è¦ \((xï¼Œy)\) çš„ pair data.</p>
</blockquote>
<p>P71</p>
<h1 id="classifier-guidance"><a class="header" href="#classifier-guidance">Classifier Guidance</a></h1>
<p>P72</p>
<h2 id="bayes-rule-in-action"><a class="header" href="#bayes-rule-in-action">Bayesâ€™ Rule in Action</a></h2>
<p><img src="diffusion-tutorial-part/Fundamentals/../../assets/D1-72.png" alt="" /> </p>
<blockquote>
<p>âœ… \(p(y)\) ä¸ \(\mathbf{x} _ t\) æ— å…³ï¼Œå› æ­¤å¯ä»¥å»æ‰ã€‚</p>
</blockquote>
<h2 id="è®­ç»ƒæ–¹æ³•"><a class="header" href="#è®­ç»ƒæ–¹æ³•">è®­ç»ƒæ–¹æ³•</a></h2>
<blockquote>
<p>âœ… ç¬¬ä¸€æ­¥ï¼šéœ€è¦ä¸€ä¸ªè®­å¥½çš„p(x)çš„ diffusion model ã€‚<br />
âœ… ç¬¬äºŒæ­¥ï¼šè®­ç»ƒä¸€ä¸ªåˆ†ç±»ç½‘ç»œï¼Œè¾“å…¥xtèƒ½å¤Ÿæ­£ç¡®åœ°é¢„æµ‹æ§åˆ¶æ¡ä»¶ï¼ˆyä¸ä¸€å®šæ˜¯ç¦»æ•£çš„ç±»åˆ«ï¼‰ã€‚<br />
âœ… ç¬¬ä¸‰æ­¥ï¼šå–ç¬¬äºŒæ­¥çš„æ¢¯åº¦ï¼Œç”¨ä¸€å®šçš„æƒé‡\(w \)ç»“åˆåˆ°ç¬¬ä¸€æ­¥çš„forwardè¿‡ç¨‹ä¸­ã€‚\(w \)å†³å®šåˆ†ç±»å™¨çš„å½±å“åŠ›ã€‚</p>
</blockquote>
<blockquote>
<p>âœ… åªéœ€è¦éƒ¨åˆ†pair dataå’Œå¤§é‡çš„épair dataã€‚ä½†éœ€è¦å•ç‹¬è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨ã€‚</p>
</blockquote>
<h2 id="ç›¸å…³è®ºæ–‡-1"><a class="header" href="#ç›¸å…³è®ºæ–‡-1">ç›¸å…³è®ºæ–‡</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2021</td><td>Score-Based Generative Modeling through Stochastic Differential Equations</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/14.html">link</a></td><td></td><td></td></tr>
<tr><td></td><td>2021</td><td>Controllable and Compositional Generation with Latent-Space Energy-Based Models</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2021</td><td>Diffusion models beat GANs on image synthesis</td><td></td><td></td><td></td></tr>
</tbody></table>
<h1 id="classifier-free-guidance"><a class="header" href="#classifier-free-guidance">Classifier-free Guidance</a></h1>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2021</td><td>Classifier-Free Diffusion Guidance</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/6.html">link</a></td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><p>P76</p>
<h1 id="summary"><a class="header" href="#summary">Summary</a></h1>
<p>We reviewed diffusion fundamentals in 4 parts:</p>
<ul>
<li>Discrete-time diffusion models</li>
<li>Continuous-time diffusion models</li>
<li>Accelerated sampling from diffusion models</li>
<li>Guidance and conditioning.</li>
</ul>
<p>Next, we will review different applications and use cases of diffusion models after a break.</p>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="architecture"><a class="header" href="#architecture">Architecture</a></h1>
<p>P5</p>
<h2 id="u-net-based-diffusion-architecture"><a class="header" href="#u-net-based-diffusion-architecture">U-Net Based Diffusion Architecture</a></h2>
<h3 id="u-net-architecture"><a class="header" href="#u-net-architecture">U-Net Architecture</a></h3>
<p><img src="diffusion-tutorial-part/../assets/D2-5-1.png" alt="" /> </p>
<blockquote>
<p>âœ… U-Netçš„æ˜¯Large Scale Image Diffusion Modelä¸­æœ€å¸¸ç”¨çš„backboneã€‚</p>
</blockquote>
<blockquote>
<p>ğŸ” Ronneberger et al., <u>â€œU-Net: Convolutional Networks for Biomedical Image Segmentationâ€, </u>MICCAI 2015</p>
</blockquote>
<h3 id="pipeline"><a class="header" href="#pipeline">Pipeline</a></h3>
<p><img src="diffusion-tutorial-part/../assets/D2-5-2.png" alt="" /> </p>
<blockquote>
<p>âœ… åŒ…å«Inputã€U-Net backboneã€Conditionã€‚<br />
âœ… Condition é€šå¸¸ç”¨ Concat æˆ– Cross attention çš„æ–¹å¼ä¸ Content ç›¸ç»“åˆã€‚</p>
</blockquote>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>45</td><td>2022</td><td>High-Resolution Image Synthesis with Latent Diffusion Models</td><td><strong>Stable Diffusion</strong>, U-Net Based Diffusion Architecture<br>âœ… (1)ï¼šåœ¨ latent space ä¸Šå·¥ä½œ<br> âœ… (2)ï¼šå¼•å…¥å¤šç§ conditionï¼</td><td>UNet</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/45.html">link</a></td></tr>
<tr><td></td><td>2022</td><td>Photorealistic text-to-image diffusion models with deep language understanding</td><td>Imagen</td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>ediffi: Text-to-image diffusion models with an ensemble of expert denoiser</td><td>eDiff-I</td><td></td><td></td></tr>
</tbody></table>
<p>P7</p>
<h2 id="transformer-architecture"><a class="header" href="#transformer-architecture">Transformer Architecture</a></h2>
<h3 id="vision-transformervit"><a class="header" href="#vision-transformervit">Vision Transformer(ViT)</a></h3>
<p><img src="diffusion-tutorial-part/../assets/D2-7-1.png" alt="" /> </p>
<p>Dosovitskiy et al., <u>â€œAn image is worth 16x16 words: Transformers for image recognition at scaleâ€, </u>ICLR 2021</p>
<h3 id="pipeline-1"><a class="header" href="#pipeline-1">Pipeline</a></h3>
<p><img src="diffusion-tutorial-part/../assets/D2-7-2.png" alt="" /> </p>
<blockquote>
<p>âœ… ç‰¹ç‚¹ï¼š<br />
âœ… 1. æŠŠ image patches å½“ä½œ token.<br />
âœ… 2. åœ¨ Shallow layer ä¸ deep layer ä¹‹é—´å¼•å…¥ long skip connection.</p>
</blockquote>
<p>Bao et al.,<u> &quot;All are Worth Words: a ViT Backbone for Score-based Diffusion Models&quot;, </u>arXiv 2022</p>
<p>P8</p>
<h3 id="application"><a class="header" href="#application">Application</a></h3>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2022</td><td>Scalable Diffusion Models with Transformers</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>simple diffusion: End-to-end diffusion for high resolution images</td><td></td><td></td><td></td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><p>P9</p>
<h1 id="å›¾åƒç¼–è¾‘"><a class="header" href="#å›¾åƒç¼–è¾‘">å›¾åƒç¼–è¾‘</a></h1>
<p>P10</p>
<h2 id="gaussian-noiseæ–¹æ³•"><a class="header" href="#gaussian-noiseæ–¹æ³•">Gaussian Noiseæ–¹æ³•</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2022</td><td>SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/23.html">link</a></td><td></td></tr>
</tbody></table>
<h2 id="ddim-inversionæ–¹æ³•"><a class="header" href="#ddim-inversionæ–¹æ³•">DDIM Inversionæ–¹æ³•</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>23</td><td>2023</td><td>Dual diffusion implicit bridges for image-to-image translation</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/23.html">link</a></td><td></td></tr>
<tr><td>24</td><td>2023</td><td>DiffEdit: Diffusion-based semantic image editing with mask guidance</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/24.html">link</a></td><td></td></tr>
<tr><td>25</td><td>2023</td><td>Imagic: Text-Based Real Image Editing with Diffusion Models</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/25.html">link</a></td><td></td></tr>
</tbody></table>
<h2 id="attention-based-æ–¹æ³•"><a class="header" href="#attention-based-æ–¹æ³•">Attention based æ–¹æ³•</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2023</td><td>Prompt-to-Prompt Image Editing with Cross-Attention Control</td><td>é€šè¿‡æ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ä¸­çš„ attention mapsè¿›è¡Œå›¾åƒç¼–è¾‘</td><td>attentionæ§åˆ¶</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/20.html">link</a></td></tr>
<tr><td></td><td>2023</td><td>NULL-text Inversion for Editing Real Images Using Guided Diffusion Models</td><td>é’ˆå¯¹çœŸå®å›¾åƒï¼ˆéç”Ÿæˆå›¾åƒï¼‰çš„ç¼–è¾‘ï¼Œä»¥<a href="https://caterpillarstudygroup.github.io/ReadPapers/6.html">CFG</a>ä¸ºåŸºç¡€ï¼Œfix conditionåˆ†æ”¯ï¼Œä¼˜åŒ–æ— conditionåˆ†æ”¯ï¼Œä½¿å…¶embeddingå‘conditionåˆ†æ”¯çš„embeddingé è¿‘</td><td>attentionæ§åˆ¶</td><td></td></tr>
<tr><td></td><td></td><td>Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation</td><td>åœ¨ä¸Šä¸€ç¯‡çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡attentionæ³¨å…¥çš„æ–¹å¼åŠ é€Ÿä¸Šè¿°æµç¨‹</td><td>attentionæ§åˆ¶</td><td></td></tr>
<tr><td></td><td>2023</td><td>InstructPix2Pix: Learning to Follow Image Editing Instructions</td><td>åœ¨ä¸Šä¸€ç¯‡çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡attentionæ³¨å…¥çš„æ–¹å¼åŠ é€Ÿä¸Šè¿°æµç¨‹</td><td>attentionæ§åˆ¶</td><td></td></tr>
</tbody></table>
<p>P32</p>
<h1 id="ç‰¹å®šå¯¹è±¡å®šåˆ¶åŒ–çš„å›¾åƒç”Ÿæˆ"><a class="header" href="#ç‰¹å®šå¯¹è±¡å®šåˆ¶åŒ–çš„å›¾åƒç”Ÿæˆ">ç‰¹å®šå¯¹è±¡å®šåˆ¶åŒ–çš„å›¾åƒç”Ÿæˆ</a></h1>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>52</td><td>2024</td><td>Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models</td><td>å¤šä¸ªç‰¹å®šå¯¹è±¡çš„å›¾åƒç”Ÿæˆï¼Œè®©å¤šä¸ªç‰¹å®šçš„å¯¹è±¡ç”Ÿæˆåˆ°ä¸€å¼ å›¾åƒä¸­ï¼Œå¹¶ç”¨2D poseæ§åˆ¶å¯¹è±¡çš„åŠ¨ä½œ</td><td>TI, LoRA</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/51.html">link</a></td></tr>
<tr><td>62</td><td>2023</td><td>Ruiz et al., â€œDreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation,â€</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/62.html">link</a></td></tr>
<tr><td>63</td><td>2023</td><td>Gal et al., <u>&quot;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&quot;</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/63.html">link</a></td></tr>
<tr><td><strong>38</strong></td><td>2021</td><td>Lora: Low-rank adaptation of large language models</td><td>å¯¹å·²è®­å¥½çš„å¤§æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œç”Ÿæˆæƒ³è¦çš„é£æ ¼ã€‚å­¦ä¹ å…¶ä¸­çš„æ®‹å·®ã€‚æ®‹å·®é€šå¸¸å¯ä»¥ç”¨low rank Matrixæ¥æ‹Ÿåˆï¼Œå› æ­¤ç§°ä¸ºlow-rank adaptationã€‚low rankçš„å¥½å¤„æ˜¯è¦è®­ç»ƒæˆ–è°ƒæ•´çš„å‚æ•°éå¸¸å°‘ã€‚</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/38.html">link</a></td></tr>
<tr><td></td><td></td><td>Lora + Dreambooth (by Simo Ryu)</td><td></td><td></td><td><a href="https://github.com/cloneofsimo/lora">https://github.com/cloneofsimo/lora</a></td></tr>
<tr><td></td><td>2023</td><td>Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models</td><td>å°†å¤šä¸ªLoRAèåˆåˆ°ä¸€ä¸ªæ¨¡å‹æ—¶ï¼Œè§£å†³LoRAä¹‹é—´çš„å†²çªé—®é¢˜ã€‚</td><td></td><td></td></tr>
</tbody></table>
<p>P43</p>
<h1 id="å¤šä¸ªç‰¹å®šå¯¹è±¡å®šåˆ¶åŒ–çš„å›¾åƒç”Ÿæˆ"><a class="header" href="#å¤šä¸ªç‰¹å®šå¯¹è±¡å®šåˆ¶åŒ–çš„å›¾åƒç”Ÿæˆ">å¤šä¸ªç‰¹å®šå¯¹è±¡å®šåˆ¶åŒ–çš„å›¾åƒç”Ÿæˆ</a></h1>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>52</td><td>2024</td><td>Mix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models</td><td>å¤šä¸ªç‰¹å®šå¯¹è±¡çš„å›¾åƒç”Ÿæˆï¼Œè®©å¤šä¸ªç‰¹å®šçš„å¯¹è±¡ç”Ÿæˆåˆ°ä¸€å¼ å›¾åƒä¸­ï¼Œå¹¶ç”¨2D poseæ§åˆ¶å¯¹è±¡çš„åŠ¨ä½œ</td><td>TI, LoRA</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/51.html">link</a></td></tr>
<tr><td>64</td><td>2023</td><td>Kumari et al., <u>&quot;Multi-Concept Customization of Text-to-Image Diffusion&quot;</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/64.html">link</a></td></tr>
<tr><td></td><td>2023</td><td>Tewel et al., Key-Locked Rank One Editing for Text-to-Image Personalization&quot;</td><td>âœ… æ–¹æ³•ï¼šdynamic rank one update. <br> âœ… Perffusion è§£å†³ Image Personalization çš„ overfitting é—®é¢˜çš„æ–¹æ³•ï¼š  <br> âœ… (1) è®­ç»ƒæ—¶ï¼ŒIntroducing new <em>xxxx</em> that locks the new concepts cross-attention keys to their sub-ordinate category.    <br> âœ… (2) æ¨æ–­æ—¶ï¼Œå¼•å…¥ a gate rank one approach å¯ç”¨äºæ§åˆ¶ the learned conceptçš„å½±å“åŠ›ã€‚    <br> âœ… (3) å…è®¸ medel æŠŠä¸åŒçš„ concept ç»“åˆåˆ°ä¸€èµ·ï¼Œå¹¶å­¦åˆ°ä¸åŒconcept ä¹‹é—´çš„è”ç³»ã€‚<br>Results: å¯ä»¥å¾ˆå¥½åœ°model the interaction of the two conceptionã€‚</td><td><img src="diffusion-tutorial-part/ApplicationOnImage/../../assets/D2-55.png" alt="" /></td><td></td></tr>
<tr><td>65</td><td>2023</td><td>Mou et al., T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models&quot;,</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/65.html">link</a></td></tr>
<tr><td></td><td>2013</td><td>Adding Conditional Control to Text-to-Image Diffusion Models</td><td></td><td></td><td></td></tr>
<tr><td>66</td><td>2023</td><td>Li et al., <u>&quot;GLIGEN: Open-Set Grounded Text-to-Image Generation&quot;,</u></td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/66.html">link</a></td></tr>
</tbody></table>
<p>P64</p>
<p>P67</p>
<h1 id="other-applications"><a class="header" href="#other-applications">Other applications</a></h1>
<p>P68</p>
<h2 id="your-diffusion-model-is-secretly-a-zero-shot-classifier"><a class="header" href="#your-diffusion-model-is-secretly-a-zero-shot-classifier">Your Diffusion Model is Secretly a Zero-Shot Classifier</a></h2>
<blockquote>
<p>âœ… ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„ diffusion model ï¼ˆä¾‹å¦‚stable diffusion modelï¼‰ï¼Œæ— é¡»é¢å¤–è®­ç»ƒå¯ä»¥ç”¨ä½œåˆ†ç±»å™¨ï¼Œç”šè‡³èƒ½å®Œæˆ Zero-shot çš„åˆ†ç±»ä»»åŠ¡ã€‚</p>
</blockquote>
<p>Li et al., <u>&quot;Your Diffusion Model is Secretly a Zero-Shot Classifier&quot;,</u> arXiv 2023</p>
<h3 id="pipeline-2"><a class="header" href="#pipeline-2">Pipeline</a></h3>
<p><img src="diffusion-tutorial-part/ApplicationOnImage/../../assets/D2-68.png" alt="" /></p>
<blockquote>
<p>âœ… è¾“å…¥å›¾åƒ\(x\)ï¼Œç”¨éšæœºå™ªå£°\(\epsilon  \)åŠ å™ªï¼›å†ç”¨ condition c é¢„æµ‹å™ªå£° \(\epsilon  _\theta \)ã€‚ä¼˜åŒ–æ¡ä»¶ C ä½¿å¾— \(\epsilon  _\theta \) æœ€æ¥è¿‘ \(\epsilon \). å¾—åˆ°çš„ C å°±æ˜¯åˆ†ç±»ã€‚</p>
</blockquote>
<p>P69</p>
<h2 id="improving-robustness-using-generated-data"><a class="header" href="#improving-robustness-using-generated-data">Improving Robustness using Generated Data</a></h2>
<blockquote>
<p>âœ… ä½¿ç”¨ diffusion Model åšæ•°æ®å¢å¼ºã€‚</p>
</blockquote>
<p><img src="diffusion-tutorial-part/ApplicationOnImage/../../assets/D2-69.png" alt="" /></p>
<p><strong>Overview of the approach:</strong></p>
<ol>
<li>train a generative model and a nonï¿¾robust classifier, which are used to provide pseudo-labels to the generated data.</li>
<li>The generated and original training data are combined to train a robust classifier.</li>
</ol>
<p>Gowal et al., <u>&quot;Improving Robustness using Generated Data&quot;,</u> NeurIPS 2021</p>
<p>P70</p>
<h2 id="better-diffusion-models-further-improve-adversarial-training"><a class="header" href="#better-diffusion-models-further-improve-adversarial-training">Better Diffusion Models Further Improve Adversarial Training</a></h2>
<p><img src="diffusion-tutorial-part/ApplicationOnImage/../../assets/D2-70.png" alt="" /></p>
<p>Wang et al., <u>&quot;Better Diffusion Models Further Improve Adversarial Training&quot;,</u> ICML 2023</p>
<p>P72</p>
<h1 id="reference-1"><a class="header" href="#reference-1">Reference</a></h1>
<ul>
<li>Bao et al., <u>&quot;All are Worth Words: a ViT Backbone for Score-based Diffusion Models&quot;,</u> arXiv 2022</li>
<li>Peebles and Xie, <u>&quot;Scalable Diffusion Models with Transformers&quot;,</u> arXiv 2022</li>
<li>Bao et al., <u>&quot;One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale&quot;,</u> arXiv 2023</li>
<li>Jabri et al., <u>&quot;Scalable Adaptive Computation for Iterative Generation&quot;,</u> arXiv 2022</li>
<li>Hoogeboomet al., <u>&quot;simple diffusion: End-to-end diffusion for high resolution images&quot;,</u> arXiv 2023</li>
<li>Meng et al., <u>&quot;SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations&quot;,</u> ICLR 2022</li>
<li>Li et al., <u>&quot;Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models&quot;,</u> NeurIPS 2022</li>
<li>Avrahami et al., <u>&quot;Blended Diffusion for Text-driven Editing of Natural Images&quot;,</u> CVPR 2022</li>
<li>Hertz et al., <u>&quot;Prompt-to-Prompt Image Editing with Cross-Attention Control&quot;,</u> ICLR 2023</li>
<li>Kawar et al., <u>&quot;Imagic: Text-Based Real Image Editing with Diffusion Models&quot;,</u> CVPR 2023</li>
<li>Couairon et al., <u>&quot;DiffEdit: Diffusion-based semantic image editing with mask guidance&quot;,</u> ICLR 2023</li>
<li>Sarukkai et al., <u>&quot;Collage Diffusion&quot;,</u>  arXiv 2023</li>
<li>Bar-Tal et al., <u>&quot;MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation&quot;,</u>  ICML 2023</li>
<li>Gal et al., <u>&quot;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&quot;,</u> ICLR 2023</li>
<li>Ruiz et al., <u>&quot;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&quot;,</u> CVPR 2023</li>
<li>Kumari et al., <u>&quot;Multi-Concept Customization of Text-to-Image Diffusion&quot;,</u>  CVPR 2023</li>
<li>Tewel et al., <u>&quot;Key-Locked Rank One Editing for Text-to-Image Personalization&quot;,</u>  SIGGRAPH 2023</li>
<li>Zhao et al., <u>&quot;A Recipe for Watermarking Diffusion Models&quot;,</u>  arXiv 2023</li>
<li>Hu et al., <u>&quot;LoRA: Low-Rank Adaptation of Large Language Models&quot;,</u> ICLR 2022</li>
<li>Li et al., <u>&quot;GLIGEN: Open-Set Grounded Text-to-Image Generation&quot;,</u> CVPR 2023</li>
<li>Avrahami et al., <u>&quot;SpaText: Spatio-Textual Representation for Controllable Image Generation&quot;,</u> CVPR 2023</li>
<li>Zhang and Agrawala, <u>&quot;Adding Conditional Control to Text-to-Image Diffusion Models&quot;,</u> arXiv 2023</li>
<li>Mou et al., <u>&quot;T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models&quot;,</u> arXiv 2023</li>
<li>Orgad et al., <u>&quot;Editing Implicit Assumptions in Text-to-Image Diffusion Models&quot;,</u> arXiv 2023</li>
<li>Han et al., <u>&quot;SVDiff: Compact Parameter Space for Diffusion Fine-Tuning&quot;,</u> arXiv 2023</li>
<li>Xie et al., <u>&quot;DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameterï¿¾Efficient Fine-Tuning&quot;,</u> rXiv 2023</li>
<li>Saharia et al., <u>&quot;Palette: Image-to-Image Diffusion Models&quot;,</u> SIGGRAPH 2022</li>
<li>Whang et al., <u>&quot;Deblurring via Stochastic Refinement&quot;,</u> CVPR 2022</li>
<li>Xu et al., <u>&quot;Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models&quot;,</u> arXiv 2023</li>
<li>Saxena et al., <u>&quot;Monocular Depth Estimation using Diffusion Models&quot;,</u> arXiv 2023</li>
<li>Li et al., <u>&quot;Your Diffusion Model is Secretly a Zero-Shot Classifier&quot;,</u> arXiv 2023</li>
<li>Gowal et al., <u>&quot;Improving Robustness using Generated Data&quot;,</u> NeurIPS 2021</li>
<li>Wang et al., <u>&quot;Better Diffusion Models Further Improve Adversarial Training&quot;,</u> ICML 2023</li>
</ul>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="å›¾åƒå»å™ªå›¾åƒè¶…åˆ†å›¾åƒè¡¥å…¨"><a class="header" href="#å›¾åƒå»å™ªå›¾åƒè¶…åˆ†å›¾åƒè¡¥å…¨">å›¾åƒå»å™ª/å›¾åƒè¶…åˆ†/å›¾åƒè¡¥å…¨</a></h1>
<div style="break-before: page; page-break-before: always;"></div><p>P68</p>
<h1 id="diffusion-models-for-large-contents"><a class="header" href="#diffusion-models-for-large-contents">Diffusion Models for Large Contents</a></h1>
<p>åŒæ ·çš„æ–¹æ³•ä¹Ÿå¯ç”¨äºApplications such as long images, looped motion, 360 imagesâ€¦</p>
<ul>
<li>Suppose model is trained on small, squared images, how to extend it to larger images?</li>
<li>Outpainting is always a solution, but not a very efficient one!</li>
</ul>
<p>Let us generate this image with a diffusion model only trained on squared regions:</p>
<p><img src="diffusion-tutorial-part/ApplicationOnImage/../../assets/D3-68-1.png" alt="" /></p>
<ol>
<li>Generate the center region \(q(\mathbf{x} _ 1,\mathbf{x} _ 2)\)</li>
<li>Generate the <strong>surrounding region conditioned on parts of the center image</strong> \(q(\mathbf{x} _ 3|\mathbf{x} _ 2)\)</li>
</ol>
<p><img src="diffusion-tutorial-part/ApplicationOnImage/../../assets/D3-68-2.png" alt="" /></p>
<p>Latency scales linearly with the content size!</p>
<blockquote>
<p>âœ… æ ¹æ®å·¦è¾¹çš„å›¾ç”Ÿæˆå³è¾¹çš„å›¾ï¼Œå­˜åœ¨çš„é—®é¢˜ï¼šæ…¢<br />
âœ… ç›´æ¥ç”Ÿæˆå¤§å›¾æ²¡æœ‰è¿™æ ·çš„æ•°æ®ã€‚<br />
âœ… å¹¶è¡ŒåŒ–çš„ç”Ÿæˆã€‚</p>
</blockquote>
<p>P69</p>
<h2 id="diffcollage"><a class="header" href="#diffcollage">DiffCollage</a></h2>
<ul>
<li>Unlike autoregressive models, diffusion models can generate large contents <strong>in parallel</strong>!</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationOnImage/../../assets/D3-69-1.png" alt="" /></p>
<p>P70</p>
<ul>
<li>A â€œlargeâ€ diffusion model from â€œsmallâ€ diffusion models!</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationOnImage/../../assets/D3-70-1.png" alt="" /></p>
<p><img src="diffusion-tutorial-part/ApplicationOnImage/../../assets/D3-70-2.png" alt="" /></p>
<p>P71</p>
<h2 id="more-works"><a class="header" href="#more-works">More Works</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2023</td><td>Zhang et al., <u>&quot;DiffCollage: Parallel Generation of Large Content with Diffusion Models&quot;</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>JimÃ©nez, <u>&quot;Mixture of Diffusers for scene composition and high resolution image generation&quot;,</u> arXiv 2023</td><td>- Based on similar ideas but differ in how overlapping regions are mixed.<br> âœ… è¿™ç§å¹¶è¡ŒåŒ–æ–¹æ³•å¯ä»¥ç”¨äºå„ç§ overlapping çš„åœºæ™¯ã€‚</td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>Bar-Tal et al., <u>&quot;MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation&quot;,</u> ICML 2023</td><td></td><td></td><td></td></tr>
</tbody></table>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="diffusion-on-various-3d-representations"><a class="header" href="#diffusion-on-various-3d-representations">Diffusion on various 3D representations</a></h1>
<p>P12</p>
<h2 id="3d-shape-generation-and-completion-through-point-voxel-diffusion"><a class="header" href="#3d-shape-generation-and-completion-through-point-voxel-diffusion">3D Shape Generation and Completion through Point-Voxel Diffusion</a></h2>
<p>A set of points with location information.</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-12.png" alt="" /></p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-12-1.png" alt="" /></p>
<p>Zhou et al., <u>&quot;3D Shape Generation and Completion through Point-Voxel Diffusion&quot;,</u> ICCV 2021<br />
Liu et al, <u>&quot;Point-Voxel CNN for Efficient 3D Deep Learning&quot;,</u> NeurIPS 2019</p>
<blockquote>
<p>âœ… åˆ†æ”¯1ï¼šé€é¡¶ç‚¹çš„ MLP (å¯¹åº”å›¾ä¸­ b)<br />
âœ… åˆ†æ”¯2ï¼šVOX å¯ä»¥çœ‹ä½œæ˜¯ä½åˆ†è¾¨ç‡çš„ points<br />
âœ… ä¼˜ç‚¹æ˜¯ç»“æ„åŒ–ï¼Œå¯ç”¨äº CNN<br />
â“ VOX â†’ pointsï¼Œä½åˆ†è¾¨åˆ°é«˜åˆ†è¾¨ç‡è¦æ€ä¹ˆåšï¼Ÿ<br />
â“ æ€ä¹ˆæŠŠ voxel å†…çš„ç‚¹è½¬æ¢ä¸º voxel çš„ç‰¹å¾ï¼Ÿ</p>
</blockquote>
<p>P13</p>
<h3 id="result"><a class="header" href="#result">Result</a></h3>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-13.png" alt="" /></p>
<blockquote>
<p>âœ… Completionï¼šæ·±åº¦å›¾ â†’ å®Œæ•´ç‚¹<br />
âœ… æ–¹æ³•ï¼š(1) åŸºäºæ·±åº¦å›¾ç”Ÿæˆç‚¹äº‘ (2) ç”¨ inpainting æŠ€æœ¯è¡¥å…¨<br />
âœ… generation å’Œ completion æ˜¯ä¸¤ç§ä¸åŒçš„ task.</p>
</blockquote>
<p>P14</p>
<h2 id="lion"><a class="header" href="#lion">LION</a></h2>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-14.png" alt="" /></p>
<p>Zeng et al., <u>&quot;LION: Latent Point Diffusion Models for 3D Shape Generation&quot;,</u> NeurIPS 2022</p>
<blockquote>
<p>âœ… ç‰¹ç‚¹ï¼š<br />
âœ… 1ã€latent diffusion model for point clouds.<br />
âœ… 2ã€point-voxel CNN æ¶æ„ï¼Œç”¨äºæŠŠ shape ç¼–ç æˆ latent shape åŠ lantent point.<br />
âœ… 3ã€diffusion model æŠŠ latent point é‡å»ºå‡ºåŸå§‹ç‚¹ã€‚</p>
</blockquote>
<p>P15</p>
<h2 id="point-e"><a class="header" href="#point-e">Point-E</a></h2>
<p>Point-E uses a synthetic view from fine-tuned GLIDE, and then â€liftsâ€ the image to a 3d point cloud.</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-15-1.png" alt="" /></p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-15-2.png" alt="" /></p>
<p>Nichol et al., <u>&quot;Point-E: A System for Generating 3D Point Clouds from Complex Prompts&quot;,</u> arXiv 2022</p>
<blockquote>
<p>âœ… point E taskï¼šæ–‡ç”Ÿæˆç‚¹äº‘ã€‚<br />
âœ… ç¬¬1æ­¥ï¼šæ–‡ç”Ÿå›¾ï¼Œç”¨ fine-tuned GLIDE<br />
âœ… ç¬¬2æ­¥ï¼šå›¾ç”Ÿç‚¹ï¼Œç”¨ transformer-based diffusion model.</p>
</blockquote>
<p>P16</p>
<h1 id="diffusion-models-for-signed-distance-functions"><a class="header" href="#diffusion-models-for-signed-distance-functions">Diffusion Models for Signed Distance Functions</a></h1>
<p>SDF is a function representation of a surface.<br />
For each location x, |SDF(x)| = smallest distance to any point on the surface.</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-16.png" alt="" /></p>
<p>P17</p>
<h2 id="neural-wavelet-domain-diffusion-for-3d-shape-generation"><a class="header" href="#neural-wavelet-domain-diffusion-for-3d-shape-generation">Neural Wavelet-domain Diffusion for 3D Shape Generation</a></h2>
<ul>
<li>Memory of SDF grows cubically with resolution</li>
<li>Wavelets can be used for compression!</li>
<li>Diffusion for coarse coefficients, then predict detailed ones.</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-17-1.png" alt="" /></p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-17-2.png" alt="" /></p>
<p>Hui et al., <u>&quot;Neural Wavelet-domain Diffusion for 3D Shape Generation&quot;,</u> arXiv 2022</p>
<blockquote>
<p>âœ… è¿™é‡Œè¯´çš„ SDFï¼Œæ˜¯ç”¨ç¦»æ•£çš„æ–¹å¼æ¥è®°å½•æ¯ä¸ªç‚¹çš„ distance.<br />
âœ… Wavelet æŠŠ SDF å˜ä¸º coarse ç³»æ•°ï¼Œdiffusion model ç”Ÿæˆ coarse ç³»æ•°ï¼Œå†é€šè¿‡å¦ä¸€æ¨¡å‹å˜ä¸º detailed</p>
</blockquote>
<p>P18</p>
<h2 id="diffusionsdf"><a class="header" href="#diffusionsdf">DiffusionSDF</a></h2>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-18.png" alt="" /></p>
<p><strong>Latent space diffusion for SDFs, where conditioning can be provided with cross attention</strong></p>
<p>Chou et al., <u>&quot;DiffusionSDF: Conditional Generative Modeling of Signed Distance Functions&quot;,</u> arXiv 2022</p>
<blockquote>
<p>âœ… åŸç†ä¸ä¸Šä¸€é¡µç›¸ä¼¼ï¼Œåªæ˜¯æŠŠ waveles æ¢æˆäº† VAE.</p>
</blockquote>
<p>P19</p>
<h1 id="diffusion-models-for-nerf"><a class="header" href="#diffusion-models-for-nerf">Diffusion Models for NeRF</a></h1>
<p>Neural Radiance Fields (NeRF) is another representation of a 3D object.</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-19.png" alt="" /></p>
<blockquote>
<p>âœ… NeRFï¼šç”¨ä½“çš„æ–¹å¼æ¥æè¿° 3D ç‰©ä½“<br />
âœ… (1) ä» diffusion ä¸­æå– image ï¼ˆ2ï¼‰ä» image è®¡ç®— loss (3) loss æ›´æ–° image (4) image æ›´æ–° NeRFï¼<br />
âœ… \(ï¼ˆx,y,z,\theta ,\phi ï¼‰\) æ˜¯æ¯ä¸ªç‚¹åœ¨å‘é‡ä¸­çš„è¡¨ç¤ºï¼Œå…¶ä¸­å‰ä¸‰ç»´æ˜¯ world coordinateï¼Œåé¢ä¸¤ç»´æ˜¯ viewing direction<br />
âœ… density æè¿°è¿™ä¸ªç‚¹æœ‰å¤šé€æ˜ã€‚<br />
âœ… F æ˜¯ä¸€ä¸ªå°å‹çš„ç½‘ç»œï¼Œä¾‹å¦‚ MLP.</p>
</blockquote>
<p>P20</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-20-1.png" alt="" /><br />
<strong>NeRF</strong><br />
(Fully implicit)</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-20-2.png" alt="" /><br />
<strong>Voxels</strong><br />
(Explicit / hybrid)</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-20-3.png" alt="" /><br />
<strong>Triplanes</strong><br />
(Factorized, hybrid)</p>
<p>Image from EG3D paper.</p>
<p>P21</p>
<blockquote>
<p>âœ… Nerf å¯ä»¥æœ‰ä¸‰ç§è¡¨ç¤ºå½¢å¼</p>
</blockquote>
<ul>
<li>Triplanes, regularized ReLU Fields, the MLP of NeRFs...</li>
<li>A good representation is important!</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-21-1.png" alt="" /><br />
Triplane diffusion</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-21-2.png" alt="" /><br />
Regularized ReLU Fields</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-21-3.png" alt="" /><br />
Implicit MLP of NeRFs</p>
<p>Shue et al., <u>&quot;3D Neural Field Generation using Triplane Diffusion&quot;,</u> arXiv 2022<br />
Yang et al., <u>&quot;Learning a Diffusion Prior for NeRFs&quot;,</u> ICLR Workshop 2023<br />
Jun and Nichol, <u>&quot;Shap-E: Generating Conditional 3D Implicit Functions&quot;,</u> arXiv 2023</p>
<blockquote>
<p>âœ… è¿™ä¸‰ç§è¡¨ç¤ºå½¢å¼éƒ½å¯ä»¥ä¸ diffuson ç»“åˆã€‚<br />
âœ… å¥½çš„è¡¨ç¤ºå½¢å¼å¯¹diffusion çš„æ•ˆæœå¾ˆé‡è¦ã€‚</p>
</blockquote>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P23</p>
<p>ç”±äºç¼ºå°‘3Dæ•°æ®ï¼ŒæŠŠ2D T2I Base Modelä½œä¸ºå…ˆéªŒæ¥å®ç°3Dç”Ÿæˆã€‚</p>
<h1 id="sds"><a class="header" href="#sds">SDS</a></h1>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td><strong>68</strong></td><td>2023</td><td>Poole et al., &quot;DreamFusion: Text-to-3D using 2D Diffusion&quot;</td><td></td><td>SDS</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/68.html">link</a></td></tr>
<tr><td></td><td>2023</td><td>Lin et al., <u>&quot;Magic3D: High-Resolution Text-to-3D Content Creation&quot;</td><td>2x speed and higher resolution <br>Accelerate NeRF with Instant-NGP, for coarse representations. <br> Optimize a fine mesh model with differentiable renderer.<br> âœ… Instant NGP ä»£æ›¿å·¦ä¸‹çš„ Nerf MLPï¼ä»¥ coarse representetion ä½œä¸º condition æ¥ç”Ÿæˆ fine mesh model.</td><td>Extensions to SDS<br><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-30.png" alt="" /></td><td></td></tr>
<tr><td></td><td>2023</td><td>Wang et al.,&quot;Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation&quot;,</td><td></td><td>Alternative to SDS</td><td></td></tr>
<tr><td></td><td>2023</td><td>Wang et al., &quot;ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation&quot;,</td><td></td><td>Alternative to SDS</td><td></td></tr>
</tbody></table>
<p>P31</p>
<h1 id="alternative-to-sds-score-jacobian-chaining"><a class="header" href="#alternative-to-sds-score-jacobian-chaining">Alternative to SDS: Score Jacobian Chaining</a></h1>
<p>A different formulation, motivated from approximating 3D score.</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-31.png" alt="" /></p>
<p>In principle, the diffusion model is the noisy 2D score (over clean images),<br />
but in practice, the diffusion model suffers from out-of-distribution (OOD) issues!</p>
<p>For diffusion model on noisy images, <strong>the non-noisy images are OOD</strong>!</p>
<blockquote>
<p>âœ… 2D sample, 3D score</p>
</blockquote>
<p>P32</p>
<h2 id="score-jacobian-chaining"><a class="header" href="#score-jacobian-chaining">Score Jacobian Chaining</a></h2>
<p>SJC approximates noisy score with â€œPerturb-and-Average Scoringâ€, which is not present in SDS.</p>
<ul>
<li>Use score model on multiple noise-perturbed data, then average it.</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-32.png" alt="" /></p>
<blockquote>
<p>âœ… é€šè¿‡è¿™ç§æ–¹æ³•æ¥è¿‘ä¼¼ clean image çš„è¾“å‡ºï¼Œè§£å†³ clean image çš„ OOD é—®é¢˜ã€‚</p>
</blockquote>
<p>P33</p>
<h2 id="sjc-and-sds"><a class="header" href="#sjc-and-sds">SJC and SDS</a></h2>
<p>SJC is a competitive alternative to SDS.</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-33.png" alt="" /> </p>
<p>P34</p>
<h1 id="alternative-to-sds-prolificdreamer"><a class="header" href="#alternative-to-sds-prolificdreamer">Alternative to SDS: ProlificDreamer</a></h1>
<ul>
<li>SDS-based method often set classifier-guidance weight to 100, which limits the â€œdiversityâ€ of the generated samples.</li>
<li>ProlificDreamer reduces this to 7.5, leading to diverse samples.</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-34.png" alt="" /> </p>
<p>P35</p>
<h2 id="prolificdreamer-and-variational-score-distillation"><a class="header" href="#prolificdreamer-and-variational-score-distillation">ProlificDreamer and Variational Score Distillation</a></h2>
<p>Instead of maximizing the likelihood under diffusion model, VSD minimizes the KL divergence via variational inference.</p>
<p>$$
\begin{matrix}
\min_{\mu } D _ {\mathrm{KL} }(q^\mu _ 0(\mathbf{x} _ 0|y)||p _ 0(\mathbf{x} _ 0|y)). \\
\quad \mu \quad \text{is the distribution of NeRFs} .
\end{matrix}
$$</p>
<p>Suppose is a \(\theta _ \tau \sim \mu \) NeRF sample, then VSD simulates this ODE:</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-35.png" alt="" /> </p>
<ul>
<li>Diffusion model can be used to approximate score of noisy real images.</li>
<li>How about noisy rendered images?   sss</li>
</ul>
<blockquote>
<p>âœ… ç¬¬ä¸€é¡¹ç”± diffusion model å¾—åˆ°ï¼Œåœ¨æ­¤å¤„å½“ä½œ GTï¼</p>
</blockquote>
<p>P36</p>
<ul>
<li>Learn another diffusion model to approximate the score of noisy rendered images!</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-36.png" alt="" /></p>
<blockquote>
<p>âœ… ä½¿ç”¨ LoRA è¿‘ç¬¬äºŒé¡¹ã€‚</p>
</blockquote>
<p>P37</p>
<h2 id="why-does-vsd-work-in-practice"><a class="header" href="#why-does-vsd-work-in-practice">Why does VSD work in practice?</a></h2>
<ul>
<li>
<p>The valid text-to-image NeRFs form a distribution with infinite possibilities!</p>
</li>
<li>
<p>In SDS, epsilon is the score of noisy â€œdirac distributionâ€ over finite renders, which converges to the true score with infinite renders!</p>
</li>
<li>
<p>In VSD, the LoRA model aims to <strong>represent the (true) score of noisy distribution over infinite number of renders!</strong></p>
</li>
<li>
<p>If the generated NeRF distribution is only one point and LoRA overfits perfectly, then VSD = SDS!</p>
</li>
<li>
<p>But LoRA has good generalization (and learns from a trajectory of NeRFs), so closer to the true score!</p>
</li>
<li>
<p>This is analogous to</p>
<ul>
<li>Representing the dataset score via mixture of Gaussians on the dataset (SDS), versus</li>
<li>Representing the dataset score via the LoRA UNet (VSD)</li>
</ul>
</li>
</ul>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P40</p>
<h2 id="3dim"><a class="header" href="#3dim">3DiM</a></h2>
<ul>
<li>Condition on a frame and two poses, predict another frame.</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-40-1.png" alt="" /></p>
<p>UNet with frame cross-attention</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-40-2.png" alt="" /></p>
<p>Sample based on stochastic conditions,<br />
allowing the use of multiple conditional frames.</p>
<p>Watson et al., <u>&quot;Novel View Synthesis with Diffusion Models&quot;,</u> ICLR 2023</p>
<blockquote>
<p>âœ… UNetï¼Œ2 branchï¼Œåˆ†åˆ«ç”¨äºåŸå§‹è§’åº¦å’Œè¦ç”Ÿæˆçš„è§’åº¦ã€‚<br />
âœ… å¼•å…¥ step 2 æ˜¯ä¸ºäº†å†…å®¹ä¸€è‡´æ€§ã€‚<br />
âœ… frameï¼šåæ ‡ç³»ã€‚åœ¨ä¸åŒçš„åæ ‡ç³»ä¸‹çœ‹åˆ°çš„æ˜¯ä¸åŒçš„è§†è§’ã€‚<br />
â“ ä¸ºä»€ä¹ˆæœ‰ä¸¤ä¸ªposeï¼Ÿ<br />
âœ… æ¯ä¸ª frame çš„å†…éƒ¨ç”± cross-attention è¿æ¥ã€‚</p>
</blockquote>
<p>P41</p>
<h2 id="genvs"><a class="header" href="#genvs">GenVS</a></h2>
<ul>
<li>3D-aware architecture with latent feature field.</li>
<li>Use diffusion model to improve render quality based on structure.</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-41.png" alt="" /></p>
<p>Chan et al., <u>&quot;Generative Novel View Synthesis with 3D-Aware Diffusion Models&quot;,</u> arXiv 2023</p>
<blockquote>
<p>âœ… (1) ç”Ÿæˆ feature field (2) render å…¶ä¸­ä¸€ä¸ªè§†è§’ (3) ä¼˜åŒ–æ¸²æŸ“æ•ˆæœ<br />
âœ… (2) æ˜¯ MLP (3) æ˜¯ diffusionï¼</p>
</blockquote>
<h2 id="more"><a class="header" href="#more">More</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2023</td><td>Watson et al.,&quot;Novel View Synthesis with Diffusion Models&quot;</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2024</td><td>CAT3D</td><td></td><td></td><td></td></tr>
</tbody></table>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2023</td><td>NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360Â° Views</td><td>SDS + Fine-tuned CLIP text embedding + Depth supervision</td><td><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-43.png" alt="" /></td><td></td></tr>
<tr><td></td><td>2023</td><td>Zero-1-to-3: Zero-shot One Image to 3D Object</td><td>Generate novel view from 1 view and pose, with 2d model. <br> Then, run SJC / SDS-like optimizations with view-conditioned model.</td><td><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-44.png" alt="" /></td><td></td></tr>
<tr><td></td><td>2024</td><td>CAT3D</td><td></td><td></td><td></td></tr>
</tbody></table>
<p>P43 
NeuralLift-360</p>
<blockquote>
<p>âœ… æ•´ä½“ä¸Šæ˜¯ç±»ä¼¼ SDS çš„ä¼˜åŒ–æ–¹æ³•ï¼Œå†ç»“åˆå…¶å®ƒçš„æŸå¤±å‡½æ•°ã€‚<br />
âœ… (1) æ¸²æŸ“ä¸åŒè§†è§’ï¼Œå¹¶å¯¹æ¸²æŸ“ç»“æœç”¨ clip scoreæ‰“åˆ†ã€‚<br />
âœ… (2) ç›‘ç£æ·±åº¦ä¿¡æ¯ã€‚</p>
</blockquote>
<p>P44<br />
Zero 1-to-3</p>
<blockquote>
<p>âœ… (1) ç”¨ 2D diffusion ç”Ÿæˆå¤šè§†è§’ã€‚ç”¨ SDS å¯¹å¤šè§†è§’å›¾åƒç”Ÿæˆ3Dï¼</p>
</blockquote>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P46</p>
<h1 id="instruct-nerf2nerf"><a class="header" href="#instruct-nerf2nerf">Instruct NeRF2NeRF</a></h1>
<p>Edit a 3D scene with text instructions</p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-46.png" alt="" /></p>
<p>Haque et al., <u>&quot;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions&quot;,</u> arXiv 2023</p>
<blockquote>
<p>âœ… ç”¨ Nerf æ¥æè¿° 3D sceneã€‚é€šè¿‡æ–‡ä»¶æ¡ä»¶æŠŠåŸ Nerfï¼Œå˜æˆå¦ä¸€ä¸ª Nerfï¼Œä»è€Œå¾—åˆ°æ–°çš„ 3D scene.</p>
</blockquote>
<p>P47</p>
<h2 id="æ–¹æ³•"><a class="header" href="#æ–¹æ³•">æ–¹æ³•</a></h2>
<p><strong>Edit a 3D scene with text instructions</strong></p>
<ul>
<li>Given existing scene, use Instruct Pix2Pix to edit image at different viewpoints.</li>
<li>Continue to train the NeRF and repeat the above process</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-47.png" alt="" /></p>
<blockquote>
<p>âœ… é¦–å…ˆæœ‰ä¸€ä¸ªè®­å¥½çš„ Nerf. å¯¹ä¸€ä¸ªç‰¹å®šçš„åœºæ™¯ä½¿ç”¨ Instruct Pix 2 pix åœ¨ 2D ä¸Šç¼–è¾‘è®­ç»ƒæ–°çš„ Werf.<br />
âœ… åŸºäº score disllation sampling.</p>
</blockquote>
<p>P48</p>
<p>With each iteration, the edits become more consistent.</p>
<p>P49</p>
<h1 id="vox-e-text-guided-voxel-editing-of-3d-objects"><a class="header" href="#vox-e-text-guided-voxel-editing-of-3d-objects">Vox-E: Text-guided Voxel Editing of 3D Objects</a></h1>
<ul>
<li>Text-guided object editing with SDS</li>
<li>Regularize the structure of the new voxel grid.</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-49.png" alt="" /></p>
<p>Sella et al., <u>&quot;Vox-E: Text-guided Voxel Editing of 3D Objects&quot;,</u> arXiv 2023</p>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P73</p>
<h2 id="outline"><a class="header" href="#outline">Outline</a></h2>
<ul>
<li>Safety and limitations of diffusion models</li>
</ul>
<p>P74</p>
<h2 id="data-memorization-in-diffusion-models"><a class="header" href="#data-memorization-in-diffusion-models">Data Memorization in Diffusion Models</a></h2>
<ul>
<li>Due to the likelihood-base objective function, <strong>diffusion models can â€memorizeâ€ data</strong>.</li>
<li>And with a higher chance than GANs!</li>
<li>Nevertheless, a lot of â€œmemorized imagesâ€ are highly-duplicated in the dataset.</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-74.png" alt="" /></p>
<p>Carlini et al., <u>&quot;Extracting Training Data from Diffusion Models&quot;,</u> arXiv 2023</p>
<p>P75</p>
<h2 id="erasing-concepts-in-diffusion-models"><a class="header" href="#erasing-concepts-in-diffusion-models">Erasing Concepts in Diffusion Models</a></h2>
<ul>
<li>Fine-tune a model to remove unwanted concepts.</li>
<li>From original model, <strong>obtain score via negative CFG</strong>.</li>
<li><strong>A new model is fine-tuned</strong> from the new score function.</li>
</ul>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-75-1.png" alt="" /></p>
<p><img src="diffusion-tutorial-part/ApplicationsOn3D/../../assets/D3-75-2.png" alt="" /></p>
<p>Gandikota et al., <u>&quot;Erasing Concepts from Diffusion Models&quot;,</u> arXiv 2023</p>
<blockquote>
<p>âœ… è€ƒè™‘åˆ°ç‰ˆæƒç­‰é—®é¢˜ã€‚<br />
âœ… finetune å·²æœ‰çš„ text-2-image modelï¼<br />
âœ… ä½¿ç”¨ negative CFG åŸæœ‰ä¿¡æ¯ä¸ä¼šå—åˆ°å½±å“ã€‚</p>
</blockquote>
<h1 id="reference-2"><a class="header" href="#reference-2">Reference</a></h1>
<p>P77</p>
<h2 id="part-i"><a class="header" href="#part-i">Part I</a></h2>
<p>Ho et al., <u>&quot;Denoising Diffusion Probabilistic Models&quot;,</u> NeurIPS 2020<br />
Song et al., <u>&quot;Score-Based Generative Modeling through Stochastic Differential Equations&quot;,</u> ICLR 2021<br />
Kingma et al., <u>&quot;Variational Diffusion Models&quot;,</u> arXiv 2021<br />
Karras et al., <u>&quot;Elucidating the Design Space of Diffusion-Based Generative Models&quot;,</u> NeurIPS 2022<br />
Song et al., <u>&quot;Denoising Diffusion Implicit Models&quot;,</u> ICLR 2021<br />
Jolicoeur-Martineau et al., &quot;Gotta Go Fast When Generating Data with Score-Based Models&quot;,</u> arXiv 2021<br />
Liu et al., <u>&quot;Pseudo Numerical Methods for Diffusion Models on Manifolds&quot;,</u> ICLR 2022<br />
Lu et al., <u>&quot;DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps&quot;,</u> NeurIPS 2022<br />
Lu et al., <u>&quot;DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models&quot;,</u> NeurIPS 2022<br />
Zhang and Chen, <u>&quot;Fast Sampling of Diffusion Models with Exponential Integrator&quot;,</u> arXiv 2022<br />
Zhang et al., <u>&quot;gDDIM: Generalized denoising diffusion implicit models&quot;,</u> arXiv 2022<br />
Zhao et al., <u>&quot;UniPC: A Unified Predictor-Corrector Framework for Fast Sampling of Diffusion Models&quot;,</u> arXiv 2023<br />
Shih et al., <u>&quot;Parallel Sampling of Diffusion Models&quot;,</u> arxiv 2023<br />
Chen et al., <u>&quot;A Geometric Perspective on Diffusion Models&quot;,</u> arXiv 2023<br />
Xiao et al., <u>&quot;Tackling the Generative Learning Trilemma with Denoising Diffusion GANs&quot;,</u> arXiv 2021<br />
Salimans and Ho, <u>&quot;Progressive Distillation for Fast Sampling of Diffusion Models&quot;,</u> ICLR 2022<br />
Meng et al., <u>&quot;On Distillation of Guided Diffusion Models&quot;,</u> arXiv 2022<br />
Dockhorn et al., <u>&quot;GENIE: Higher-Order Denoising Diffusion Solvers&quot;,</u> NeurIPS 2022<br />
Watson et al., <u>&quot;Learning Fast Samplers for Diffusion Models by Differentiating Through Sample Quality&quot;,</u> ICLR 2022<br />
Phung et al., <u>&quot;Wavelet Diffusion Models Are Fast and Scalable Image Generators&quot;,</u> CVPR 2023<br />
Dhariwal and Nichol, <u>&quot;Diffusion Models Beat GANs on Image Synthesis&quot;,</u> arXiv 2021<br />
Ho and Salimans, <u>&quot;Classifier-Free Diffusion Guidance&quot;,</u> NeurIPS Workshop 2021<br />
Automatic1111, <u>&quot;Negative Prompt&quot;,</u> GitHub<br />
Hong et al., <u>&quot;Improving Sample Quality of Diffusion Models Using Self-Attention Guidance&quot;,</u> arXiv 2022<br />
Saharia et al., <u>&quot;Image Super-Resolution via Iterative Refinement&quot;,</u> arXiv 2021<br />
Ho et al., <u>&quot;Cascaded Diffusion Models for High Fidelity Image Generation&quot;,</u> JMLR 2021<br />
Sinha et al., <u>&quot;D2C: Diffusion-Denoising Models for Few-shot Conditional Generation&quot;,</u> NeurIPS 2021<br />
Vahdat et al., <u>&quot;Score-based Generative Modeling in Latent Space&quot;,</u> arXiv 2021<br />
Daras et al., <u>&quot;Score-Guided Intermediate Layer Optimization: Fast Langevin Mixing for Inverse Problems&quot;,</u> ICML 2022</p>
<p>P78</p>
<h2 id="part-i-contd"><a class="header" href="#part-i-contd">Part I (contâ€™d)</a></h2>
<p>Bortoli et al.,<u> &quot;Diffusion SchrÃ¶dinger Bridge&quot;,</u> NeurIPS 2021<br />
Bortoli et al.,<u> &quot;Riemannian Score-Based Generative Modelling&quot;,</u> NeurIPS 2022<br />
Neklyudov et al., <u>&quot;Action Matching: Learning Stochastic Dynamics from Samples&quot;,</u> ICML 2023<br />
Bansal et al., <u>&quot;Cold Diffusion: Inverting Arbitrary Image Transforms Without Noise&quot;,</u> arXiv 2022<br />
Daras et al., <u>&quot;Soft Diffusion: Score Matching for General Corruptions&quot;,</u> TMLR 2023<br />
Delbracio and Milanfar, <u>&quot;Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration&quot;,</u> arXiv 2023<br />
Luo et al., <u>&quot;Image Restoration with Mean-Reverting Stochastic Differential Equations&quot;,</u> ICML 2023</p>
<p>P79</p>
<h2 id="part-ii"><a class="header" href="#part-ii">Part II</a></h2>
<p>Bao et al., <u>&quot;All are Worth Words: a ViT Backbone for Score-based Diffusion Models&quot;,</u> arXiv 2022<br />
Peebles and Xie, <u>&quot;Scalable Diffusion Models with Transformers&quot;,</u> arXiv 2022<br />
Bao et al., <u>&quot;One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale&quot;,</u> arXiv 2023<br />
Jabri et al., <u>&quot;Scalable Adaptive Computation for Iterative Generation&quot;,</u> arXiv 2022<br />
Hoogeboom et al., <u>&quot;simple diffusion: End-to-end diffusion for high resolution images&quot;,</u> arXiv 2023<br />
Meng et al., <u>&quot;SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations&quot;,</u> ICLR 2022<br />
Li et al., <u>&quot;Efficient Spatially Sparse Inference for Conditional GANs and Diffusion Models&quot;,</u> NeurIPS 2022<br />
Avrahami et al., <u>&quot;Blended Diffusion for Text-driven Editing of Natural Images&quot;,</u> CVPR 2022<br />
Hertz et al., <u>&quot;Prompt-to-Prompt Image Editing with Cross-Attention Control&quot;,</u> ICLR 2023<br />
Kawar et al., <u>&quot;Imagic: Text-Based Real Image Editing with Diffusion Models&quot;,</u> CVPR 2023<br />
Couairon et al., <u>&quot;DiffEdit: Diffusion-based semantic image editing with mask guidance&quot;,</u> ICLR 2023<br />
Sarukkai et al., <u>&quot;Collage Diffusion&quot;,</u> arXiv 2023<br />
Bar-Tal et al., <u>&quot;MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation&quot;,</u> ICML 2023<br />
Gal et al., <u>&quot;An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion&quot;,</u> ICLR 2023<br />
Ruiz et al., <u>&quot;DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation&quot;,</u> CVPR 2023<br />
Kumari et al., <u>&quot;Multi-Concept Customization of Text-to-Image Diffusion&quot;,</u> CVPR 2023<br />
Tewel et al., <u>&quot;Key-Locked Rank One Editing for Text-to-Image Personalization&quot;,</u> SIGGRAPH 2023<br />
Zhao et al., <u>&quot;A Recipe for Watermarking Diffusion Models&quot;,</u> arXiv 2023<br />
Hu et al., <u>&quot;LoRA: Low-Rank Adaptation of Large Language Models&quot;,</u> ICLR 2022<br />
Li et al., <u>&quot;GLIGEN: Open-Set Grounded Text-to-Image Generation&quot;,</u> CVPR 2023<br />
Avrahami et al., <u>&quot;SpaText: Spatio-Textual Representation for Controllable Image Generation&quot;,</u> CVPR 2023<br />
Zhang and Agrawala, <u>&quot;Adding Conditional Control to Text-to-Image Diffusion Models&quot;,</u> arXiv 2023<br />
Mou et al., <u>&quot;T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models&quot;,</u> arXiv 2023<br />
Orgad et al., <u>&quot;Editing Implicit Assumptions in Text-to-Image Diffusion Models&quot;,</u> arXiv 2023<br />
Han et al., <u>&quot;SVDiff: Compact Parameter Space for Diffusion Fine-Tuning&quot;,</u> arXiv 2023<br />
Xie et al., <u>&quot;DiffFit: Unlocking Transferability of Large Diffusion Models via Simple Parameter-Efficient Fine-Tuning&quot;,</u> arXiv 2023<br />
Saharia et al., <u>&quot;Palette: Image-to-Image Diffusion Models&quot;,</u> SIGGRAPH 2022<br />
Whang et al., <u>&quot;Deblurring via Stochastic Refinement&quot;,</u> CVPR 2022<br />
Xu et al., <u>&quot;Open-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models&quot;,</u> arXiv 2023<br />
Saxena et al., <u>&quot;Monocular Depth Estimation using Diffusion Models&quot;,</u> arXiv 2023<br />
Li et al., <u>&quot;Your Diffusion Model is Secretly a Zero-Shot Classifier&quot;,</u> arXiv 2023<br />
Gowal et al., <u>&quot;Improving Robustness using Generated Data&quot;,</u> NeurIPS 2021<br />
Wang et al., <u>&quot;Better Diffusion Models Further Improve Adversarial Training&quot;,</u> ICML 2023</p>
<p>P81</p>
<h2 id="part-iii"><a class="header" href="#part-iii">Part III</a></h2>
<p>Jalal et al., <u>&quot;Robust Compressed Sensing MRI with Deep Generative Priors&quot;,</u> NeurIPS 2021<br />
Song et al., <u>&quot;Solving Inverse Problems in Medical Imaging with Score-Based Generative Models&quot;,</u> ICLR 2022<br />
Kawar et al., <u>&quot;Denoising Diffusion Restoration Models&quot;,</u> NeurIPS 2022<br />
Chung et al., <u>&quot;Improving Diffusion Models for Inverse Problems using Manifold Constraints&quot;,</u> NeurIPS 2022<br />
Ryu and Ye, <u>&quot;Pyramidal Denoising Diffusion Probabilistic Models&quot;,</u> arXiv 2022<br />
Chung et al., <u>&quot;Diffusion Posterior Sampling for General Noisy Inverse Problems&quot;,</u> arXiv 2022<br />
Feng et al., <u>&quot;Score-Based Diffusion Models as Principled Priors for Inverse Imaging&quot;,</u> arXiv 2023<br />
Song et al., <u>&quot;Pseudoinverse-Guided Diffusion Models for Inverse Problems&quot;,</u> ICLR 2023<br />
Mardani et al., <u>&quot;A Variational Perspective on Solving Inverse Problems with Diffusion Models&quot;,</u> arXiv 2023<br />
Delbracio and Milanfar, <u>&quot;Inversion by Direct Iteration: An Alternative to Denoising Diffusion for Image Restoration&quot;,</u> arxiv 2023<br />
Stevens et al., <u>&quot;Removing Structured Noise with Diffusion Models&quot;,</u> arxiv 2023<br />
Wang et al., <u>&quot;Zero-Shot Image Restoration Using Denoising Diffusion Null-Space Model&quot;,</u> ICLR 2023<br />
Zhou et al., <u>&quot;3D Shape Generation and Completion through Point-Voxel Diffusion&quot;,</u> ICCV 2021<br />
Zeng et al., <u>&quot;LION: Latent Point Diffusion Models for 3D Shape Generation&quot;,</u> NeurIPS 2022<br />
Nichol et al., <u>&quot;Point-E: A System for Generating 3D Point Clouds from Complex Prompts&quot;,</u> arXiv 2022<br />
Chou et al., <u>&quot;DiffusionSDF: Conditional Generative Modeling of Signed Distance Functions&quot;,</u> arXiv 2022<br />
Cheng et al., <u>&quot;SDFusion: Multimodal 3D Shape Completion, Reconstruction, and Generation&quot;,</u> arXiv 2022<br />
Hui et al., <u>&quot;Neural Wavelet-domain Diffusion for 3D Shape Generation&quot;,</u> arXiv 2022<br />
Shue et al., <u>&quot;3D Neural Field Generation using Triplane Diffusion&quot;,</u> arXiv 2022<br />
Yang et al., <u>&quot;Learning a Diffusion Prior for NeRFs&quot;,</u> ICLR Workshop 2023<br />
Jun and Nichol, <u>&quot;Shap-E: Generating Conditional 3D Implicit Functions&quot;,</u> arXiv 2023<br />
Poole et al., <u>&quot;DreamFusion: Text-to-3D using 2D Diffusion&quot;,</u> arXiv 2022<br />
Lin et al., <u>&quot;Magic3D: High-Resolution Text-to-3D Content Creation&quot;,</u> arXiv 2022<br />
Wang et al., <u>&quot;Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation&quot;,</u> arXiv 2022<br />
Metzer et al., <u>&quot;Latent-NeRF for Shape-Guided Generation of 3D Shapes and Textures&quot;,</u> arXiv 2022<br />
Hong et al., <u>&quot;Debiasing Scores and Prompts of 2D Diffusion for Robust Text-to-3D Generation&quot;,</u> CVPR Workshop 2023<br />
Watson et al., <u>&quot;Novel View Synthesis with Diffusion Models&quot;,</u> arXiv 2022<br />
Chan et al., <u>&quot;Generative Novel View Synthesis with 3D-Aware Diffusion Models&quot;,</u> arXiv 2023<br />
Xu et al., <u>&quot;NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360Â° Views&quot;,</u> arXiv 2022<br />
Zhou and Tulsiani, <u>&quot;SparseFusion: Distilling View-conditioned Diffusion for 3D Reconstruction&quot;,</u> arXiv 2022</p>
<p>P82</p>
<h2 id="part-iii-contd"><a class="header" href="#part-iii-contd">Part III (contâ€™d)</a></h2>
<p>Seo et al., <u>&quot;DITTO-NeRF: Diffusion-based Iterative Text To Omni-directional 3D Model&quot;,</u> arXiv 2023<br />
Haque et al., <u>&quot;Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions&quot;,</u> arXiv 2023<br />
Sella et al., <u>&quot;Vox-E: Text-guided Voxel Editing of 3D Objects&quot;,</u> arXiv 2023<br />
Harvey et al., <u>&quot;Flexible Diffusion Modeling of Long Videos&quot;,</u> arXiv 2022<br />
Voleti et al., <u>&quot;MCVD: Masked Conditional Video Diffusion for Prediction, Generation, and Interpolation&quot;,</u> NeurIPS 2022<br />
Mei and Patel, <u>&quot;VIDM: Video Implicit Diffusion Models&quot;,</u> arXiv 2022<br />
Wang et al., <u>&quot;Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models&quot;,</u> arXiv 2023<br />
Ceylan et al., <u>&quot;Pix2Video: Video Editing using Image Diffusion&quot;,</u> arXiv 2023<br />
Esser et al., <u>&quot;Structure and Content-Guided Video Synthesis with Diffusion Models&quot;,</u> arXiv 2023<br />
JimÃ©nez, <u>&quot;Mixture of Diffusers for scene composition and high resolution image generation&quot;,</u> arXiv 2023<br />
Bar-Tal et al., <u>&quot;MultiDiffusion: Fusing Diffusion Paths for Controlled Image Generation&quot;,</u> arXiv 2023<br />
Zhang et al., <u>&quot;DiffCollage: Parallel Generation of Large Content with Diffusion Models&quot;,</u> CVPR 2023<br />
Zhang et al., <u>&quot;MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model&quot;,</u> arXiv 2022<br />
Tevet et al., <u>&quot;Human Motion Diffusion Model&quot;,</u> arXiv 2022<br />
Chen et al., <u>&quot;Executing your Commands via Motion Diffusion in Latent Space&quot;,</u> CVPR 2023<br />
Du et al., <u>&quot;Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model&quot;,</u> CVPR 2023<br />
Shafir et al., <u>&quot;Human Motion Diffusion as a Generative Prior&quot;,</u> arXiv 2023<br />
Somepalli et al., <u>&quot;Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models&quot;,</u> CVPR 2023<br />
Carlini et al., <u>&quot;Extracting Training Data from Diffusion Models&quot;,</u> arXiv 2023<br />
Gandikota et al., <u>&quot;Erasing Concepts from Diffusion Models&quot;,</u> arXiv 2023<br />
Kumari et al., <u>&quot;Ablating Concepts in Text-to-Image Diffusion Models&quot;,</u> arXiv 2023<br />
Somepalli et al., <u>&quot;Understanding and Mitigating Copying in Diffusion Models&quot;,</u> arXiv 2023</p>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="video-diffusion-modelsæ€ç»´å¯¼å›¾"><a class="header" href="#video-diffusion-modelsæ€ç»´å¯¼å›¾">Video Diffusion Modelsæ€ç»´å¯¼å›¾</a></h1>
<ul>
<li>Fundamentals of Diffusion Models</li>
<li><strong>Video Generation</strong>
<ul>
<li>Close-Source T2I Works</li>
<li>Open-Source T2I Base Models</li>
<li>Works Based on T2I Base Models</li>
<li>Works Based on T2V Base Models</li>
<li>é•¿è§†é¢‘ç”Ÿæˆ</li>
<li>StoryBoard</li>
<li>å¤šç”Ÿæˆä»»åŠ¡</li>
<li>Human Video Generation(<a href="VideoDiffusionModels/../HumanVideoGeneration.html">link</a>)</li>
</ul>
</li>
<li><strong>Video Editing</strong></li>
<li>è§†é¢‘ç”Ÿæˆçš„è¯„ä»·æŒ‡æ ‡</li>
<li>æ•°æ®é›†</li>
<li>Summary</li>
</ul>
<h1 id="reference-3"><a class="header" href="#reference-3">Reference</a></h1>
<p><strong>Mike Shou</strong></p>
<p>Asst Prof, National U. of Singapore</p>
<p>Joint work with Pei Yang &amp; Jay Wu</p>
<p>Slides:<a href="https://sites.google.com/view/showlab/tutorial">https://sites.google.com/view/showlab/tutorial</a> </p>
<p><img src="VideoDiffusionModels/../assets/08-001.png" alt="" /></p>
<p>P2</p>
<h2 id="others"><a class="header" href="#others">Others</a></h2>
<ul>
<li>CVPR Tutorial (English): <a href="https://www.youtube.com/watch?v=cS6JQpEY9cs">https://www.youtube.com/watch?v=cS6JQpEY9cs</a></li>
<li>Lilâ€™s blog: <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a></li>
<li>Hung-yi Lee (Chinese):
<ul>
<li><a href="https://www.youtube.com/watch?v=azBugJzmz-o">https://www.youtube.com/watch?v=azBugJzmz-o</a></li>
<li><a href="https://www.youtube.com/watch?v=ifCDXFdeaaM">https://www.youtube.com/watch?v=ifCDXFdeaaM</a></li>
</ul>
</li>
<li>Xing et al., â€œA Survey on Video Diffusion Models,â€ arXiv 2023. </li>
</ul>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P31</p>
<p><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-31.png" alt="" /></p>
<p>P34</p>
<h1 id="problem-definition"><a class="header" href="#problem-definition">Problem Definition</a></h1>
<p><strong>Text-Guided Video Generation</strong></p>
<p>è¾“å…¥ï¼šText promptï¼ˆæˆ–å…¶å®ƒæ§åˆ¶ä¿¡å·ï¼‰<br />
è¾“å‡ºï¼švideo</p>
<h2 id="t2i---t2v"><a class="header" href="#t2i---t2v">T2I -&gt; T2V</a></h2>
<blockquote>
<p>âœ… ç”±äºå·²æœ‰ä¸€ä¸ªå¼€æºçš„å¤§æ•°æ®æ–‡ç”Ÿå›¾é¢„è®­ç»ƒæ¨¡å‹Stale Diffusion Modelã€‚ä¸ºäº†å……åˆ†åˆ©ç”¨è¿™ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼Œé€šå¸¸çš„åšæ³•æ˜¯<strong>æŠŠè¿™ä¸ªæ–‡ç”Ÿå›¾æ¨¡å‹æ”¹é€ æˆæ–‡ç”Ÿè§†é¢‘æ¨¡å‹</strong>ã€‚å³ï¼Œä» 2D è¾“å‡ºå˜æˆ 3D è¾“å‡ºã€‚</p>
</blockquote>
<h2 id="t2it2v---ti2v"><a class="header" href="#t2it2v---ti2v">T2I/T2V -&gt; TI2V</a></h2>
<p>ç›´æ¥ä»æ–‡æœ¬ç”Ÿæˆè§†é¢‘ï¼Œå¾ˆéš¾å¯¹è§†é¢‘å†…å®¹è¿›è¡Œæ›´ç»†èŠ‚çš„æ§åˆ¶ï¼Œå› æ­¤æ¼”ç”Ÿå‡ºäº†Image-2-Videoä»»åŠ¡ã€‚I2Vé€šå¸¸æ˜¯é€šè¿‡åœ¨é¢„è®­ç»ƒT2Içš„åŸºç¡€ä¸Šï¼Œå¼•å…¥reference imageçš„æ³¨å…¥å’Œæ—¶åºå±‚æ¥å®ç°ã€‚ä¹Ÿå¯ä»¥é€šè¿‡ç›´æ¥åœ¨é¢„è®­ç»ƒçš„T2Vä¸Šå¢åŠ reference imageçš„æ³¨å…¥æ¥å®ç°ã€‚</p>
<h2 id="t2it2vti2v--å…¶å®ƒæ§åˆ¶ä¿¡å·"><a class="header" href="#t2it2vti2v--å…¶å®ƒæ§åˆ¶ä¿¡å·">T2I/T2V/TI2V + å…¶å®ƒæ§åˆ¶ä¿¡å·</a></h2>
<p>é€‰ä¸€ä¸ªåˆé€‚çš„ï¼ˆå¼€æºï¼‰é¢„è®­ç»ƒæ¨¡å‹ï¼Œåœ¨æ­¤åŸºç¡€ä¸Š</p>
<ul>
<li>æ³¨å…¥è‡ªå·±çš„æ§åˆ¶ä¿¡å·ï¼Œä¾‹å¦‚å›¾åƒã€æ§åˆ¶ç‚¹ã€å…‰æµã€æ‹–æ‹½ç­‰</li>
<li>æ„é€ ç‰¹å®šçš„ï¼ˆç›¸å¯¹äºè®­ç»ƒåŸºæ¨¡å‹æ¥è¯´ï¼‰å°‘é‡çš„è®­ç»ƒæ•°æ®</li>
<li>æ ¹æ®ä»»åŠ¡ç‰¹æ€§å¼•å…¥ä¸€äº›æŠ€å·§</li>
<li>ç»è¿‡ï¼ˆç›¸å¯¹äºè®­ç»ƒåŸºæ¨¡å‹æ¥è¯´ï¼‰å°‘é‡çš„è®­ç»ƒ
å°±å¾—åˆ°äº†é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„å‚åŸŸçš„è§†é¢‘ç”Ÿæˆæ¨¡å‹ã€‚</li>
</ul>
<p>å¯¹äºå¤§å¤šæ•°ç¤¾åŒºç©å®¶æ¥è¯´ï¼Œåªèƒ½è·å–åˆ°å¼€æºçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå› æ­¤è¦å…ˆäº†è§£å¯ç”¨çš„å¼€æºæ¨¡å‹ã€‚</p>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-30.png" alt="" /></p>
<p>P36</p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>55</td><td>2022</td><td>Video Diffusion Models</td><td>å¼•å…¥conv(2+1)Dï¼Œtemporal attention</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/55.html">link</a></td></tr>
<tr><td>56</td><td>2022</td><td>Make-A-Video: Text-to-Video Generation without Text-Video Data</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/56.html">link</a></td></tr>
<tr><td>48</td><td>2023</td><td>Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models</td><td>T2I(LDM) -&gt; T2V(SVD)<br>Cascaded generation</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/48.html">link</a></td></tr>
<tr><td>57</td><td>2023</td><td>Zhang et al., â€œShow-1: Marrying Pixel and Latent Diffusion Models for Text-to-Video Generation,â€</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/57.html">link</a></td></tr>
<tr><td>59</td><td>2023</td><td>Guo et al., â€œAnimateDiff: Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning,â€ arXiv 2023.</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/55.html">link</a></td></tr>
<tr><td></td><td>2022</td><td>Imagen Video: Ho et al., â€œImagen Video: High Definition Video Generation with Diffusion Models,â€ arXiv 2022.</td><td>Leverage pretrained T2I models for video generation; Cascaded generation<br> âœ… å…ˆåœ¨ image ä¸Šåš cascade ç”Ÿæˆ <br> âœ… è§†é¢‘æ˜¯åœ¨å›¾åƒä¸Šå¢åŠ æ—¶é—´ç»´åº¦çš„è¶…åˆ†   <br> âœ… æ¯æ¬¡çš„è¶…åˆ†éƒ½æ˜¯ç‹¬ç«‹çš„ diffusion model?   <br> â“ temporal è¶…åˆ†å…·ä½“æ˜¯æ€ä¹ˆåšçš„ï¼Ÿ<br> 7 cascade models in total.  <br> 1 Base model (16x40x24) <br> 3 Temporal super-resolution models. <br> 3 Spatial super-resolution models. <br> âœ… é€šè¿‡ 7 æ¬¡ cascadeï¼Œé€æ­¥æå‡é¡ºç‡å’Œåƒç´ çš„åˆ†è¾¨ç‡ï¼Œæ¯ä¸€æ­¥çš„è®­ç»ƒå¯¹ä¸Šä¸€æ­¥æ˜¯ä¾èµ–çš„ã€‚</td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-63-1.png" alt="" /> <br> <img src="VideoDiffusionModels/VideoGeneration/../../assets/08-63-2.png" alt="" /><br><img src="VideoDiffusionModels/VideoGeneration/../../assets/D3-52.png" alt="" /></td><td></td></tr>
<tr><td></td><td>2023</td><td>Chen et al., â€œGenTron: Delving Deep into Diffusion Transformers for Image and Video Generation,â€</td><td>Transformer-based diffusion for text-to-video generation<br> âœ…Transformer-based architecture extended from DiT (class-conditioned transformer-based LDM) <br> âœ…Train T2I \(\to \)  insert temporal self-attn \(\to \) joint image-video finetuning (motion-free guidance)</td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-91.png" alt="" /></td><td></td></tr>
<tr><td></td><td>2023</td><td>Gupta et al., â€œPhotorealistic Video Generation with Diffusion Models,â€</td><td>Transformer-based diffusion for text-to-video generation<br> âœ…Transformer-based denoising diffusion backbone<br> âœ…Joint image-video training via unified image/video latent space (created by a joint 3D encoder with causal 3D conv layers, allowing the first frame of a video to be tokenized independently)<br> âœ…Window attention to reduce computing/memory costs<br> âœ…Cascaded pipeline for high-quality generation</td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-93.png" alt="" /></td><td></td></tr>
</tbody></table>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P67<br />
<img src="VideoDiffusionModels/VideoGeneration/../../assets/08-67.png" alt="" /></p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>58</td><td>2023</td><td>Wang et al., â€œModelScope Text-to-Video Technical Report,â€</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/58.html">link</a></td></tr>
<tr><td></td><td>2023</td><td>ZeroScope</td><td>âœ… ZeroScope åœ¨ ModelScope ä¸Š finetuneï¼Œä½¿ç”¨äº†éå¸¸å°ä½†è´¨é‡éå¸¸é«˜çš„æ•°æ®ï¼Œå¾—åˆ°äº†é«˜åˆ†è¾¨ç‡çš„ç”Ÿæˆæ•ˆæœã€‚</td><td></td><td></td></tr>
<tr><td>50</td><td>2023</td><td>Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets</td><td>Scaling latent video diffusion models to large datasets<br><strong>Data Processing and Annotation</strong></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/50.html">link</a></td></tr>
<tr><td></td><td>2024</td><td>CogVideo</td><td>Transformer Based</td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>Wang et al., â€œLAVIE: High-Quality Video Generation with Cascaded Latent Diffusion Models,â€</td><td>Joint image-video finetuning with curriculum learning<br> âœ… æä¾›äº†ä¸€å¥—é«˜è´¨é‡æ•°æ®é›†ï¼Œç”Ÿæˆçš„è§†é¢‘è´¨é‡ä¹Ÿæ›´å¥½ï¼ˆè®­ç»ƒé›†å¾ˆé‡è¦ï¼‰ã€‚</td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-81.png" alt="" /></td><td></td></tr>
<tr><td></td><td>2023</td><td>Chen et al., â€œVideoCrafter1: Open Diffusion Models for High-Quality Video Generation,â€</td><td>Latent diffusion inserted with temporal layers</td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-80.png" alt="" /></td><td></td></tr>
</tbody></table>
<p>P74</p>
<h3 id="å…¶å®ƒç›¸å…³å·¥ä½œ"><a class="header" href="#å…¶å®ƒç›¸å…³å·¥ä½œ">å…¶å®ƒç›¸å…³å·¥ä½œ</a></h3>
<table><thead><tr><th></th><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>&quot; Robot dancing in times square,â€ arXiv 2023.</td><td>&quot; Clown fish swimming through the coral reef,â€ arXiv 2023.</td><td>&quot; Melting ice cream dripping down the cone,â€ arXiv 2023.</td><td>&quot; Hyper-realistic photo of an abandoned industrial site during a storm,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-74-1.png" alt="" /></td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-74-2.png" alt="" /></td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-74-3.png" alt="" /></td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-74-4.png" alt="" /></td></tr>
</tbody></table>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P75</p>
<h1 id="text-2-video"><a class="header" href="#text-2-video">Text-2-Video</a></h1>
<p>P102</p>
<h2 id="text2video-zero"><a class="header" href="#text2video-zero">Text2Video-Zero</a></h2>
<p>Use Stable Diffusion to generate videos without any finetuning</p>
<blockquote>
<p>âœ… å®Œå…¨æ²¡æœ‰ç»è¿‡è®­ç»ƒï¼Œä½¿ç”¨ T2I Base Model(stable diffusion Model) ç”Ÿæˆè§†é¢‘ã€‚</p>
</blockquote>
<p><strong>Motivation: How to use Stable Diffusion for video generation without finetuning?</strong></p>
<ul>
<li>Start from noises of similar pattern</li>
<li>Make intermediate features of different frames to be similar</li>
</ul>
<p>Khachatryan et al., â€œText2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators,â€ arXiv 2023.</p>
<p>P103</p>
<h3 id="step-1"><a class="header" href="#step-1">Step 1</a></h3>
<ul>
<li>Start from noises of similar pattern: given the first frameâ€™s noise, define a global scene motion, used to translate the first frameâ€™s noise to generate similar initial noise for other frames</li>
</ul>
<p><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-103.png" alt="" /> </p>
<p>P104</p>
<h3 id="step2"><a class="header" href="#step2">Step2</a></h3>
<ul>
<li>Make intermediate features of different frames to be similar: always use K and V from the first frame in self-attention</li>
</ul>
<p><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-104.png" alt="" /></p>
<blockquote>
<p>âœ… ç”Ÿæˆç”µå½±çº§åˆ«çš„è§†é¢‘ï¼Œè€Œä¸æ˜¯å‡ ç§’é’Ÿçš„è§†é¢‘ã€‚</p>
</blockquote>
<p>P105</p>
<h3 id="step3"><a class="header" href="#step3">Step3</a></h3>
<ul>
<li>Optional background smoothing: regenerate the background, average with the first frame</li>
</ul>
<p><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-105.png" alt="" /> </p>
<p>P106</p>
<blockquote>
<p>âœ… æ–‡æœ¬ â†’ ç»“æ„åŒ–çš„ä¸­é—´è„šæœ¬ â†’ è§†é¢‘</p>
</blockquote>
<h2 id="more-works-1"><a class="header" href="#more-works-1">More Works</a></h2>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-107-1.png" alt="" /></td><td><strong>MagicVideo</strong> (Zhou et al.) <br> Insert causal attention to Stable Diffusion for better temporal coherence <br> â€œMagicVideo: Efficient Video Generation With Latent Diffusion Models,â€ arXiv 2022.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-107-2.png" alt="" /></td><td><strong>Simple Diffusion Adapter</strong> (Xing et al.) <br> Insert lightweight adapters to T2I models, shift latents, and finetune adapters on videos <br>â€œSimDA: Simple Diffusion Adapter for Efficient Video Generation,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-107-3.png" alt="" /></td><td><strong>Dual-Stream Diffusion Net</strong> (Liu et al.) <br> Leverage multiple T2I networks for T2V <br> â€œDual-Stream Diffusion Net for Text-to-Video Generation,â€ arXiv 2023.</td></tr>
<tr><td></td><td>MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation,2024</td></tr>
</tbody></table>
<blockquote>
<p>âœ… ç”¨çº¯æ–‡æœ¬çš„å½¢å¼æŠŠå›¾ç‰‡æè¿°å‡ºæ¥ã€‚<br />
âœ… æ–¹æ³•ï¼šå‡†å¤‡å¥½ pair dataï¼Œå¯¹ GPT åš fine-tune.<br />
âœ… ç”¨ç»“æ„åŒ–çš„ä¸­é—´è¡¨ç¤ºç”Ÿæˆå›¾ç‰‡ã€‚<br />
âœ… å…ˆç”¨ GPT è¿›è¡Œæ–‡æœ¬è¡¥å…¨ã€‚</p>
</blockquote>
<h1 id="image-2-video"><a class="header" href="#image-2-video">Image-2-Video</a></h1>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>51</td><td>2023</td><td>Motion-Conditioned Diffusion Model for Controllable Video Synthesis</td><td>âœ… ç”¨æˆ·æä¾›çš„ç¨€ç–è¿åŠ¨è½¨è¿¹ -&gt; denseå…‰æµ<br>âœ… denseå…‰æµï¼ˆconditionï¼‰ + Image -&gt; è§†é¢‘</td><td>Two-stage,  è‡ªå›å½’ç”Ÿæˆ</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/51.html">link</a></td></tr>
<tr><td>44</td><td>2024</td><td>Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling</td><td>âœ… ç”¨æˆ·æä¾›çš„æ§åˆ¶ä¿¡å·ï¼ˆconditionï¼‰+ Image -&gt; denseå…‰æµ<br>âœ… denseå…‰æµï¼ˆconditionï¼‰ + Image -&gt; è§†é¢‘</td><td>Two-stage</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/44.html">link</a></td></tr>
<tr><td></td><td>2023</td><td><strong>LFDM</strong> (Ni et al.) <br> â€œConditional Image-to-Video Generation with Latent Flow Diffusion Models,â€</td><td>âœ…è§†é¢‘-&gt;å…‰æµ + Mask<br>âœ… å…‰æµ+Mask+å›¾åƒ -&gt;è§†é¢‘</td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-141-3.png" alt="" /></td><td></td></tr>
<tr><td></td><td>2024</td><td>Generative Image Dynamics (Li et al.) <br> â€œGenerative Image Dynamics,â€</td><td>å›¾åƒï¼ˆæ— conditionï¼‰ -&gt; SV <br>âœ… SV + åŠ› -&gt; å…‰æµ <br>âœ… å…‰æµ + Image -&gt; è§†é¢‘</td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-141-2.png" alt="" /></td><td></td></tr>
<tr><td></td><td>2023</td><td>LaMD: Latent Motion Diffusion for Video Generation</td><td>è§†é¢‘ -&gt; å›¾åƒç‰¹å¾ + è¿åŠ¨ç‰¹å¾<br>âœ… è¿åŠ¨ç‰¹å¾+å›¾åƒç‰¹å¾-&gt;è§†é¢‘</td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-141-2.png" alt="" /></td><td></td></tr>
</tbody></table>
<h2 id="more-works-é—­æº"><a class="header" href="#more-works-é—­æº">More Works é—­æº</a></h2>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-95-1.png" alt="" /></td><td><strong>Latent Shift</strong> (An et al.)<br>Shift latent features for better temporal coherence <br> â€œLatent-Shift: Latent Diffusion with Temporal Shift for Efficient Text-to-Video Generation,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-95-2.png" alt="" /></td><td><strong>Video Factory</strong> (Wang et al.)<br> Modify attention mechanism for better temporal coherence <br> â€œVideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-95-3.png" alt="" /></td><td><strong>PYoCo</strong> (Ge et al.)<br> Generate video frames starting from similar noise patterns <br> â€œPreserve Your Own Correlation: A Noise Prior for Video Diffusion Models,â€ ICCV 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-95-4.png" alt="" /></td><td><strong>VideoFusion</strong> (Lorem et al.)<br> Decompose noise into shared â€œbaseâ€ and individual â€œresidualsâ€<br>â€œVideoFusion: ecomposed Diffusion Models for High-Quality Video Generation,â€ CVPR 2023.</td></tr>
</tbody></table>
<blockquote>
<p>âœ… Framwork (1) åœ¨åŸæ¨¡å‹ä¸­åŠ å…¥ temporal layers (2) fix åŸæ¨¡å‹ï¼Œè®­ç»ƒæ–°çš„ layers (3) æŠŠ lager æ’å…¥åˆ°ç›®æ ‡ T2 I æ¨¡å‹ä¸­ã€‚</p>
</blockquote>
<h1 id="sound2video"><a class="header" href="#sound2video">Sound2Video</a></h1>
<h2 id="the-power-of-sound-tpos"><a class="header" href="#the-power-of-sound-tpos">The Power of Sound (TPoS)</a></h2>
<p>Sound- and text-guided video generation</p>
<ul>
<li>Input/output: a text prompt + an audio segment â†’ a video</li>
</ul>
<p><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-139.png" alt="" /> </p>
<p>Jeong et al., â€œThe Power of Sound (TPoS): Audio Reactive Video Generation with Stable Diffusion,â€ ICCV 2023.</p>
<h2 id="more-works-2"><a class="header" href="#more-works-2">More Works</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2023</td><td><strong>AADiff</strong>: Audio-Aligned Video Synthesis with Text-to-Image Diffusion</td><td></td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-140-1.png" alt="" /></td><td></td></tr>
<tr><td></td><td>2023</td><td><strong>Generative Disco</strong> (Liu et al.)<br> â€œGenerative Disco: Text-to-Video Generation for Music Visualization,</td><td></td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-140-2.png" alt="" /></td><td></td></tr>
</tbody></table>
<h1 id="bain-activity-2-video"><a class="header" href="#bain-activity-2-video">Bain Activity 2 Video</a></h1>
<blockquote>
<p>âœ… å¤§è„‘ä¿¡å·æ§åˆ¶ç”Ÿæˆã€‚</p>
</blockquote>
<p>Brain activity-guided video generation</p>
<ul>
<li>Task: human vision reconstruction via fMRI signal-guided video generation</li>
</ul>
<p><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-142.png" alt="" /> </p>
<p>Chen et al., â€œCinematic Mindscapes: High-quality Video Reconstruction from Brain Activity,â€ arXiv 2023.</p>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="image-2-video-1"><a class="header" href="#image-2-video-1">Image-2-Video</a></h1>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>47</td><td>2024</td><td>Puppet-Master: Scaling Interactive Video Generation as a Motion Prior for Part-Level Dynamics</td><td>æ‹–æ‹½æ§åˆ¶çš„å¯¹è±¡é›¶ä»¶çº§è¿åŠ¨çš„è§†é¢‘ç”Ÿæˆ</td><td>é›¶ä»¶çº§è¿åŠ¨æ•°æ®é›†</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/47.html">link</a></td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><p>P108 </p>
<h1 id="25-storyboard"><a class="header" href="#25-storyboard">2.5 Storyboard</a></h1>
<p>P109<br />
<img src="VideoDiffusionModels/VideoGeneration/../../assets/08-109.png" alt="" /> </p>
<p>P110 </p>
<h2 id="what-is-a-storyboard"><a class="header" href="#what-is-a-storyboard">What is a storyboard?</a></h2>
<blockquote>
<p>âœ… éš¾ç‚¹ï¼šä¿æŒå†…å®¹çš„ä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p>P111</p>
<p>A concept in film production</p>
<p><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-111.png" alt="" /> </p>
<ul>
<li>Rough sketches/drawings with notes</li>
<li>Example: Inception by Christopher Nola</li>
</ul>
<p>Storyboard image from deviantart.com.</p>
<p>P112 </p>
<h2 id="how-to-generate-such-a-storyboard"><a class="header" href="#how-to-generate-such-a-storyboard">How to generate such a storyboard?</a></h2>
<ul>
<li>
<p>As humans, over the years, we have acquired such â€œvisual priorâ€ about object location, object shape, relation, etc.</p>
</li>
<li>
<p>Can LLM model such visual prioï¼Ÿ</p>
</li>
</ul>
<p>P113</p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>61</td><td>2023</td><td>Xie et al., â€œVisorGPT: Learning Visual Prior via Generative Pre-Training,â€</td><td>A â€œdiffusion over diffusionâ€ architecture for very long video generation</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/61.html">link</a></td></tr>
<tr><td></td><td>2023</td><td>Lin et al., â€œVideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning,â€</td><td>Use storyboard as condition to generate video<br> âœ… Control Netï¼ŒæŠŠæ–‡æœ¬è½¬ä¸º Pixel å›¾ç‰‡ã€‚</td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-121.png" alt="" /> <img src="VideoDiffusionModels/VideoGeneration/../../assets/08-122.png" alt="" /></td><td></td></tr>
<tr><td></td><td>2024</td><td>Xie et al., â€œLearning Long-form Video Prior via Generative Pre-Training,â€</td><td>GPT can be trained to learn better long-form video prior (e.g., object position, relative size, human interaction)<br> âœ… ç”¨ GPT-4 In-context learning æœºåˆ¶ç”Ÿæˆç»“æ„åŒ–æ–‡æœ¬<br> âœ… GPT ç¼ºå°‘ä¸€äº›è§†è§‰ä¸Šçš„ commen sense ä¸»è¦æ˜¯ç¼ºå°‘ç›¸å…³æ•°æ®é›†ã€‚ <br> âœ… å› æ­¤è¿™é‡Œæä¾›äº†ä¸€ä¸ªæ•°æ®é›†<strong>Storyboard20K</strong>ã€‚</td><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-124.png" alt="" /></td><td><a href="https://github.com/showlab/Long-form-Video-Prior">dataset</a></td></tr>
<tr><td>41</td><td>2024</td><td>STORYDIFFUSION: CONSISTENT SELF-ATTENTION FOR LONG-RANGE IMAGE AND VIDEO GENERATION</td><td>å…ˆç”Ÿæˆä¸€è‡´çš„å…³é”®å¸§ï¼Œå†æ’å¸§æˆä¸­é—´å›¾åƒ</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/41.html">link</a></td></tr>
</tbody></table>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-125-1.png" alt="" /></td><td><strong>Dysen-VDM</strong> (Fei et al.)<br>Storyboard through scene graphs<br>â€œEmpowering Dynamics-aware Text-to-Video Diffusion with Large Language Models,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-125-2.png" alt="" /></td><td><strong>DirectT2V</strong> (Hong et al.) <br> Storyboard through bounding boxes <br> â€œLarge Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-125-3.png" alt="" /></td><td><strong>Free-Bloom</strong> (Huang et al.)<br>Storyboard through detailed text prompts<br> â€œFree-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator,â€ NeurIPS 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-125-4.png" alt="" /></td><td><strong>LLM-Grounded Video Diffusion Models</strong> (Lian et al.) <br> Storyboard through foreground bounding boxes <br> â€œLLM-grounded Video Diffusion Models,â€ arXiv 2023.</td></tr>
</tbody></table>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P126</p>
<h1 id="26-long-video-generation"><a class="header" href="#26-long-video-generation">2.6 Long video generation</a></h1>
<p>P127</p>
<p><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-127.png" alt="" /> </p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>60</td><td>2023</td><td>Yin et al., â€œNUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation,â€</td><td>A â€œdiffusion over diffusionâ€ architecture for very long video generation</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/60.html">link</a></td></tr>
</tbody></table>
<p>P133</p>
<h2 id="long-video-generation-more-works"><a class="header" href="#long-video-generation-more-works">Long Video Generation: More Works</a></h2>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-133-1.png" alt="" /></td><td><strong>Latent Video Diffusion Models for High-Fidelity Long Video Generation</strong> (He et al.) <br> Generate long videos via autoregressive generation &amp; interpolation <br> â€œLatent Video Diffusion Models for High-Fidelity Long Video Generation,â€ arXiv 2022.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-133-2.png" alt="" /></td><td><strong>VidRD</strong> (Gu et al.) <br> Autoregressive long video generation <br> â€œReuse and Diffuse: Iterative Denoising for Text-to-Video Generation,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-133-3.png" alt="" /></td><td><strong>VideoGen</strong> (Li et al.) <br> Cascaded pipeline for long video generation <br> â€œVideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation,â€ arXiv 2023.</td></tr>
</tbody></table>
<blockquote>
<p>âœ… å·²æœ‰ä¸€æ®µè§†é¢‘ï¼Œé€šè¿‡ guidance æˆ–æ–‡æœ¬æè¿°ï¼Œä¿®æ”¹è§†é¢‘ã€‚</p>
</blockquote>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P139</p>
<blockquote>
<p>âœ… ç”¨æ–‡ç”Ÿå›¾æ¨¡å‹ç”Ÿæˆ appearance, dynamics æ¥è‡ªäº reference video.</p>
</blockquote>
<p>P141</p>
<blockquote>
<p>âœ… å½“å‰å¸§åªä¸ä¸Šå¸§å’Œå‰ä¸€å¸§åš attentionï¼Œå¤§å¤§å‡å°‘è®¡ç®—é‡ã€‚<br />
âœ… åœ¨æ‰€æœ‰å¸§ä¸Šåš attention å¼€é”€æ¯”è¾ƒå¤§ã€‚<br />
âœ… è§£å†³æ–¹æ³•ï¼šå‰ä¸€å¸§ä¸ç¬¬ä¸€å¸§ã€‚<br />
â“ æ€ä¹ˆä¿è¯ç”ŸæˆåŠ¨ä½œä¸åŸè§†é¢‘åŠ¨ä½œçš„ä¸€è‡´æ€§å‘¢?</p>
</blockquote>
<p>P142</p>
<blockquote>
<p>âœ… å¯¹è¦ç¼–è¾‘çš„è§†é¢‘ï¼Œå…ˆ DDIM Inversionï¼Œå¾—åˆ° inverfed noiseï¼Œè¿™æ˜¯ä¿ç•™äº†åŸè§†é¢‘ pattern çš„ noise.<br />
âœ… ç”¨è¿™ä¸ª noise ä½œä¸º init noiseï¼Œè¿˜åŸå‡ºçš„è§†é¢‘è·ŸåŸè§†é¢‘æœ‰æ¯”è¾ƒå¥½çš„ç»“æ„åŒ–ä¿ç•™ã€‚<br />
âœ… è§£æ³•æ–¹æ³•</p>
</blockquote>
<p>P144</p>
<h1 id="å¤šç”Ÿæˆä»»åŠ¡"><a class="header" href="#å¤šç”Ÿæˆä»»åŠ¡">å¤šç”Ÿæˆä»»åŠ¡</a></h1>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-144-1.png" alt="" /></td><td><strong>MovieFactory</strong> (Zhu et al.) <br> â€œMovieFactory: Automatic Movie Creation from Text using Large Generative Models for Language and Images,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-144-2.png" alt="" /></td><td><strong>CoDi</strong> (Tang et al.) <br> â€œAny-to-Any Generation via Composable Diffusion,â€ NeurIPS 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-144-3.png" alt="" /></td><td><strong>MM-Diffusion</strong> (Ruan et al.) <br> â€œMM-Diffusion: Learning Multi-Modal Diffusion Models for Joint Audio and Video Generation,â€ CVPR 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoGeneration/../../assets/08-144-4.png" alt="" /></td><td><strong>NExT-GPT</strong> (Wu et al.) <br> â€œNExT-GPT: Any-to-Any Multimodal LLM,â€ arXiv 2023.</td></tr>
<tr><td></td><td></td></tr>
</tbody></table>
<blockquote>
<p>âœ… åœ¨ç‰©ä½“æ”¹å˜æ¯”è¾ƒå¤§çš„æƒ…å†µä¸‹ï¼Œdiffusion æ¯”å…¶å®ƒç”Ÿæˆæ–¹æ³•æ•ˆæœæ›´å¥½ã€‚</p>
</blockquote>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="video-editing"><a class="header" href="#video-editing">Video Editing</a></h1>
<div style="break-before: page; page-break-before: always;"></div><p>P147</p>
<h1 id="3-video-editing"><a class="header" href="#3-video-editing">3 Video Editing</a></h1>
<h2 id="31-tuning-based"><a class="header" href="#31-tuning-based">3.1 Tuning-based</a></h2>
<p>P148</p>
<h2 id="one-shot-tuned"><a class="header" href="#one-shot-tuned">One-Shot Tuned</a></h2>
<p>P149 </p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-149.png" alt="" /> </p>
<p>P150</p>
<h2 id="tune-a-video"><a class="header" href="#tune-a-video">Tune-A-Video</a></h2>
<p>One-shot tuning of T2I models for T2V generation/editing</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/D3-66-3.png" alt="" /></p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-150.png" alt="" /> </p>
<p>Wu et al., â€œTune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation,â€ ICCV 2023.</p>
<p><a href="https://github.com/showlab/Tune-A-Video">https://github.com/showlab/Tune-A-Video</a></p>
<h3 id="motivation"><a class="header" href="#motivation"><strong>Motivation</strong></a></h3>
<p>Motivation: appearance from pretrained T2I models, dynamics from a reference video </p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-152.png" alt="" /></p>
<p>P153</p>
<h3 id="æ–¹æ³•-1"><a class="header" href="#æ–¹æ³•-1">æ–¹æ³•</a></h3>
<p><strong>Obs #1: Still images that accurately represent the verb terms</strong></p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-153-1.png" alt="" /> </p>
<p><strong>Obs #2: Extending attention to spatio-temporal yields consistent content</strong></p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-153-2.png" alt="" /> </p>
<p>P154</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-154.png" alt="" /> </p>
<p>P155</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-155.png" alt="" /> </p>
<p><strong>Full finetuning</strong>: finetunes the entire network</p>
<ul>
<li>inefficient, especially when #frames increases;</li>
<li>prone to overfitting â†’ poor editing ability.</li>
</ul>
<p><strong>Our tuning strategy</strong>: update the specific projection matrices</p>
<ul>
<li>parameter efficient and fast (~10 min);</li>
<li>retains the original property of pre-trained T2I diffusion models.</li>
</ul>
<p>\begin{align*} \mathcal{V} ^\ast =\mathcal{D} (\mathrm{DDIM-samp} (\mathrm{DDIM-inv} (\varepsilon (\mathcal{V} )),\tau^\ast  ))\end{align*}</p>
<p><strong>Structure guidance via DDIM inversion</strong></p>
<ul>
<li>preserves the structural information</li>
<li>improves temporal consistency</li>
</ul>
<p>P156</p>
<h3 id="ä¸»è§‚æ•ˆæœ"><a class="header" href="#ä¸»è§‚æ•ˆæœ">ä¸»è§‚æ•ˆæœ</a></h3>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-156.png" alt="" /><br />
P157<br />
<img src="VideoDiffusionModels/VideoEditing/../../assets/08-157.png" alt="" /> 
P158<br />
<img src="VideoDiffusionModels/VideoEditing/../../assets/08-158.png" alt="" /> 
P159<br />
<img src="VideoDiffusionModels/VideoEditing/../../assets/08-159.png" alt="" /><br />
P160<br />
<img src="VideoDiffusionModels/VideoEditing/../../assets/08-160.png" alt="" /> </p>
<p>P161</p>
<h3 id="å®¢è§‚æŒ‡æ ‡"><a class="header" href="#å®¢è§‚æŒ‡æ ‡">å®¢è§‚æŒ‡æ ‡</a></h3>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-161.png" alt="" /> </p>
<p><strong>Automatic metrics â€“ CLIP Score</strong></p>
<ul>
<li><em>Frame Consistency</em>: the average cosine similarity between all pairs of video frames</li>
<li><em>Textual Alignment</em>: average CLIP score between all frames of output videos and corresponding edited prompts</li>
</ul>
<p><strong>User study</strong> </p>
<p>Compare two videos generated by our method and a baseline (shown in random order):</p>
<ul>
<li><em>Which video has better temporal consistency?</em></li>
<li><em>Which video better aligns with the textual description?</em></li>
</ul>
<p>Wu et al., â€œTune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation,â€ ICCV 2023.</p>
<blockquote>
<p>âœ… base modelï¼šæ²¡æœ‰å¤ªå¤š motion.</p>
</blockquote>
<p>P162</p>
<h2 id="dreamix"><a class="header" href="#dreamix">Dreamix</a></h2>
<p>Few-shot finetuning for personalized video editing</p>
<p><strong>Main idea: Mixed Video-Image Finetuning</strong></p>
<ul>
<li>Finetune Imagen Video (Ho et al., 2022) which is a strong video foundation model</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-162-1.png" alt="" /> </p>
<ul>
<li>Finetuned to generate individual frames (bypassing temporal attentions) &amp; video</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-162-2.png" alt="" /> </p>
<p>Molad et al, â€œDreamix: Video Diffusion Models are General Video Editors,â€ arXiv 2023.</p>
<p>P163</p>
<h2 id="dreamix-1"><a class="header" href="#dreamix-1">Dreamix</a></h2>
<p>Few-shot finetuning for personalized video editing</p>
<p><strong>Inference Overview</strong></p>
<ul>
<li>Corrupt the input video by downsampling and add noise</li>
<li>Apply the finetuned video diffusion model to denoise and upscale</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-163.png" alt="" /> </p>
<p>Molad et al, â€œDreamix: Video Diffusion Models are General Video Editors,â€ arXiv 2023.</p>
<p>P164</p>
<h2 id="one-shot-tuned-video-editing-more-works"><a class="header" href="#one-shot-tuned-video-editing-more-works">One-Shot Tuned Video Editing: More Works</a></h2>
<blockquote>
<p>éƒ¨åˆ†ç¬”è®°ç§»è‡³Mike Shouç« èŠ‚</p>
</blockquote>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-164-1.png" alt="" /></td><td>\(\mathbf{EI^2}\) (Zhang et al.)<br> Modify self-attention for better temporal consistency <br> â€œTowards Consistent Video Editing with Text-to-Image rDiffusion Models,â€ arXiv 2023.</td></tr>
</tbody></table>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2023</td><td>Video-P2P: Video Editing with Cross-attention Control</td><td>Improve input-output semantic consistency of video editing via shared embedding optimization and cross-attention controlã€‚<br><img src="VideoDiffusionModels/VideoEditing/../../assets/08-164-2.png" alt="" /></td><td>attentionæ§åˆ¶</td><td></td></tr>
</tbody></table>
<blockquote>
<p>âœ… ä¸éœ€è¦è®­ç»ƒçš„æ–¹å¼ã€‚</p>
</blockquote>
<p>P165</p>
<h2 id="one-shot-tuned-video-editing-more-works-1"><a class="header" href="#one-shot-tuned-video-editing-more-works-1">One-Shot Tuned Video Editing: More Works</a></h2>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-165.png" alt="" /> </p>
<p><strong>Compared to training-free editing methods:</strong> </p>
<ul>
<li>Cons: still need 1 video for training</li>
<li>Pros: supports significant shape change </li>
</ul>
<p>Wu et al., â€œTune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation,â€ ICCV 2023.</p>
<p>P166</p>
<h2 id="multiple-shot-tuned"><a class="header" href="#multiple-shot-tuned">Multiple-Shot Tuned</a></h2>
<p>Video Editing: Text Conditioned</p>
<p>P167</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-167.png" alt="" /> </p>
<p>P168</p>
<h2 id="motiondirector"><a class="header" href="#motiondirector">MotionDirector</a></h2>
<p>Tune on multiple videos of a motion to be customised</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-168-1.png" alt="" /> </p>
<p>Zhao et al., â€œMotionDirector: Motion Customization of Text-to-Video Diffusion Models,â€ arXiv 2023.</p>
<p>P169</p>
<h2 id="motiondirector-1"><a class="header" href="#motiondirector-1">MotionDirector</a></h2>
<p>Tune on multiple videos of a motion to be customised</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-169.png" alt="" /> </p>
<p>Zhao et al., â€œMotionDirector: Motion Customization of Text-to-Video Diffusion Models,â€ arXiv 2023.</p>
<p>P170</p>
<h2 id="motiondirector-2"><a class="header" href="#motiondirector-2">MotionDirector</a></h2>
<p>Tune on multiple videos of a motion to be customised</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-170.png" alt="" /> </p>
<p>Zhao et al., â€œMotionDirector: Motion Customization of Text-to-Video Diffusion Models,â€ arXiv 2023</p>
<p>P171</p>
<h2 id="motiondirector-3"><a class="header" href="#motiondirector-3">MotionDirector</a></h2>
<h2 id="tune-on-multiple-videos-of-a-motion-to-be-customised"><a class="header" href="#tune-on-multiple-videos-of-a-motion-to-be-customised">Tune on multiple videos of a motion to be customised</a></h2>
<ul>
<li>MokonDirector can customize foundakon models to generate videos with desired mokons.</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-171.png" alt="" /> </p>
<p>Zhao et al., â€œMotionDirector: Motion Customization of Text-to-Video Diffusion Models,â€ arXiv 2023.</p>
<p>P172</p>
<h2 id="motiondirector-4"><a class="header" href="#motiondirector-4">MotionDirector</a></h2>
<p>Tune on multiple videos of a motion to be customised</p>
<ul>
<li>The challenge is generalizing the learned motions to diverse appearance.</li>
<li>MotionDirector learns the appearances and motions in reference videos in a decoupled way, to avoid overfitting on the limited appearances.</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-172.png" alt="" /> </p>
<p>Zhao et al., â€œMotionDirector: Motion Customization of Text-to-Video Diffusion Models,â€ arXiv 2023.</p>
<p>P173</p>
<h2 id="motiondirector-5"><a class="header" href="#motiondirector-5">MotionDirector</a></h2>
<p>Tune on multiple videos of a motion to be customised</p>
<ul>
<li>Decouple appearance and motion.</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-173.png" alt="" /> </p>
<p>Zhao et al., â€œMogonDirector: Mogon Customizagon of Text-to-Video Diffusion Models,â€ arXiv 2023.</p>
<p>P174</p>
<h2 id="motiondirector-6"><a class="header" href="#motiondirector-6">MotionDirector</a></h2>
<p>Tune on muleple videos of a moeon to be customised</p>
<ul>
<li>Comparing with other methods.</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-174.png" alt="" /> </p>
<p>Zhao et al., â€œMotionDirector: Motion Customization of Text-to-Video Diffusion Models,â€ arXiv 2023.</p>
<p>P175</p>
<h2 id="motiondirector-7"><a class="header" href="#motiondirector-7">MotionDirector</a></h2>
<p>Tune on multiple videos of a motion to be customised</p>
<ul>
<li>Comparing with other methods.</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-175.png" alt="" /> </p>
<p>Zhao et al., â€œMotionDirector: Motion Customization of Text-to-Video Diffusion Models,â€ arXiv 2023.</p>
<p>P176</p>
<h2 id="mofondirector"><a class="header" href="#mofondirector">MofonDirector</a></h2>
<p>Tune on multiple videos of a motion to be customised</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-176.png" alt="" /> </p>
<p>Zhao et al., â€œMotionDirector: Motion Customization of Text-to-Video Diffusion Models,â€ arXiv 2023.</p>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<blockquote>
<p>âœ… åœ¨ä¸€ä¸ªè§†é¢‘ä¸Šè®­ç»ƒåå¯ä»¥å¯¹è§†é¢‘è¿›è¡Œç¼–è¾‘ã€‚<br />
âœ… è®­ç»ƒè¿‡ç¨‹ï¼š(1) å¯¹æ¨¡å‹çš„æ—¶åŸŸæ¨¡å— finetuneï¼<br />
âœ… (2) å¯¹å›¾åƒæ‰“ä¹±åç”¨å›¾åƒ finetuneï¼<br />
âœ… æŠŠè§†é¢‘å’Œå›¾ç‰‡è¿›è¡Œ mix finetune.<br />
âœ… å›¾ç‰‡ finetune ä¼šæŠŠ tenmporal æ¨¡å— fix ä½ã€‚</p>
</blockquote>
<blockquote>
<p>âœ… éœ€è¦è®­ç»ƒçš„æ¨¡å‹ï¼Œä¸”é’ˆå¯¹ä¸€ä¸ªæ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚<br />
âœ… åŸºæœ¬æ³›å¼ï¼šè¾“å…¥ï¼šä¸€æ®µè§†é¢‘ï¼Œä¸€ä¸ªæ–‡ç”Ÿå›¾æ¨¡å‹ï¼Œä¸€ä¸ªæ–‡æœ¬æç¤ºè¯ã€‚è¾“å‡ºï¼šåŸºäºå®šåˆ¶åŒ–çš„æ–‡ç”Ÿå›¾å¾—åˆ°æ–‡ç”Ÿè§†é¢‘ã€‚<br />
âœ… ä¸åœ¨å¤§è§„æ¨¡ä¸Šè®­ç»ƒï¼Œåªåœ¨ä¸€ä¸ªè§†é¢‘ä¸Šè®­ç»ƒï¼Œåªéœ€ååˆ†é’Ÿã€‚</p>
</blockquote>
<blockquote>
<p>âœ… æ¨æ–­è¿‡ç¨‹ï¼š(1) æŠŠè§†é¢‘ dounsampleï¼Œç»´åº¦å˜å°ã€‚<br />
âœ… (2) åŠ å…¥å™ªå£°ä½œä¸ºåˆå§‹å™ªå£°ï¼Œç±»ä¼¼äº DDIM Inversion.<br />
âœ… (3) ç”¨ diffusion model ç”Ÿæˆã€‚<br />
âœ… (4) ä¸Šé‡‡æ ·ã€‚<br />
âœ… å¦‚æœæœ‰æ›´å¤š reference vedio æ˜¯ä¸æ˜¯èƒ½å­¦å¾—æ›´å¥½ã€‚<br />
âœ… (1) ç”¨å‡ æ®µè§†é¢‘å­¦ä¹  conceptï¼<br />
âœ… (2) æŠŠ concept æ¥å…¥åˆ° diffusion model ä¸­ã€‚<br />
âœ… é€šè¿‡å¤šæ®µè§†é¢‘å­¦ä¹  motion concept.</p>
</blockquote>
<blockquote>
<p>âœ… ä¸ä»…å­¦ motionï¼Œè¿˜å¯ä»¥å­¦ camera motionï¼Œcamera motionï¼Œç‰©ä½“è½¨è¿¹ã€‚ </p>
</blockquote>
<blockquote>
<p>âœ… æ€ä¹ˆæŠŠä¸€ä¸ª concept åº”ç”¨åˆ°ä¸åŒçš„ç‰©ä½“ä¸Šã€‚<br />
âœ… æ€æ ·åªå­¦ motion è€Œä¸è¢«ç‰©ä½“çš„ appearance å½±å“ï¼Œèƒ½ä¸èƒ½ decouple.<br />
âœ… åˆ†æ”¯1ï¼šspatial pathï¼Œç°è‰²ä¸º spatial LoRAï¼Œå­¦ä¹ å¤–è¡¨ä¿¡æ¯ã€‚<br />
âœ… åˆ†æ”¯2ï¼štemporal pathï¼Œè“è‰²ä¸º temporal LoRAï¼Œè¿™ä¸ª path ç”¨äºå­¦ä¹  motion.<br />
âœ… debiasï¼šå»æ‰ appreance å¯¹ loss çš„å½±å“ã€‚<br />
âœ… temporal LORA å­¦ä¹ æ—¶ä½¿ç”¨ä½†ä¸ä¿®æ”¹ spatial LORA çš„ Weight.<br />
âœ… åº”ç”¨ï¼š(1) ä¹Ÿå¯ä»¥ç”¨äº one shot<br />
âœ… (2) å¯ä»¥ç”¨äº appreace å’Œ motion çš„ç»„åˆ<br />
âœ… (3) å¯ä»¥ç”¨äº Image Animation </p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P177</p>
<h1 id="3-video-editing-1"><a class="header" href="#3-video-editing-1">3 Video Editing</a></h1>
<h2 id="32-training-free"><a class="header" href="#32-training-free">3.2 Training-free</a></h2>
<p>P178<br />
<img src="VideoDiffusionModels/VideoEditing/../../assets/08-178.png" alt="" /> </p>
<blockquote>
<p>âœ… è§†é¢‘ç¼–è¾‘é¢†åŸŸæ¯”è¾ƒéš¾çš„é—®é¢˜ï¼šæ€ä¹ˆä¿æŒæ—¶åºä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p>P179</p>
<h2 id="tokenflow"><a class="header" href="#tokenflow">TokenFlow</a></h2>
<p>Consistent high-quality semantic edits</p>
<p>Main challenge using T2I to edit videos without finetuning: temporal consistency</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-179.png" alt="" /> </p>
<p>Geyer et al., â€œTokenFlow: Consistent Diffusion Features for Consistent Video Edigng,â€ arXiv 2023.</p>
<p>P180</p>
<h2 id="tokenflow-1"><a class="header" href="#tokenflow-1">TokenFlow</a></h2>
<p>Consistent high-quality semantic edits</p>
<p><strong>Key Idea</strong></p>
<ul>
<li>Achieve consistency by enforcing the inter-frame correspondences in the original video</li>
</ul>
<p>Geyer et al., â€œTokenFlow: Consistent Diffusion Features for Consistent Video Edigng,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… åœ¨ UNet ä¸­æŠ½å‡º feature map ä¹‹åï¼Œæ‰¾ correspondence å¹¶è®°å½•ä¸‹æ¥ã€‚åœ¨ denoise è¿‡ç¨‹ä¸­æŠŠè¿™ä¸ª correspondence åº”ç”¨èµ·æ¥ã€‚<br />
â“ ä»€ä¹ˆæ˜¯ inter-frame correspondence? ä¾‹å¦‚æ¯ä¸€å¸§çš„ç‹—çš„çœ¼ç›çš„è¿åŠ¨ã€‚è¦è®©ç”Ÿæˆè§†é¢‘çš„ç‹—çš„çœ¼æ™´å…·æœ‰ç›¸åŒçš„è¿åŠ¨ã€‚</p>
</blockquote>
<p>P181</p>
<h2 id="tokenflow-2"><a class="header" href="#tokenflow-2">TokenFlow</a></h2>
<p>Consistent high-quality semanec edits</p>
<p><strong>Main idea</strong></p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-181.png" alt="" /> </p>
<p>Geyer et al., â€œTokenFlow: Consistent Diffusion Features for Consistent Video Editing,â€ arXiv 2023.</p>
<p>P182</p>
<h2 id="tokenflow-3"><a class="header" href="#tokenflow-3">TokenFlow</a></h2>
<p>Consistent high-quality semantic edits</p>
<p><strong>Main idea</strong></p>
<p>During conditional denoising, use features from corresponding positions in preceding and following frames instead of the pixel's own feature at output of extended-attention</p>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-182.png" alt="" /></td><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-182-1.png" alt="" /></td></tr>
</tbody></table>
<p>Geyer et al., â€œTokenFlow: Consistent Diffusion Features for Consistent Video Editing,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… é€å¸§ç¼–è¾‘æŠ–åŠ¨ä¸¥é‡ï¼Œè€Œ Token Flow æ›´ç¨³å®šã€‚</p>
</blockquote>
<p>P183</p>
<h2 id="tokenflow-4"><a class="header" href="#tokenflow-4">TokenFlow</a></h2>
<p>Consistent high-quality semantic edits</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-183.png" alt="" /> </p>
<p>Geyer et al., â€œTokenFlow: Consistent Diffusion Features for Consistent Video Editing,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… åœ¨ DDIM inversion è¿‡ç¨‹ä¸­ï¼ŒæŠŠ attention maps ä¿å­˜ä¸‹æ¥äº†ï¼Œåœ¨ denoise æ—¶ï¼ŒæŠŠè¿™ä¸ª map ç»“åˆè¿›å»ã€‚<br />
âœ… åœ¨ attention map ä¸Šçš„æ¼”è¿›ã€‚</p>
</blockquote>
<p>P184</p>
<h2 id="fatezero"><a class="header" href="#fatezero">FateZero</a></h2>
<p>Attention map fusing for better temporal consistency</p>
<p><strong>Methodology</strong> </p>
<ul>
<li>During DDIM inversion, save inverted self-/cross-attention maps</li>
<li>During editing, use some algorithms to blend inverted maps and generated maps</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-184.png" alt="" /> </p>
<p>Qi et al., â€œFateZero: Fusing Attentions for Zero-shot Text-based Video Editing,â€ ICCV 2023.</p>
<blockquote>
<p>âœ… å¯¹äºè¾“å…¥æ–‡æœ¬çš„æ¯ä¸ª wordtoken, éƒ½å¯ä»¥é€šè¿‡ attentior map æ‰¾åˆ°å›¾åƒä¸­çš„å¤§æ¦‚ä½ç½®ï¼ŒæŠŠè¦å»é™¤çš„ token mask æ‰ï¼Œå‰©ä¸‹éƒ¨åˆ†ä¿ç•™ã€‚ç”Ÿæˆå›¾åƒåˆ™æŠŠé token éƒ¨åˆ† mask æ‰ï¼Œä»¥æ­¤è¿›è¡Œä¸¤éƒ¨åˆ†çš„èåˆã€‚</p>
</blockquote>
<p>P185</p>
<h2 id="fatezero-1"><a class="header" href="#fatezero-1">FateZero</a></h2>
<p>Attention map fusing for better temporal consistency</p>
<p><strong>Methodology</strong></p>
<ul>
<li>During DDIM inversion, save inverted self-/cross-avenkon maps</li>
<li>During edikng, use some algorithms to blend inverted maps and generated maps</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-185.png" alt="" /> </p>
<p>Qi et al., â€œFateZero: Fusing Attentions for Zero-shot Text-based Video Editing,â€ ICCV 2023.</p>
<p>P186</p>
<h2 id="fatezero-2"><a class="header" href="#fatezero-2">FateZero</a></h2>
<p>Attention map fusing for better temporal consistency</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-186.png" alt="" /> </p>
<p>Qi et al., â€œFateZero: Fusing Akengons for Zero-shot Text-based Video Edigng,â€ ICCV 2023.</p>
<p>P187</p>
<h2 id="training-free-video-editing-more-works"><a class="header" href="#training-free-video-editing-more-works">Training-Free Video Editing: More Works</a></h2>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-187-1.png" alt="" /></td><td><strong>MeDM</strong> (Chu et al.) <br> OpScal flow-based guidance for temporal consistency <br> â€œMeDM: Mediagng Image Diffusion Models for Video-to Video Translagon with Temporal Correspondence Guidance,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-187-2.png" alt="" /></td><td><strong>Ground-A-Video</strong> (Jeong et al.) <br> Improve temporal consistency via modified attention and optical flow <br> â€œGround-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-187-3.png" alt="" /></td><td><strong>Gen-L-Video</strong> (Lorem et al.) <br> Edit very long videos using existing generators <br> â€œGen-L-Video: Multi-Text to Long Video Generation via Temporal Co-Denoising,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-187-4.png" alt="" /></td><td><strong>FLATTEN</strong> (Cong et al.) <br> Optical flow-guided attention for temporal consistency <br> â€œFlatten: optical flow-guided attention for consistent text-to-video editing,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-187-5.png" alt="" /></td><td><strong>InFusion</strong> (Khandelwal et al.) <br> Improve temporal consistency via fusing latents <br> â€œInFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-based Video Editing,â€ ICCVW 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-187-6.png" alt="" /></td><td><strong>Vid2Vid-Zero</strong> (Wang et al.) <br> Improve temporal consistency via crossï¿¾attention guidance and null-text inversion <br> â€œZero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models,â€ arXiv 2023.</td></tr>
</tbody></table>
<blockquote>
<p>âœ… åŸºäºä¸åŒä¿¡å·çš„å„ç§ç‰ˆçš„ control net.</p>
</blockquote>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P188</p>
<h1 id="3-video-editing-2"><a class="header" href="#3-video-editing-2">3 Video Editing</a></h1>
<h2 id="33-controlled-edifng-depthposepointcontrolnet"><a class="header" href="#33-controlled-edifng-depthposepointcontrolnet">3.3 Controlled Edifng (depth/pose/point/ControlNet)</a></h2>
<p>P189</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-189.png" alt="" /> </p>
<p>P190</p>
<h2 id="depth-control"><a class="header" href="#depth-control">Depth Control</a></h2>
<blockquote>
<p>âœ… RunwayML ä¸»è¦åšçš„æ˜¯ style transfer, å¼ºåˆ¶åŠ å…¥ depth ä½œä¸º condition, å› æ­¤å¯ç§»æ¤æ€§éå¸¸é«˜ã€‚</p>
</blockquote>
<p>P191</p>
<blockquote>
<p>âœ… MIDS æ˜¯å·²æœ‰çš„æ·±åº¦ä¼°è®¡æ¨¡å‹ã€‚</p>
</blockquote>
<p>P192</p>
<h2 id="use-midas-to-offer-depth-condition"><a class="header" href="#use-midas-to-offer-depth-condition">Use MiDaS to offer depth condition</a></h2>
<p>Depth estimating network</p>
<p>Ranftl et al., â€œTowards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer,â€ TPAMI 2022.</p>
<blockquote>
<p>âœ… æ·±å˜ä¿¡æ¯ Encode æˆ latent code, ä¸ noise conca åˆ°ä¸€èµ·ã€‚</p>
</blockquote>
<p>P193</p>
<h2 id="gen-1"><a class="header" href="#gen-1">Gen-1</a></h2>
<p>Framewise depth-guided video editing</p>
<ul>
<li>Inflate Stable Diffusion to a 3D model, finetune on pretrained weights</li>
<li>Insert temporal convolution/attention layers</li>
<li>Finetune to take <strong>per-frame depth as conditions</strong></li>
</ul>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-193-1.png" alt="" /></td><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-193-2.png" alt="" /></td></tr>
</tbody></table>
<blockquote>
<p>âœ… ç‰¹ç‚¹ï¼š(1) ä¸éœ€è¦è®­ç»ƒã€‚ (2) èƒ½ä¿æŒå‰åä¸€è‡´æ€§ã€‚</p>
</blockquote>
<p>P60</p>
<h3 id="gen-1-1"><a class="header" href="#gen-1-1">Gen-1</a></h3>
<ul>
<li>Transfer the style of a video using text prompts given a â€œdriving videoâ€</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/D3-60.png" alt="" /></p>
<p>Esser et al., <u>&quot;Structure and Content-Guided Video Synthesis with Diffusion Models&quot;,</u> arXiv 2023</p>
<p>P61</p>
<h3 id="gen-1-2"><a class="header" href="#gen-1-2">Gen-1</a></h3>
<ul>
<li>Condition on structure (depth) and content (CLIP) information.</li>
<li>Depth maps are passed with latents as input conditions.</li>
<li>CLIP image embeddings are provided via cross-attention blocks.</li>
<li>During inference, CLIP text embeddings are converted to CLIP image embeddings.</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/D3-61.png" alt="" /></p>
<blockquote>
<p>âœ… ç”¨ depth estimator ä»æºè§†é¢‘æå– struct ä¿¡æ¯ï¼Œç”¨ CLIP ä»æ–‡æœ¬ä¸­æå– content ä¿¡æ¯ã€‚<br />
âœ… depth å’Œ content åˆ†åˆ«ç”¨ä¸¤ç§å½¢å¼æ³¨å…¥ã€‚depth ä½œä¸ºæ¡ä»¶ï¼Œä¸ lantent concat åˆ°ä¸€èµ·ã€‚content ä»¥ cross attention çš„å½¢å¼æ³¨å…¥ã€‚</p>
</blockquote>
<p>P194</p>
<h2 id="pix2video"><a class="header" href="#pix2video">Pix2Video</a></h2>
<p>Framewise depth-guided video editing</p>
<ul>
<li>Given a sequence of frames, generate a new set of images that reflects an edit.</li>
<li>Editing methods on individual images fail to preserve temporal information.</li>
</ul>
<p>Ceylan et al., <u>&quot;Pix2Video: Video Editing using Image Diffusion&quot;,</u> arXiv 2023</p>
<blockquote>
<p>âœ… æ²¡æœ‰ 3D diffusion modelï¼Œåªæ˜¯ç”¨ 2D diffusion model ç”Ÿæˆå¤šå¼ å›¾åƒå¹¶æ‹¼æˆåºåˆ—ã€‚å…³é”®åœ¨äºä¿æŒæ—¶åºçš„è¿ç»­æ€§ã€‚</p>
</blockquote>
<ul>
<li>Leverage a pretrained per-frame depth-conditioned Stable Diffusion model to edit frame by frame, to maintain motion consistency between source video and edited video</li>
<li>No need for training/finetuning</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-194.png" alt="" /> </p>
<p>P195</p>
<h3 id="how-to-ensure-temporal-consistency"><a class="header" href="#how-to-ensure-temporal-consistency">How to ensure temporal consistency?</a></h3>
<h4 id="obtain-initial-noise-from-ddim-inversion"><a class="header" href="#obtain-initial-noise-from-ddim-inversion">Obtain initial noise from DDIM inversion</a></h4>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-195.png" alt="" /></p>
<blockquote>
<p>âœ… (1) ç”¨æ¯ä¸€å¸§çš„åŸå§‹å›¾åƒçš„ inversion ä½œä¸º init noise.<br />
âœ… (2) ä¸‹ä¸€å¸§çš„ç”Ÿæˆä¼šå¼•ç”¨ä¸Šä¸€å¸§çš„ latent.<br />
âœ… (3) ç”Ÿæˆçš„ä¸­é—´ç»“æœä¸Šä¹Ÿä¼šæœ‰èåˆã€‚</p>
</blockquote>
<p>P196</p>
<h4 id="self-attention-injection"><a class="header" href="#self-attention-injection"><strong>Self-Attention injection:</strong></a></h4>
<p>Inject self-attention features from the previous frame in U-Net for generating the current frame</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/D3-63-1.png" alt="" /></p>
<ul>
<li>Use the latent of the previous frame as keys and values to guide latent update of the current frame</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/D3-63-2.png" alt="" /></p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/D3-64.png" alt="" /></p>
<blockquote>
<p>âœ… reconstruction guidanceï¼Œä½¿ç”Ÿæˆçš„ latent code ä¸ä¸Šä¸€å¸§æ¥è¿‘ã€‚</p>
</blockquote>
<blockquote>
<p>âœ… (1) ä½¿ç”¨ DDIM inversion æŠŠå›¾åƒè½¬ä¸º noiseï¼<br />
âœ… (2) ç›¸é‚»çš„ fram åº” inversion å‡ºç›¸ä¼¼çš„ noiseï¼<br />
âœ… ä½¿ç”¨ self-attention injection å¾—åˆ°ç›¸ä¼¼çš„ noise.</p>
</blockquote>
<p>P197</p>
<h3 id="result-1"><a class="header" href="#result-1">Result</a></h3>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-197.png" alt="" /> </p>
<p>P198</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-198.png" alt="" /> </p>
<p>P199</p>
<h2 id="controlnet--multiple-control"><a class="header" href="#controlnet--multiple-control">ControlNet / Multiple Control</a></h2>
<p>P200</p>
<h2 id="controlvideo-zhang-et-al-2023"><a class="header" href="#controlvideo-zhang-et-al-2023">ControlVideo (Zhang et al. 2023)</a></h2>
<p>ControlNet-like video editing</p>
<ul>
<li>Input structural conditions through <strong>ControlNet</strong></li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-200.png" alt="" /> </p>
<p>Zhang et al., â€œControlVideo: Training-free Controllable Text-to-Video Generation,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… ä½¿ç”¨é¢„è®­ç»ƒçš„ stable diffusion, æ— éœ€é¢å¤–è®­ç»ƒã€‚<br />
âœ… contrd net æ˜¯ä¸ stable diffusion é…å¯¹çš„ã€‚<br />
âœ… contrd net ä»¥æ·±åº¦å›¾æˆ–è¾¹ç¼˜å›¾ä¸ºæ¡ä»¶ï¼Œå¹¶åœ¨æ—¶é—´ç»´åº¦ä¸Š embed ä»¥æ­¤å¾—åˆ°çš„Zã€‚ä¸åŸå§‹è§†é¢‘æœ‰æ¯”è¾ƒå¥½çš„å¯¹åº”å…³ç³»ï¼Œä½†ä»å­˜åœ¨ temporal consistency é—®é¢˜ã€‚</p>
</blockquote>
<p>P201</p>
<h2 id="controlvideo-zhang-et-al-2023-1"><a class="header" href="#controlvideo-zhang-et-al-2023-1">ControlVideo (Zhang et al. 2023)</a></h2>
<p>ControlNet-like video editing</p>
<ul>
<li>Use pretrained weights for Stable Diffusion &amp; ControlNet, no training/finetuning</li>
<li>Inflate Stable Diffusion and ControlNet along the temporal dimension</li>
<li>Interleaved-frame smoothing during DDIM sampling for bever temporal consistency</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-201.png" alt="" /> </p>
<p>Zhang et al., â€œControlVideo: Training-free Controllable Text-to-Video Generation,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… è§£å†³ temporal consistency é—®é¢˜ï¼Œæ–¹æ³•ï¼š<br />
âœ… åœ¨æ¯ä¸ª timestepï¼Œè®©ä¸åŒå¸§æˆä¸ºå‰åä¸¤å¸§çš„èåˆã€‚<br />
â“ control net ä¸ diffusion medel æ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ</p>
</blockquote>
<p>P202</p>
<h2 id="controlvideo-zhang-et-al-2023-2"><a class="header" href="#controlvideo-zhang-et-al-2023-2">ControlVideo (Zhang et al. 2023)</a></h2>
<p>ControlNet-like video editing</p>
<ul>
<li>Use pretrained weights for Stable Diffusion &amp; ControlNet, no training/finetuning</li>
<li>Inflate Stable Diffusion and ControlNet along the temporal dimension</li>
<li>Interleaved-frame smoothing during denoising for better temporal consistency</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-202.png" alt="" /></p>
<p>Zhang et al., â€œControlVideo: Training-free Controllable Text-to-Video Generation,â€ arXiv 2023.</p>
<p>P203</p>
<h2 id="controlvideo-zhang-et-al-2023-3"><a class="header" href="#controlvideo-zhang-et-al-2023-3">ControlVideo (Zhang et al. 2023)</a></h2>
<p>ControlNet-like video editing</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-203.png" alt="" /></p>
<p>Zhang et al., â€œControlVideo: Training-free Controllable Text-to-Video Generation,â€ arXiv 2023.</p>
<p>P207</p>
<blockquote>
<p>âœ… é™¤äº† control net, è¿˜ä½¿ç”¨å…‰æµä¿¡æ¯ä½œä¸ºå¼•å¯¼ã€‚<br />
âœ… Gopï¼šGroup of Pictures.</p>
</blockquote>
<p>P208</p>
<h2 id="videocontrolnet"><a class="header" href="#videocontrolnet">VideoControlNet</a></h2>
<p>Optical flow-guided video editing; I, P, B frames in video compression</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-208.png" alt="" /></p>
<p>Hu et al., â€œVideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… å†…å®¹ä¸€è‡´æ€§ï¼Œé€‚ç”¨äº style transfer, ä½†éœ€è¦å¯¹ç‰©ä½“éƒ½è¾ƒå¤§ç¼–è¾‘åŠ›åº¦æ—¶ä¸é€‚ç”¨(ä¾‹å¦‚ç¼–è¾‘ç‰©ä½“å½¢çŠ¶)ã€‚</p>
</blockquote>
<p>P209</p>
<blockquote>
<p>âœ… ä¹Ÿæ˜¯control net å½¢å¼ï¼Œä½†ç”¨åˆ°æ›´å¤šæ§åˆ¶æ¡ä»¶ã€‚</p>
</blockquote>
<p>P210</p>
<h2 id="ccedit"><a class="header" href="#ccedit">CCEdit</a></h2>
<p>Mulemodal-guided video edieng</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-210.png" alt="" /></p>
<p>Feng et al., â€œCCEdit: Creative and Controllable Video Editing via Diffusion Models,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… ä½¿ç”¨äº†æ›´å¤šæ§åˆ¶ä¿¡æ¯ï¼Œå¹¶æŠŠå®ƒä»¬ combine åˆ°ä¸€èµ·ã€‚</p>
</blockquote>
<p>P211</p>
<h2 id="videocomposer"><a class="header" href="#videocomposer">VideoComposer</a></h2>
<p>Image-, sketch-, motion-, depth-, mask-controlled video editing</p>
<p><strong>Video Editing based on Various Conditions</strong></p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-211.png" alt="" /></p>
<p>Wang et al., â€œVideoComposer: Compositional Video Synthesis with Motion Controllability,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… æ¯ä¸ª condition è¿›æ¥ï¼Œéƒ½è¿‡ä¸€ä¸ª STC-Encoder, ç„¶åæŠŠä¸åŒ condition fuse åˆ°ä¸€èµ·ï¼Œè¾“å…¥åˆ° U-Net.</p>
</blockquote>
<p>P212</p>
<h2 id="videocomposer-1"><a class="header" href="#videocomposer-1">VideoComposer</a></h2>
<p>Image-, sketch-, motion-, depth-, mask-controlled video editing</p>
<p>â€¢ Spako-Temporal Condikon encoder (STC-encoder): a unified input interface for condikons</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-212.png" alt="" /></p>
<p>Wang et al., â€œVideoComposer: Compositional Video Synthesis with Motion Controllability,â€ arXiv 2023.</p>
<p>P214</p>
<h2 id="controlnet--and-depth-controlled-video-editing-more-works"><a class="header" href="#controlnet--and-depth-controlled-video-editing-more-works">ControlNet- and Depth-Controlled Video Editing: More Works</a></h2>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-214-1.png" alt="" /></td><td><strong>MagicProp</strong> (Yan et al.) <br> â€œMagicProp: Diffusion-based Video Editing via Motion-aware Appearance Propagation,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-214-2.png" alt="" /></td><td><strong>Make-Your-Video</strong> (Xing et al.) <br> â€œMake-Your-Video: Customized Video Generation Using Textual and Structural Guidance,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-214-3.png" alt="" /></td><td><strong>Control-A-Video</strong> (Lorem et al.) <br> â€œControl-A-Video: Controllable Text-to-Video Generagon with Diffusion Models,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-214-4.png" alt="" /></td><td><strong>MagicEdit</strong> (Liew et al.) <br> â€œMagicEdit: High-Fidelity and Temporally Coherent Video Editing,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-214-5.png" alt="" /></td><td><strong>EVE</strong> (Chen et al.) <br> â€œEVE: Efficient zero-shot text-based Video Editing with Depth Map Guidance and Temporal Consistency Constraints,â€ arXiv 2023.</td></tr>
</tbody></table>
<p>P215</p>
<h2 id="pose-control"><a class="header" href="#pose-control">Pose Control</a></h2>
<p>P216</p>
<h2 id="dreampose"><a class="header" href="#dreampose">DreamPose</a></h2>
<p>Pose- and image-guided video generation</p>
<p>Input: image  \(\quad \) Input: pose sequence   \(\quad \)  Output: Video</p>
<p>Karras et al., â€œDreamPose: Fashion Image-to-Video Synthesis via Stable Diffusion,â€ arXiv 2023.</p>
<p>P218</p>
<h2 id="magicanimate"><a class="header" href="#magicanimate">MagicAnimate</a></h2>
<p>Pose- and image-guided video generaeon</p>
<p><strong>Challenges</strong></p>
<ul>
<li>Flickering video</li>
<li>Cannot maintain background</li>
<li>Short video animation results</li>
</ul>
<p><strong>Possible Cause</strong></p>
<ul>
<li>Weak appearance preservation due to lack of temporal modeling</li>
</ul>
<p>Xu et al., â€œMagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… æŠŠ pose control net åŠ åˆ°æ ¸å¿ƒçš„ U-Net ç”Ÿæˆã€‚<br />
âœ… æŠŠåŸå§‹ U-Net fix, copy- åˆ†å¯ä»¥ train çš„ U-Net.<br />
âœ… è¾“å…¥ï¼šreference image, ä¸¤ä¸ª U-Net åœ¨éƒ¨åˆ† layer è¿›è¡Œç»“åˆè¾¾åˆ°å‰æ™¯ appearance å’ŒèƒŒæ™¯ appeorance çš„ Encode æ¨æ–­æ—¶è¾“å…¥å¤šä¸ª Sequence, å¯ä»¥ç”Ÿæˆ long video.</p>
</blockquote>
<p>P219</p>
<h2 id="magicanimate-1"><a class="header" href="#magicanimate-1">MagicAnimate</a></h2>
<p>Pose- and image-guided video generation</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-219.png" alt="" /> </p>
<p>Xu et al., â€œMagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model,â€ arXiv 2023.</p>
<p>P220</p>
<h2 id="magicanimate-2"><a class="header" href="#magicanimate-2">MagicAnimate</a></h2>
<p>Pose- and image-guided video generation</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-220.png" alt="" /> </p>
<p>Xu et al., â€œMagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model,â€ arXiv 2023.</p>
<p>P223</p>
<h2 id="magicanimate-3"><a class="header" href="#magicanimate-3">MagicAnimate</a></h2>
<p>Pose-guided video generation</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-223.png" alt="" /> </p>
<p>Xu et al., â€œMagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model,â€ arXiv 2023.</p>
<p>P224</p>
<h2 id="video-editing-under-pose-guidance-more-works"><a class="header" href="#video-editing-under-pose-guidance-more-works">Video Editing Under Pose Guidance: More Works</a></h2>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-224-1.png" alt="" /></td><td><strong>Dancing Avatar</strong> (Qin et al.)<br> Pose-guided video editing <br> â€œDancing avatar: Pose and text-guided human motion videos synthesis with image diffusion model,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-224-2.png" alt="" /></td><td><strong>Follow Your Pose</strong> (Ma et al.) <br> Pose-guided video editing  <br> â€œFollow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-224-3.png" alt="" /></td><td><strong>DisCo</strong> (Wang et al.) <br> Pose-guided video editing <br> â€œDisco: Disentangled control for referring human dance generation in real world,â€ arXiv 2023.</td></tr>
</tbody></table>
<p>P225</p>
<h2 id="point-control"><a class="header" href="#point-control">Point-Control</a></h2>
<p>P226</p>
<h2 id="videoswap"><a class="header" href="#videoswap">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><strong>Problem Formulation</strong></p>
<ul>
<li>Subject replacement: change video subject to a <strong>customized</strong> subject</li>
<li>Background preservation: preserve the unedited background same as the source video</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-226.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<blockquote>
<p>âœ… è¦æ±‚ï¼ŒèƒŒæ™¯ä¸€è‡´ï¼ŒåŠ¨ä½œä¸€è‡´ï¼Œä»…æ›¿æ¢å‰æ™¯ content.<br />
âœ… å› æ¯”å¯¹åŸè§†é¢‘æå–å…³é”®ç‚¹ï¼ŒåŸºäºå…³é”®ç‚¹è¿›è¡Œæ§åˆ¶ã€‚</p>
</blockquote>
<p>P227</p>
<h2 id="videoswap-1"><a class="header" href="#videoswap-1">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><strong>Motivation</strong></p>
<ul>
<li>Existing methods are promising but still often motion not well aligned</li>
<li>Need ensure precise correspondence of <u> <strong>semantic points</strong> </u> between the source and target</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-227.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<blockquote>
<p>âœ… (1) äººå·¥æ ‡æ³¨æ¯ä¸€å¸§çš„ semantic pointï¼ï¼ˆå°‘é‡æ ‡æ³¨ï¼Œ8å¸§ï¼‰<br />
âœ… (2) æŠŠ point map ä½œä¸º conditionï¼</p>
</blockquote>
<p>P228</p>
<h2 id="videoswap-2"><a class="header" href="#videoswap-2">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><strong>Empirical Observations</strong></p>
<ul>
<li><strong>Question</strong>: Can we <u> learn semantic point control </u> for a specific <u>source video subject</u> using only a <u>small number of source video frames</u></li>
<li><strong>Toy Experiment</strong>: Manually define and annotate a set of semantic points on 8 frame; use such point maps as condition for training a control net, i.e., T2I-Adapter.</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-228.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<blockquote>
<p>âœ… å®éªŒè¯æ˜ï¼Œå¯ä»¥ç”¨ semantic point ä½œä¸º controlï¼<br />
âœ… ç»“è®ºï¼šT2I æ¨¡å‹å¯ä»¥æ ¹æ®æ–°çš„ç‚¹çš„ä½ç½®è¿›è¡Œæ–°çš„å†…å®¹ç”Ÿæˆã€‚</p>
</blockquote>
<p>P229</p>
<h2 id="videoswap-3"><a class="header" href="#videoswap-3">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><strong>Empirical Observations</strong></p>
<ul>
<li><strong>Observation 1</strong>: If we can drag the points, the trained T2I-Aapter can generate new contents based on such dragged new points (new condition)  â†’  feasible to use semantic points as condition to control and maintain the source motion trajectory.</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-229.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<blockquote>
<p>âœ… ä¹Ÿå¯ä»¥é€šè¿‡æ‹‰éƒ¨åˆ†ç‚¹æ”¹å˜è½¦çš„å½¢çŠ¶ã€‚</p>
</blockquote>
<p>P230</p>
<h2 id="videoswap-4"><a class="header" href="#videoswap-4">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><strong>Empirical Observations</strong></p>
<ul>
<li><strong>Observation 2</strong>: Further, we can drag the semantic points to control the subjectâ€™s shape</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-230.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<blockquote>
<p>âœ… è™šçº¿æ¡†ä¸ºç±»ä¼¼äº control net çš„æ¨¡å—ï¼Œèƒ½æŠŠ semanti point æŠ½å‡ºæ¥å¹¶è¾“å…¥åˆ° denoise æ¨¡å—ä¸­ã€‚<br />
âœ… Latent Blend èƒ½æ›´å¥½ä¿ç•™èƒŒæ™¯ä¿¡æ¯ã€‚<br />
âœ… è“è‰²éƒ¨åˆ†ä¸º Motion layer.</p>
</blockquote>
<p>P231</p>
<h2 id="videoswap-5"><a class="header" href="#videoswap-5">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-231.png" alt="" /> </p>
<p><strong>Framework</strong></p>
<ul>
<li>
<p><strong>Motion layer</strong>: use pretrained and fixed AnimateDiff to ensure essential temporal consistency</p>
</li>
<li>
<p><strong>ED-LoRA</strong> \(_{(Mix-of-Show)}\): learn the wconcept to be customized</p>
</li>
<li>
<p><strong>Key design aims</strong>: </p>
<ul>
<li>Introduce semantic point correspondences to guide motion trajectory</li>
<li>Reduce human efforts of annotating points</li>
</ul>
</li>
</ul>
<p>Gu et al. â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.<br />
Gu et al. â€œMix-of-Show: Decentralized Low-Rank Adaptation for Multi-Concept Customization of Diffusion Models.â€ NeurIPS, 2023.</p>
<p>P232</p>
<h2 id="videoswap-6"><a class="header" href="#videoswap-6">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><strong>Step 1: Semantic Point Extraction</strong></p>
<ul>
<li>Reduce human efforts in annotating points
<ul>
<li>User define point at one keyframe</li>
<li>Propagate to other frames by point tracking/detector</li>
</ul>
</li>
<li>Embedding</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-232.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<blockquote>
<p>âœ… ä»€ä¹ˆæ˜¯æ¯”è¾ƒå¥½çš„ Semantic point çš„è¡¨è¾¾ï¼Ÿ</p>
</blockquote>
<p>P233</p>
<h2 id="videoswap-7"><a class="header" href="#videoswap-7">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><strong>Methodology â€“ Step 1: Semantic Point Extraction on the source video</strong></p>
<ul>
<li>Reduce human efforts in annotating points</li>
<li>Embedding
<ul>
<li>Extract DIFT embedding (intermediate U-Net feature) for each semantic point</li>
<li>Aggregate over all frames</li>
</ul>
</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-233.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<blockquote>
<p>â“ Embedding, æ€ä¹ˆè¾“äººåˆ°ç½‘ç»œä¸­ï¼Ÿ<br />
âœ… ç½‘ç»œå‚æ•°æœ¬èº«æ˜¯ fix çš„ï¼Œå¢åŠ ä¸€äº›å°çš„ MLP, æŠŠ Embeddin è½¬åŒ–ä¸ºä¸åŒçš„ scales çš„ condition map, ä½œä¸º U-Net çš„ condition.</p>
</blockquote>
<p>P234</p>
<h2 id="videoswap-8"><a class="header" href="#videoswap-8">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><strong>Methodology â€“ Step 2: Semantic Point Registration on the source video</strong></p>
<ul>
<li>Introduce several learnable MLPs, corresponding to different scales</li>
<li>Optimize the MLPs
<ul>
<li>Point Patch Loss: restrict diffusion loss to reconstruct local patch around the point</li>
<li>Semantic-Enhanced Schedule: only sample higher timestep (0.5T, T), which prevents overfitting to low-level details</li>
</ul>
</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-234.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<blockquote>
<p>âœ… æœ‰äº›åœºæ™¯ä¸‹éœ€è¦å»é™¤éƒ¨åˆ† semanfic point, æˆ–ç§»åŠ¨ point çš„ä½ç½®ã€‚</p>
</blockquote>
<p>P235</p>
<h2 id="videoswap-9"><a class="header" href="#videoswap-9">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><strong>Methodology</strong></p>
<ul>
<li>After Step1 (Semantic Point Extraction) and Step2 (Semantic Point Registration), those semantic points can be used to guide motion</li>
<li>User-point interaction for various applications</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-235.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<blockquote>
<p>âœ… åœ¨ä¸€å¸§ä¸Šåšçš„ semantic point çš„ç§»åŠ¨ï¼Œè¿ç§»åˆ°å…¶å®ƒå¸§ä¸Šã€‚</p>
</blockquote>
<p>P236</p>
<h2 id="videoswap-10"><a class="header" href="#videoswap-10">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><strong>Methodology</strong></p>
<ul>
<li>How to drag point for shape change?
<ul>
<li>Dragging at one frame is straightforward, propagating drag displacement over time is non-trivial, because of complex camera motion and subject motion in video.</li>
<li>Resort to canonical space (i.e., Layered Neural Atlas) to propagate displacement.</li>
</ul>
</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-236.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<p>P237</p>
<h2 id="videoswap-11"><a class="header" href="#videoswap-11">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><strong>Methodology</strong></p>
<ul>
<li>How to drag point for shape change?</li>
<li>Dragging at one frame is straightforward, propagating drag displacement over time is non-trivial because of complex camera motion and subject motion in video.</li>
<li>Resort to canonical space (i.e., Layered Neural Atlas) to propagate displacement.</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-237.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<p>P238</p>
<h2 id="videoswap-12"><a class="header" href="#videoswap-12">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-238-1.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<p>P239</p>
<h2 id="videoswap-13"><a class="header" href="#videoswap-13">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-239.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<blockquote>
<p>âœ… point contrd å¯ä»¥å¤„ç†å½¢å˜æ¯”è¾ƒå¤§çš„åœºæ™¯ã€‚</p>
</blockquote>
<p>P240</p>
<h2 id="videoswap-14"><a class="header" href="#videoswap-14">VideoSwap</a></h2>
<p>Customized video subject swapping via point control</p>
<p><strong>Qualitative Comparisons to previous works</strong></p>
<ul>
<li>VideoSwap can <strong>support shape change</strong> in the target swap results, leading to the correct identity of target concept. </li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-240.png" alt="" /> </p>
<p>Gu et al., â€œVideoSwap: Customized Video Subject Swapping with Interactive Semantic Point Correspondence,â€ 2023.</p>
<p>P241</p>
<blockquote>
<p>âœ… é‡å»º 3D å¯ä»¥è§£å†³æ—¶é—´ä¸€è‡´æ€§é—®é¢˜ã€‚</p>
</blockquote>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P242</p>
<h1 id="3-video-editing-3"><a class="header" href="#3-video-editing-3">3 Video Editing</a></h1>
<h2 id="34-3d-aware"><a class="header" href="#34-3d-aware">3.4 3D-Aware</a></h2>
<p>P243</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-243.png" alt="" /> </p>
<p>P244</p>
<h2 id="layered-neural-atlases"><a class="header" href="#layered-neural-atlases">Layered Neural Atlases</a></h2>
<p>Decompose a video into two images</p>
<ul>
<li>Decompose a video into a foreground image + a background image</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-244.png" alt="" /> </p>
<p>Kasten et al., â€œLayered Neural Atlases for Consistent Video Editing,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… å¯¹èƒŒæ™¯è¿›è¡Œç¼–è¾‘ï¼ˆå›¾ç‰‡ç¼–è¾‘ã€é£æ ¼è¿ç§»ï¼‰å†ä¼ æ’­åˆ°ä¸åŒå¸§ä¸Šå»ã€‚</p>
</blockquote>
<p>P245</p>
<h2 id="layered-neural-atlases-1"><a class="header" href="#layered-neural-atlases-1">Layered Neural Atlases</a></h2>
<p>Decompose a video into two images</p>
<ul>
<li>Decompose a video into a foreground image + a background image</li>
<li>Edit the foreground/background image = edit the video</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-245.png" alt="" /> </p>
<p>Kasten et al., â€œLayered Neural Atlases for Consistent Video Editing,â€ arXiv 2023.</p>
<p>P246</p>
<h2 id="videdit"><a class="header" href="#videdit">VidEdit</a></h2>
<p>Atlas-based video editing</p>
<ul>
<li>Decompose a video into a foreground image + a background image</li>
<li>Edit the foreground/background image = edit the video</li>
<li>Use diffusion to edit foreground/background atlas</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-246.png" alt="" /> </p>
<p>Video from Kasten et al., â€œLayered Neural Atlases for Consistent Video Edigng,â€ arXiv 2023.<br />
Couairon et al., â€œVidEdit: Zero-Shot and Spagally Aware Text-Driven Video Edigng,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… å‰æ™¯ç¼–è¾‘ï¼š(1) æŠ å‡ºç¬¬ä¸€å¸§å‰æ™¯å¹¶è¿›è¡Œç¼–è¾‘å¾—åˆ° Partial Atlas.<br />
âœ… (2) Partial Atlas ä½œä¸ºä¸‹ä¸€å¸§çš„ condition æ•´ä½“ä¸Šæ˜¯è‡ªå›å½’çš„ã€‚<br />
âœ… æ‰€æœ‰ Partial åˆèµ·æ¥å¾—åˆ°ä¸€ä¸ªæ•´ä½“ã€‚<br />
âœ… èƒŒæ™¯ä½¿ç”¨æ·±åº¦ä¿¡æ¯ä½œä¸º cordition.</p>
</blockquote>
<p>P247</p>
<h2 id="stablevideo--shape-aware-text-drive-layered-video-editing"><a class="header" href="#stablevideo--shape-aware-text-drive-layered-video-editing">StableVideo &amp; Shape-aware Text-drive Layered Video Editing</a></h2>
<p>Atlas-based video edieng</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-247.png" alt="" /> </p>
<p>Lee et al., â€œShape-aware Text-driven Layered Video Editing,â€ CVPR 2023.<br />
Chai et al., â€œStableVideo: Text-driven Consistency-aware Diffusion Video Editing,â€ ICCV 2023.</p>
<p>P248</p>
<h2 id="stablevideo"><a class="header" href="#stablevideo">StableVideo</a></h2>
<p>Atlas-based video editing</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-248.png" alt="" /> </p>
<p>Chai et al., â€œStableVideo: Text-driven Consistency-aware Diffusion Video Edigng,â€ ICCV 2023.</p>
<blockquote>
<p>âœ… ç»™ä¸€ä¸ªåœºæ™¯çš„å¤šè§†è§’å›¾ç‰‡ï¼ŒåŸºäº MLP å­¦ä¹  3D åœºæ™¯çš„éšå¼è¡¨è¾¾ã€‚</p>
</blockquote>
<p>P249</p>
<h2 id="content-deformation-field-codef"><a class="header" href="#content-deformation-field-codef">Content Deformation Field (CoDeF)</a></h2>
<p>Edit a video = edit a canonical image + learned deformaeon field</p>
<ul>
<li>
<p>Limitations of Neural Layered Atlases</p>
<ul>
<li>Limited capacity for faithfully reconstructing intricate video details, missing subtle motion features like blinking eyes and slight smiles</li>
<li>Distorted nature of the estimated atlas leads to impaired semantic information</li>
</ul>
</li>
<li>
<p>Content Deformation Field: inspired by dynamic NeRF works, a new way of representing video, as a 2d canonical image + 3D deformation field over time</p>
</li>
<li>
<p>Edit a video = edit a canonical image + learned deformation field</p>
</li>
</ul>
<p>Ouyang et al., â€œCoDeF: Content Deformation Fields for Temporally Consistent Video Processing,â€ arXiv 2023.</p>
<p>P250</p>
<h2 id="content-deformation-field-codef-1"><a class="header" href="#content-deformation-field-codef-1">Content Deformation Field (CoDeF)</a></h2>
<p>Edit a video = edit a canonical image + learned deformation field</p>
<p><strong>Problem Formulation</strong></p>
<ul>
<li>Decode a video into a 2D canonical field and a 3D temporal deformation field</li>
<li>Deformation Field: video (x, y, t) â†’ canonical image coordinate (xâ€™, yâ€™)</li>
<li>Canonical Field: (xâ€™, yâ€™) â†’ (r, g, b), like a â€œ2D imageâ€</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-250.png" alt="" /> </p>
<p>Ouyang et al., â€œCoDeF: Content Deformation Fields for Temporally Consistent Video Processing,â€ arXiv 2023.</p>
<p>P251</p>
<h2 id="content-deformation-field-codef-2"><a class="header" href="#content-deformation-field-codef-2">Content Deformation Field (CoDeF)</a></h2>
<p>Edit a video = edit a canonical image + learned deformation field</p>
<p><strong>CoDeF compared to Atlas</strong></p>
<ul>
<li>Superior robustness to non-rigid motion</li>
<li>Effective reconstruction of subtle movements (e.g. eyes blinking)</li>
<li>More accurate reconstruction: 4.4dB higher PSNR</li>
</ul>
<p>Ouyang et al., â€œCoDeF: Content Deformation Fields for emporally Consistent Video Processing,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… CoDef æŠŠ 3D è§†é¢‘å‹ç¼©ä¸º 2D Imageï¼Œå› æ­¤å¯ä»¥åˆ©ç”¨å¾ˆå¤š 2D ç®—æ³•ï¼Œå†æŠŠ deformation ä¼ é€’åˆ°æ•´ä¸ªè§†é¢‘ã€‚</p>
</blockquote>
<p>P252</p>
<h2 id="content-deformation-field-codef-3"><a class="header" href="#content-deformation-field-codef-3">Content Deformation Field (CoDeF)</a></h2>
<p>Edit a video = edit a canonical image + learned deformation field</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-252.png" alt="" /> </p>
<p>Ouyang et al., â€œCoDeF: Content Deformation Fields for emporally Consistent Video Processing,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… åœ¨æ—¶åºä¸Šæœ‰æ¯”è¾ƒå¥½çš„ä¸€è‡´æ€§ã€‚<br />
âœ… ç”±äºä½¿ç”¨äº† control netï¼Œä¸åŸè§†é¢‘åœ¨ Spatial level ä¹Ÿä¿æŒå¾—éå¸¸å¥½ã€‚</p>
</blockquote>
<p>P253</p>
<h2 id="content-deformafon-field-codef"><a class="header" href="#content-deformafon-field-codef">Content Deformafon Field (CoDeF)</a></h2>
<p>Edit a video = edit a canonical image + learned deformation field</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-253.png" alt="" /> </p>
<p>Ouyang et al., â€œCoDeF: Content Deformation Fields for emporally Consistent Video Processing,â€ arXiv 2023.</p>
<p>P254</p>
<h2 id="dynvideo-e"><a class="header" href="#dynvideo-e">DynVideo-E</a></h2>
<p>Edit a video = edit a canonical <del>image</del> 3D NeRF</p>
<p>Canonical image in CoDeF is still 2D</p>
<p>Can we represent the video in a truly 3D space?</p>
<p>Liu et al., â€œDynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing,â€ arXiv 2023.</p>
<p>P255</p>
<h2 id="dynvideo-e-1"><a class="header" href="#dynvideo-e-1">DynVideo-E</a></h2>
<p>Edit a video = edit a canonical <del>image</del> 3D NeRF</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-255.png" alt="" /> </p>
<p>Liu et al., â€œDynVideo-E: Harnessing Dynamic NeRF for arge-Scale Motion- and View-Change Human-Centric Video Editing,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… åˆ©ç”¨ç°æœ‰æˆç†ŸæŠ€æœ¯ï¼ŒæŠŠ 3D åœºæ™¯ç”¨ Nerf è¡¨ç¤ºå‡ºæ¥ç¼–è¾‘ä¹Ÿæ˜¯åœ¨ 3D ä¸Šè¿›è¡Œã€‚</p>
</blockquote>
<p>P256</p>
<blockquote>
<p>âœ… Nerf åœ¨äººä½“æˆåƒä¸Šæ¯”è¾ƒå¥½ã€‚<br />
âœ… Dynamic NeRF æœ¬èº«ä¹Ÿæ˜¯æ¯”è¾ƒéš¾çš„ã€‚</p>
</blockquote>
<p>P257</p>
<h2 id="dynvideo-e-2"><a class="header" href="#dynvideo-e-2">DynVideo-E</a></h2>
<p>Edit a video = edit a canonical <del>image</del> 3D NeRF</p>
<p><strong>Main idea</strong></p>
<ul>
<li>For the first time introduce the dynamic NeRF as an innovative video representation for large-scale motion- and view-change human-centric video editing.</li>
</ul>
<p>Liu et al., â€œDynVideo-E: Harnessing Dynamic NeRF for Large-Scale Motion- and View-Change Human-Centric Video Editing,â€ arXiv 2023.</p>
<blockquote>
<p>âœ… ä¸ç›´æ¥ç¼–è¾‘å›¾åƒï¼Œè€Œæ˜¯ç¼–è¾‘ Nerfï¼<br />
âœ…ï¼ˆ1ï¼‰è®¤ä¸ºèƒŒæ™¯é™æ­¢ï¼Œå­¦å‡ºèƒŒæ™¯ Neofï¼<br />
âœ… Stale Diffusion ç”¨æ¥è®¡ç®— Loss.</p>
</blockquote>
<p>P258</p>
<h2 id="dynvideo-e-3"><a class="header" href="#dynvideo-e-3">DynVideo-E</a></h2>
<p>Edit a video = edit a canonical <del>image</del> 3D NeRF</p>
<p>Follow HOSNeRF, represent the video as:</p>
<ul>
<li>Background NeRF</li>
<li>Human NeRF</li>
<li>Deformation Field</li>
</ul>
<p>Edit background NeRF and human NeRF respectively</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-258.png" alt="" /> </p>
<p>Liu et al., â€œHOSNeRF: Dynamic Human-Object-Scene Neural Radiance Fields from a Single Video,â€ ICCV 2023.<br />
Liu et al., â€œDynVideo-E: Harnessing Dynamic NeRF for Large-Scale Mogon- and View-Change Human-Centric Video Edigng,â€ arXiv 2023.</p>
<p>P259</p>
<h2 id="dynvideo-e-4"><a class="header" href="#dynvideo-e-4">DynVideo-E</a></h2>
<p>DynVideo-E significantly outperforms SOTA approaches on two challenging datasets by a large margin of 50% âˆ¼ 95% in terms of human preference</p>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P263</p>
<h1 id="3-video-editing-4"><a class="header" href="#3-video-editing-4">3 Video Editing</a></h1>
<h2 id="35-other-guidance"><a class="header" href="#35-other-guidance">3.5 Other Guidance</a></h2>
<p>P264</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-264.png" alt="" /></p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2023</td><td>InstructPix2Pix: Learning to Follow Image Editing Instructions</td><td>åœ¨ä¸Šä¸€ç¯‡çš„åŸºç¡€ä¸Šï¼Œé€šè¿‡attentionæ³¨å…¥çš„æ–¹å¼åŠ é€Ÿä¸Šè¿°æµç¨‹</td><td>attentionæ§åˆ¶</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/21.html">link</a></td></tr>
</tbody></table>
<p>P266</p>
<h2 id="instructvid2vid"><a class="header" href="#instructvid2vid">InstructVid2Vid</a></h2>
<p>Instruction-guided Video Editing</p>
<ul>
<li>Generate âŸ¨instruction, videoâŸ© dataset using ChatGPT, BLIP and Tune-A-Video</li>
<li>Train inflated Stable Diffusion for instruction-guided video editing</li>
</ul>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-266.png" alt="" /> </p>
<p>Qin et al., â€œInstructVid2Vid: Controllable Video Editing with Natural Language Instructions,â€ arXiv 2023.</p>
<blockquote>
<p>âœ…ï¼ˆ1ï¼‰æŠŠè¯´è¯çš„éƒ¨åˆ† mask æ‰ ï¼ˆ2ï¼‰ç”¨ diffusion æ ¹æ® Audio Feature ç”Ÿæˆè¯´è¯çš„éƒ¨åˆ†ã€‚<br />
âœ… é¢å¤–çº¦æŸï¼šï¼ˆ1ï¼‰reference çŠ¶æ€ ï¼ˆ2ï¼‰å‰åå¸§ smooth<br />
âœ… è¯­éŸ³é©±åŠ¨å˜´å½¢ã€‚</p>
</blockquote>
<p>P267</p>
<h2 id="speech-driven-video-editing-via-an-audio-conditioned-diffusion-model"><a class="header" href="#speech-driven-video-editing-via-an-audio-conditioned-diffusion-model">Speech Driven Video Editing via an Audio-Conditioned Diffusion Model</a></h2>
<p>Speech-driven video editing</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-267.png" alt="" /> </p>
<p>Bigioi et al., â€œSpeech Driven Video Editing via an Audio-Conditioned Diffusion Model,â€ arXiv 2023.</p>
<p>P268</p>
<h2 id="soundini"><a class="header" href="#soundini">Soundini</a></h2>
<p>Sound-guided video editing</p>
<p><img src="VideoDiffusionModels/VideoEditing/../../assets/08-268.png" alt="" /> </p>
<p>Lee et al., â€œSoundini: Sound-Guided Diffusion for Natural Video Editing,â€ arXiv 2023.</p>
<p>P269</p>
<h2 id="video-editing-under-various-guidance-more-works"><a class="header" href="#video-editing-under-various-guidance-more-works">Video Editing Under Various Guidance: More Works</a></h2>
<table><thead><tr><th></th><th></th></tr></thead><tbody>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-269-1.png" alt="" /></td><td><strong>Collaborative Score Distillation</strong> (Kim et al.) <br> Instruction-guide video editing <br> â€œCollaborative Score Distillation for Consistent Visual Synthesis,â€ NeurIPS 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-269-2.png" alt="" /></td><td><strong>Make-A-Protagonist</strong> (Zhao et al.) <br> Video ediSng with an ensemble of experts <br> â€œMake-A-Protagonist: Generic Video Edigng with An Ensemble of Experts,â€ arXiv 2023.</td></tr>
<tr><td><img src="VideoDiffusionModels/VideoEditing/../../assets/08-269-3.png" alt="" /></td><td><strong>DragNUWA</strong> (Yin et al.) <br> Multimodal-guided video editing <br> â€œDragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory,â€ arXiv 2023.</td></tr>
</tbody></table>
<p>P272</p>
<blockquote>
<p>âœ… showlab/Awesome-Video-Diffusion</p>
</blockquote>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="è¯„ä»·æŒ‡æ ‡"><a class="header" href="#è¯„ä»·æŒ‡æ ‡">è¯„ä»·æŒ‡æ ‡</a></h1>
<p><img src="VideoDiffusionModels/./../assets/08-46.png" alt="" /> </p>
<p>å›¾åƒè´¨é‡ã€è§†é¢‘è´¨é‡ã€ä¸€è‡´æ€§ã€å¤šæ ·æ€§ã€ç¾å­¦å’ŒåŠ¨ä½œå‡†ç¡®æ€§</p>
<p><img src="VideoDiffusionModels/./../assets/c5094236dee05a597cc12eb2a5b13473_2_Table_I_-1681426925.png" alt="" /></p>
<h2 id="image-level-evaluation-metrics"><a class="header" href="#image-level-evaluation-metrics">Image-level Evaluation Metrics</a></h2>
<ul>
<li>FrÃ©chet Inception Distance (FID, â†“): semantic similarity between images</li>
<li>Peak Signal-to-Noise Ratio (PSNR, â†‘): pixel-level similarity between images</li>
<li>Structural Similarity Index (SSIM, â†“): pixel-level similarity between images</li>
<li>CLIPSIM (â†‘): image-text relevance</li>
</ul>
<h3 id="frÃ©chet-inception-distance-fid"><a class="header" href="#frÃ©chet-inception-distance-fid">FrÃ©chet Inception Distance (FID)</a></h3>
<blockquote>
<p>âœ… FIDï¼šè¯„ä¼°ä¸¤ä¸ª distribution çš„å·®è·æœ‰å¤šå¤§ã€‚<br />
âœ… ç”±äºä½¿ç”¨äº†ç½‘ç»œçš„é«˜å±‚ featureï¼Œå¯ä»¥è¯„ä»· highï¼evel çš„è¯­ä¹‰ç›¸ä¼¼æ€§ã€‚</p>
</blockquote>
<p><img src="VideoDiffusionModels/../assets/lhy2-8.png" alt="" /> </p>
<blockquote>
<p>âœ… CNNï¼‹Softmax æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„å›¾åƒåˆ†ç±»ç½‘ç»œï¼Œå– softmax ä¸Šä¸€å±‚åšä¸ºå›¾åƒçš„ feature.<br />
âœ… å–å¤§é‡çœŸå®å›¾åƒçš„ feature å’Œé¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆçš„å›¾ feature.<br />
âœ… å‡è®¾ä¸¤ç±»å›¾åƒçš„ feature å„è‡ªç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œè®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒçš„è·ç¦»ã€‚<br />
âœ… ä¼˜ç‚¹ï¼šè¯„ä»·ç»“æœä¸äººç±»ç›´è§‰å¾ˆæ¥è¿‘ï¼Œç¼ºç‚¹ï¼šéœ€è¦å¤§é‡ sample.</p>
</blockquote>
<p>P49</p>
<h3 id="peak-signal-to-noise-ratio-psnr"><a class="header" href="#peak-signal-to-noise-ratio-psnr">Peak Signal-to-Noise Ratio (PSNR)</a></h3>
<p>Pixel-level similarity between images</p>
<ul>
<li>For two images \(x,y \text{ of shape }  M\times N\):</li>
</ul>
<p>\begin{align*} \mathrm{PSNR} (x,y) = 10 \log_{10}{} \frac{255^2}{\mathrm{MSE} (x,y)}  \end{align*}</p>
<p>where</p>
<p>\begin{align*} \mathrm{MSE} (x,y) = \frac{1}{MN} \sum_{i=1}^{M} \sum_{j=1}^{N} (x_{ij}-y_{ij})^2\end{align*}</p>
<p>P50</p>
<h3 id="structural-similarity-index-measure-ssim"><a class="header" href="#structural-similarity-index-measure-ssim">Structural Similarity Index Measure (SSIM)</a></h3>
<p>Pixel-level similarity between images</p>
<ul>
<li>
<p>Model any image distortion as a combination of:<br />
(1) loss of correlation, (2) luminance distortion, (3) contrast distortion</p>
</li>
<li>
<p>For two images \(x,y \text{ of shape }  M\times N\):</p>
</li>
</ul>
<p>\begin{align*}  \mathrm{SSIM} (x,y)=l(x,y)\cdot c(x,y)\cdot s(x,y)\end{align*}</p>
<p>where</p>
<p>\begin{align*} \begin{cases}
\text{Lumiannce Comparison Funckon:} l(x,y)=\frac{2\mu _x\mu _y+C_1}{\mu _x^2+\mu _y^2+C_1}  \\ 
\text{Contrast Comparison Funckon:} c(x,y)=\frac{2\sigma  _x\sigma  _y+C_2}{\sigma  _x^2+\sigma  _y^2+C_2}  \\ 
\text{Structure Comparison Funckon:} s(x,y)=\frac{\sigma  _{xy}+C_3}{\sigma  _{x}\sigma  _{y}+C_3}  \end{cases}\end{align*}</p>
<p>P51</p>
<h3 id="clip-similarity"><a class="header" href="#clip-similarity">CLIP Similarity</a></h3>
<blockquote>
<p>âœ… CLIP Scoreï¼Œè¡¡é‡ä¸æ–‡å­—çš„åŒ¹é…åº¦ã€‚ 
<img src="VideoDiffusionModels/./../assets/08-51.png" alt="" /> </p>
</blockquote>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2017</td><td>GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</td><td></td><td></td><td><a href="https://arxiv.org/abs/1706.08500">link</a></td></tr>
<tr><td></td><td>2023</td><td>Hung-Yi Lee, â€œMachine Learning 2023 Spring,â€ National Taiwan University.</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2010</td><td>HorÃ© et al., â€œImage Quality Metrics: PSNR vs. SSIM,â€</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2004</td><td>Wang et al., â€œImage Quality Assessment: from Error Visibility to Structural Similarity,â€</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2021</td><td>Radford et al., â€œLearning Transferable Visual Models From Natural Language Supervision,â€</td><td></td><td></td><td></td></tr>
</tbody></table>
<h2 id="video-level-evaluation-metrics"><a class="header" href="#video-level-evaluation-metrics"><strong>Video-level Evaluation Metrics</strong></a></h2>
<ul>
<li>FrÃ©chet Video Distance (FVD, â†“): semantic similarity &amp; temporal coherence</li>
<li>Kernel Video Distance (KVD, â†“): video quality (via semantic features and MMD)</li>
<li>Video Inception Score (IS, â†‘): video quality and diversity</li>
<li>Frame Consistency CLIP Score (â†‘): frame temporal semantic consistency</li>
</ul>
<p>P52</p>
<h3 id="frÃ©chet-video-distance-fvd"><a class="header" href="#frÃ©chet-video-distance-fvd">FrÃ©chet Video Distance (FVD)</a></h3>
<p>Semantic similarity and temporal coherence between two videos</p>
<p><img src="VideoDiffusionModels/./../assets/08-52.png" alt="" /> </p>
<p>P53</p>
<h3 id="kernel-video-distance"><a class="header" href="#kernel-video-distance">Kernel Video Distance</a></h3>
<p>Video quality assessment via semantic features and MMD</p>
<p><img src="VideoDiffusionModels/./../assets/08-53.png" alt="" /></p>
<p>P54</p>
<h3 id="video-inception-score-is"><a class="header" href="#video-inception-score-is">Video Inception Score (IS)</a></h3>
<p>Video quality and diversity</p>
<p><img src="VideoDiffusionModels/./../assets/08-54.png" alt="" /> </p>
<blockquote>
<p>âœ… å¤šæ ·æ€§ï¼Œåœ¨ä¸ç»™å®š condition çš„æƒ…å†µç”Ÿæˆçš„åˆ†å¸ƒçš„å¤šæ ·æ€§ã€‚<br />
âœ… è´¨é‡ï¼šåœ¨ç»™ condition çš„æ¡ä»¶ä¸‹åº”ç”Ÿæˆç‰¹å®šçš„ç±»åˆ«ã€‚</p>
</blockquote>
<p>P55</p>
<h3 id="frame-consistence-clip-scores"><a class="header" href="#frame-consistence-clip-scores">Frame Consistence CLIP scores</a></h3>
<p>Frame temporal semantic consistency</p>
<ul>
<li>Compute CLIP image embeddings for all frames</li>
<li>Report average cosine similarity between all pairs of frames</li>
</ul>
<p><img src="VideoDiffusionModels/./../assets/08-55.png" alt="" /> </p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2019</td><td>Unterthiner et al., â€œFVD: A new Metric for Video Generation,â€</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2018</td><td>Unterthiner et al., â€œTowards Accurate Generative Models of Video: A New Metric &amp; Challenges,â€</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2016</td><td>Salimans et al., â€œImproved Techniques for Training GANs,â€</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2018</td><td>Barratt et al., â€œA Note on the Inception Score,â€</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2020</td><td>Saito et al., â€œTrain Sparsely, Generated Densely: Memory-Efficient Unsupervised Training of High-Resolution Temporal GAN,â€</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2021</td><td>Radford et al., â€œLearning Transferable Visual Models From Natural Language Supervision,â€</td><td></td><td></td><td></td></tr>
</tbody></table>
<p>P57</p>
<h2 id="ä¸»è§‚è¯„ä»·"><a class="header" href="#ä¸»è§‚è¯„ä»·">ä¸»è§‚è¯„ä»·</a></h2>
<h3 id="hybrid-evaluationevalcrafter"><a class="header" href="#hybrid-evaluationevalcrafter">Hybrid evaluationï¼š<strong>EvalCrafter</strong></a></h3>
<ul>
<li>Creates a balanced prompt list for evaluation</li>
<li><strong>Multi-criteria decision analysis</strong> on 18 metrics: visual quality, content qualityâ€¦</li>
<li>Regress the coefficients of all metrics to generate an overall score aligned with user opinions</li>
</ul>
<p><img src="VideoDiffusionModels/./../assets/08-57.png" alt="" /> </p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2023</td><td>Liu et al., â€œEvalCrafter: Benchmarking and Evaluating Large Video Generation Models,â€</td><td></td><td></td><td></td></tr>
</tbody></table>
<p>P45</p>
<h1 id="datasets"><a class="header" href="#datasets">Datasets</a></h1>
<p>The WebVid-10M Dataset</p>
<p><img src="VideoDiffusionModels/./../assets/08-45.png" alt="" /> </p>
<p>Bain et al., â€œFrozen in Time: A Joint Video and Image Encoder for End to End Paper,â€ ICCV 2021.</p>
<blockquote>
<p>âœ… WebVid æ˜¯å¸¸ç”¨çš„è§†é¢‘æ•°æ®é›†ï¼Œæœ‰é«˜æ¸…è§†é¢‘åŠé…å¯¹æ–‡æœ¬ã€‚   </p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><p>P1</p>
<h2 id="large-multimodal-models"><a class="header" href="#large-multimodal-models">Large Multimodal Models:</a></h2>
<h2 id="notes-on-cvpr-2023-tutorial"><a class="header" href="#notes-on-cvpr-2023-tutorial">Notes on CVPR 2023 Tutorial</a></h2>
<p><strong>Chunyuan Li</strong><br />
Microsoft Research, Redmond<br />
<a href="https://chunyuan.li">https://chunyuan.li</a></p>
<p><strong>Abstract</strong></p>
<p>This tutorial note summarizes the presentation on <strong>Large Multimodal Models: To-wards Building and Surpassing Multimodal GPT-4, a part of CVPR 2023 tutorial on Recent Advances in Vision Foundation Models</strong>. The tutorial consists of three parts. We first <strong>introduce the background on recent GPT-like large models for vision-and-language modeling</strong> to motivate the research in instruction-tuned large multimodal models (LMMs). As a pre-requisite, <strong>we describe the basics of instruction-tuning in large language models</strong>, which is further extended to the multimodal space. Lastly, <strong>we illustrate how to build the minimum prototype of multimodal GPT-4 like models</strong>
with the open-source resource, and review the recently emerged topics.</p>
<blockquote>
<p>â“ GPT æ˜¯è¯­è¨€æ¨¡å‹ï¼Œä¸ºä»€ä¹ˆè¯´å®ƒæ˜¯å¤šæ¨¡æ€æ¨¡å‹ï¼Ÿ<br />
â“ ä»€ä¹ˆæ˜¯ instruction-tuningï¼Ÿ</p>
</blockquote>
<p>P3</p>
<h2 id="1-prologue"><a class="header" href="#1-prologue">1 Prologue</a></h2>
<p>In view of the rapid assimilation and widespread adoption of OpenAI ChatGPT [32]/GPT-4 [33] in contemporary society, there has been a growing interest among academics and researchers to develop open-source large language models (LLMs), and simultaneously explore the extensions into large multimodal models (LMMs)\(^1\). In order to elucidate this popular topic for a broader audience, in the CVPR 2023 tutorial on <strong>Recent Advances in Vision Foundation Models</strong>, we have provided a lecture on <strong>Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4</strong>, based on the public materials in the literature. This note summarizes the tutorial presentation and makes it more complete. It gives guided tours through the literature and explain topics to those who seek to learn the areas on LMMs from basics to the advances. It is prepared for audience including graduate students, researchers and professionals that LMMs are outside their specialties, to help them develop perspectives, and identify trends in LMMs in an accessible way.</p>
<blockquote>
<p>âœ… åœ¨æœ¬æ–‡ä¸­ï¼ŒLMMï¼multimodal LLM</p>
</blockquote>
<p><img src="./assets/N-3.png" alt="" /></p>
<p>In the full tutorial, as shown in Figure 2, we have covered the most recent approaches and principles at the frontier of learning and applying vision foundation models, <strong>including</strong> Q1: Visual and Vision-Language Pre-training; Q2: Generic Vision Interface; Q3: Alignments in Text-to-image Generation; Q4: Large Multimodal Models; and Q5: Multimodal Agents.<br />
This note <strong>focuses</strong> <strong>on Q4: how to leverage LLM for multimodality, and train LMMs in an end-to-end fashion, so that the models can see and chat.</strong> The presentation consists of three parts. To start, we first share background on recent GPT-like large models for vision-and-language modeling in Section 2. In the 2nd part, as a pre-requisite, we will introduce the concept of instruction tuning in language domains in Section 3, which empowered ChatGPT. Finally, Section 4 covers the last part of the presentation, where we focus on how to build a minimum version of multimodal GPT-4, using LLaVA as a running example. Since LMM is a popular research topic, many new papers have appeared in this line of research in the past three months, of which we provide a summary, so that the audience may quickly get a picture on what the LMM community has been working on.<br />
The related links of the tutorial presentation on large multimodal models are available at:</p>
<ul>
<li><em>Slides</em>: <a href="https://tinyurl.com/5c2c2mtm">https://tinyurl.com/5c2c2mtm</a></li>
<li><em>YouTube Video</em>: <a href="https://youtu.be/mkI7EPD1vp8">https://youtu.be/mkI7EPD1vp8</a></li>
<li><em>Bilibili Video</em>: <a href="https://www.bilibili.com/video/BV1Ng4y1T7v3/">https://www.bilibili.com/video/BV1Ng4y1T7v3/</a><br />
For the full information and other parts of the CVPR tutorial, please see the official website at:<br />
<a href="https://vlp-tutorial.github.io/">https://vlp-tutorial.github.io/</a></li>
</ul>
<p>P4</p>
<h2 id="2-background"><a class="header" href="#2-background">2 Background</a></h2>
<h3 id="21-image-to-text-generative-models"><a class="header" href="#21-image-to-text-generative-models">2.1 Image-to-Text Generative Models</a></h3>
<p>LMMs in their current form is primarily <strong>an image-to-text generative model, which takes images as input, and outputs a text sequence.</strong> One example is illustrated in Figure 3 (a) Left. All of the model variants share very similar model architecture and training objective.</p>
<ul>
<li><em>Model Architecture</em>. As illustrated in Figure 3 (a) Right, the model typically consists of an <strong>image encoder to extract visual features</strong>, and <strong>a language model to decode the text sequence</strong>. The vision and language modalities can be <strong>optionally connected by trainable connection module.</strong> The image encoder and language model can be either trained from scratch or initialized from pre-trained models.</li>
<li><em>Training Objective</em>. As illustrated in Figure 3 (b), it typically employs an auto-regressive loss on the output text tokens. For the attention map in the Transformers [46], <strong>image tokens can attend to each other, and the text token depends on and all image tokens and the previous text tokens.</strong></li>
</ul>
<p><img src="./assets/N-4-1.png" alt="" /><br />
<img src="./assets/N-4-2.png" alt="" /></p>
<blockquote>
<p>âœ… è¯­è¨€é€šå¸¸ä½¿ç”¨è‡ªå›å½’æ–¹å¼ï¼Œå›¾åƒé€šå¸¸ä½¿ç”¨ attenion æ–¹å¼ã€‚</p>
</blockquote>
<h3 id="22-case-studies"><a class="header" href="#22-case-studies">2.2 Case Studies</a></h3>
<p>We use some known LMMs as examples to illustrate how the network architecture framework can be instantiated in different models, while maintaining the same auto-regressive training objective.</p>
<p><strong>Case Study I: LMM trained with image-text pairwise instances.</strong> Most LMMs are trained on a large number of image-text pairs, where each training sample is a pair. GIT and BLIP2 are two large models that achieve state-of-the-art (SoTA) performance on many datasets. The comparisons are shown in Figure 4(a). GIT [48] initializes image encoder with constrastive pre-trained Microsoft Florence model, and train a language model from scratch. On the other hand, BLIP2 freezes the weights of pre-trained image and language model, and a train lightweight Q-former. <strong>BLIP2 [20] shows higher sample-efficiency with the bootstrapping training method.</strong></p>
<blockquote>
<p>âœ… GIT å¯¹æ‰€æœ‰æ¨¡å—è¿›è¡Œç«¯åˆ°ç«¯è®­ç»ƒã€‚<br />
âœ… BLIP2 fix å·²æœ‰æ¨¡å—ï¼Œä»…è®­ç»ƒæ–°å¢çš„ connection æ¨¡å—ã€‚ </p>
</blockquote>
<p>p5</p>
<p><img src="./assets/N-5-1.png" alt="" /></p>
<p><img src="./assets/N-5-2.png" alt="" /></p>
<p><strong>Case Study II: LMM trained with interleaved image-text sequence instances.</strong> We use Flamingo [1] as example, shown in Figure 4(b). It connect the frozen pre-trained image and language models â€“ by adding novel architectural components in between. Specifically, <strong>Perceiver Sampler module helps reduce compute complexity, and Gated Transformer module helps stabilize training in the initial stage.</strong> Flamingo is trained on a mixture of complementary large-scale multimodal data coming only from the web, without using any data annotated for machine learning purposes. After this training is done, Flamingo can be directly adapted to vision tasks via simple few-shot learning without any additional task-specific tuning.</p>
<blockquote>
<p>â“ è¿™ä¸ªæ•°æ®é›†å’Œ pair data æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ<br />
âœ… Flamingo çš„è®­ç»ƒæ–¹å¼åŒ BLIP2ï¼</p>
</blockquote>
<p><strong>Multimodal In-Context-Learning.</strong> Beside the SoTA performance on dozens of academic bench-marks, proabably the most appealing aspect of Flamingo is that it exhibits an emerged property: Multimodal In-Context-Learning. Specifically, <strong>given a couple of image-text pairs as examples, Flamingo can zero-shot task transfer to new unseen problems, such as solving visual math problems</strong>. This means Flamingo can tackle a number of difficult problems with just a handful of task-specific examples, <strong>without any additional training required.</strong> For example in Figure 5, two new tasks are presented to Flamingo. The top row provides two image-text pairs as the context in the prompt, where the text describes the name of the animal in the image, followed by the geographical information
of the animal. Flamingo is able to understand the patterns in the task instruction illustrated by the examples, and output the corresponding information for a new image. In the bottom row, the text first shows the optical character recognition (OCR) result of the image, followed by the arithmetic result. Flamingo learns the task instruction illustrated in the multimodal context, outputs the correct answer for a new math problem in the image. Therefore, Flamingo is generally considered as the GPT-3 moment [3] in the multimodal domain.</p>
<blockquote>
<p>âœ… å¯¹äºæ–°ä»»åŠ¡ï¼Œä¸éœ€è¦è®­ç»ƒï¼Œåªéœ€è¦ç»™å‡ ä¸ªä¾‹å­å°±èƒ½å­¦ä¼šã€‚<br />
â“ Flamingo æœ‰äº¤äº’åŠŸèƒ½å—ï¼Ÿæ€æ ·å­¦ä¹ ä¾‹å­ï¼Ÿ<br />
â“ è¿™ä¸ªç‰¹æ€§ä¸ In-Context-Learning æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ</p>
</blockquote>
<p>P6<br />
<img src="./assets/N-6-1.png" alt="" /></p>
<h3 id="23-openai-multimulti-gpt4-and-research-gaps"><a class="header" href="#23-openai-multimulti-gpt4-and-research-gaps">2.3 OpenAI Multimulti GPT4 and Research Gaps</a></h3>
<p>In March 2023, OpenAI released GPT-4 [33], with impressive capability in visual understanding and reasoning. Though the model details are unknown, there is no doubt that GPT4 enables many new scenarios, based on the examples highlighted the technique report. For instance, two popular visual examples are illustrated in Figure 6. The first one identifies the uncommon visual region and exhibits
strong complex reasoning performance. The second one recognizes text in the image and captures the mere across image-text. For a while, the research community had no clue how this new ability is achieved (probably because they are not tightened to any established academic tasks/datasets), but all are determined that these are exciting results. It naturally raise a question: How can we build Multimodal GPT-4 like models?</p>
<p><img src="./assets/N-6-2.png" alt="" /></p>
<p>To answer it, we start to review the big models from OpenAI, by highlighting the most appealing properties for each model in Figure 7. There are several key observations: (i) GPT-2 [38] is the auto-regressive counterpart in the BERT era [8] for the paradigm of pre-training then fine-tuning. Compared with GPT-2, GPT-3 [3] is a 175B model trained on web-scale text corpus, which exhibits two emerging properties with a frozen model: in-context-learning [3] and chain-of-thoughts (CoT) reasoning [53].. This means, without any additional training required, the model can tackle a wide range of new problems with just a few task-specific examples and by properly prompting it step-by-step, respectively. <strong>It further leads to the paradigm from fine-tuning model weights to prompting</strong></p>
<p>P7</p>
<p><img src="./assets/N-7.png" alt="" /></p>
<p><strong>frozen models, where the latter shows higher generality and lower adaptation cost in task transfer.</strong> (ii) ChatGPT and InstructGPT [34] shows the importance of instruction-following and alignment with human intents for LLMs, by fine-tuning the base language model GPT-3/GPT-3.5 on high quality instruction-following data, and improving them with a reward model via reinforcement learning with human feedback. (\(iii\)) GPT-4 not only improves the language ability of previous models, but also allows visual signals as additional input for understanding and reasoning. We see that the newer generation model maintains/improves the existing properties of the previous ones, and enable new properties.</p>
<blockquote>
<p>âœ… In-Context-learning æŒ‡é€šè¿‡æ–°ä»»åŠ¡çš„ä¾‹å­å­¦ä¹ æ–°ä»»åŠ¡ã€‚<br />
âœ… Instruction-Following æŒ‡é€šè¿‡ç†è§£ä»»åŠ¡æè¿°å®Œæˆæ–°ä»»åŠ¡ã€‚ </p>
</blockquote>
<p>In another words, from GPT-3 to GPT-4, we see two new properties: instruction-following and multimodal input. This reveals the gap between existing LMMs such as Flamingo and multimodal GPT-4: how to perform instruction-following and alignment research in the multimodal space. and thus the focus of this tutorial &amp; note.</p>
<p>P8</p>
<h2 id="3-pre-requisite-instruction-tuning-in-large-language-models"><a class="header" href="#3-pre-requisite-instruction-tuning-in-large-language-models">3 Pre-requisite: Instruction Tuning in Large Language Models</a></h2>
<p>Note that instruction-following is a notion originated in natural language processing (NLP). To study the intuition and gain a full picture of the history, we revisit instruction tuning with LLMs.</p>
<h3 id="31-instruction-tuning"><a class="header" href="#31-instruction-tuning">3.1 Instruction Tuning</a></h3>
<p><img src="./assets/N-8-1.png" alt="" /><br />
<img src="./assets/N-8-2.png" alt="" /><br />
<img src="./assets/N-8-3.png" alt="" /></p>
<p><strong>Traditional Language Data</strong>. As a typical data instance in NLP, seq2seq representation is quite common for many language tasks: each data instance consists of two parts: sequence as the input and sequence as the output. We provide two examples in Figure 8 (a). Without any task instruction specified, we know they are translation and summarization tasks, respectively.</p>
<p>This seq2seq representation is also how NLP community used to use their data. <strong>Task instructions are implicit</strong>. Based on each data domain, <strong>individual models are trained, or sometimes multi-tasking over multiple data domain without specifying the task instructions</strong>. When such models are trained, they are <strong>hard to generalize to new tasks in a zero-shot fashion</strong>, because the models do not learn the skill to understand the task instruction, and have no ability to distinguish and generalize what task to perform in the testing stage.</p>
<p><strong>Instruct Language Data.</strong> Instead, recently researchers start to <strong>explicitly add task instructions in the model training,</strong> as shown in Figure 8 (b). Interestingly, the task instructions of most NLP tasks can be <strong>expressed in natural language</strong> as well. It leads a new data format: instruction-input-output triplets. Based on the new format, <strong>one single model can be trained, multi-tasking with specified instructions.</strong> Since models have observed many task instructions and many instances for each task in training, it <strong>is natural and easy for the models to generalize to new tasks by task composition</strong> in the inference stage.</p>
<p>P9<br />
For example, in the evaluation stage, a new task that require both summarization and translation is provided in Figure 8 (c). Though the model has never seen this new task in training, it observes individual task basis, and learn to perform on new tasks. Note that we humans are always creating new tasks in our daily life, and presumably these new tasks would never been observed by models. It is thus appealing if a model is able to solve thousands of new tasks in the wild in without training. This is partially why ChatGPT is becoming popular and prevalent quickly.</p>
<p><strong>3.2 Self-Instruct and Open-Source LLMs</strong></p>
<p>How can we collect a diverse set of high-quality instruction-following data? There are two general schemes. One is human-human interaction, where humans (task providers) provide the annotation statement and requirements, based on which another group of humans complete the annotation tasks. such a scheme is typically cost and time consuming. The other scheme is human-machine interaction, where similarly <strong>humans provide the annotation statement and requirements</strong>, but it is now the <strong>machines/models that complete the annotation tasks.</strong></p>
<p>To enable LLMs to follow natural language instructions and complete real-world tasks, researchers have been exploring methods of <strong>instruction-tuning</strong> of LLMs. This is implemented by either fine-tuning the model on a wide range of tasks using human-annotated prompts and feedback [34], or supervised finetuning using public benchmarks and datasets augmented with manually or automatically generated instructions [52]. Among these methods, Self-Instruct tuning [51] is a simple and effective method of aligning LLMs to human intent, by <strong>learning from instruction-following data generated by SoTA teacher LLMs.</strong> It turns out that the line of instruction-tuning research has produced effective means to improve the zero and few-shot generalization abilities of LLMs. Self-instruct leverages the in-context-learning ability of LLM. The pipeline is illustrated in Figure 9. Humans create a few examples (i.e., seed examples) as the context, and ask LLM such as GPT-3 or GPT-4 to create more instruct and responses that follows the requirements stated in the prompt. The machine-generated instruction-following data can be further selected to construct with the prompt for in-context-learning in the next data generation iteration. The procedure iterates till a given number of samples are collected. Due to the relatively lower cost and higher response speed of API calls (compared with human annotations), self-instruct is becoming more favorable in the research community.</p>
<p><img src="./assets/N-9.png" alt="" /></p>
<blockquote>
<p>âœ… (1) äººå·¥ç”Ÿæˆä¸€äº›ä¾‹å­ã€‚ (2) LLM é€šè¿‡ä¾‹å­å­¦ä¹ ä»»åŠ¡ã€‚(3) LLM ç”Ÿæˆæ–°çš„é—®é¢˜å¹¶å›ç­”ã€‚ï¼ˆ4ï¼‰äººå·¥æŠŠç”Ÿæˆç»“æœå˜ä¸ºæ•°æ®ã€‚</p>
</blockquote>
<p><strong>Open-Source LLMs: LLaMA Family.</strong> The open-source community has witnessed a surge of open
LLM. The success of ChatGPT [32] and GPT-4 [33] offers tremendous opportunities to improve open-source LLMs using instruction-tuning. Figure 10 compares several open-source instruction tuned LLMs. LLaMA [45] is a series of open-sourced LLMs, which match the performance of proprietary LLMs such as GPT-3. To teach LLaMA to follow instructions, Self-Instruct tuning has been quickly adopted given its superior performance and low cost. For example, to name a few early attempts in this line of research, Stanford Alpaca [43] uses 52K instruction-following samples generated by GPT-3.5, while Vicuna [47] uses around 500K high-quality instruction-following samples (150K conversions) between user and GPT [39]. To advance the SoTA of instruction-tuning for LLMs, GPT-4 is utilized as the teacher to generate the responses for the Alpaca instructions [36]. Many papers have been proposed to improve the instruction-following data to improve the model alignment quality in chat. For a comprehensive review, we suggest the readers to refer the recent paper [50], where a LLM Tulu is trained on a mix of several high-quality instruct data, and comprehensive comparisons are conducted across multiple benchmarks.</p>
<p>P10<br />
<img src="./assets/N-10-1.png" alt="" /><br />
<img src="./assets/N-10-2.png" alt="" /></p>
<p><strong>Quick Assessment of LLM Chatbots.</strong> To study the quality of LLM Chatbots, We consider <em>Vicuna-Instructions</em>-\(80^2\) [47], a dataset with <strong>80 challenging questions that baseline models find challenging.</strong> Beside generic instructions, there are 8 categories, including knowledge, math, Fermi, counterfactual, roleplay, generic, coding, writing, common-sense. To quantitatively compare the performance, we <strong>ask GPT-4 to rate the response</strong> from score 1 to 10 for any two given chatbots, then compute the relative score. The results are shown in Figure 11. Surprisingly, it turns out this evaluation metric is quite consistent across different settings. The open-source LLaMA family seem performing closely to SoTA proprietary Chatbots.</p>
<p><strong>Further Discussions.</strong> There are several important topics on LLMs that we have not covered in the tutorial presentation, but are worthwhile future exploring.</p>
<ul>
<li>
<p><em>Data-centric AI</em>. We emphasize that the developmet of these open-source LLM projects is data-centric [29], rather than model-centric, so that we hope readers could align the perspective when discussing the topic. <strong>As the training objective and network architectures are becoming similar and even identical</strong> on GPT-like projects, <strong>the key differential factor is data.</strong> For example, behaviors of the aforementioned LLMs are determined by the instruction tuning data.</p>
</li>
<li>
<p><em>False Promise?</em> There is a debate that the open LLMs could catch up with the proprietary LLMs is a false promise [14]. To align the discussions, we argue that <strong>there are two distinctive abilities for LLMs: the instruction-following ability to know which task to perform, and massive knowledge storage to complete the task with quality. Imitation models are good at the former,</strong> by mimicking ChatGPTâ€™s style <strong>but not its factuality.</strong> They authors in [14] conclude that there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. They also advocate that the highest leverage action for improving open-source models is <strong>to tackle the difficult challenge of developing better base LMs.</strong> However, unfortunately the resources to train such base LMs are only available in a few industry labs, and the formulas to train the base LMs is largely well explored. It seems more promising for most academic research labs to explore the opportunities in alignment research with affordable resources, or explore the techniques to reduce the compute the barriers.</p>
</li>
</ul>
<blockquote>
<p>âœ…  Imitation Modes ä» base model å¤„å¾—åˆ°å¤§é‡æ•°æ®ï¼Œå¯å¾—åˆ° instruction-following çš„èƒ½åŠ›ï¼Œä½†å…¶è´¨é‡æ— æ³•è¾¾åˆ° base model.</p>
</blockquote>
<ul>
<li><em>Base LLMs</em>. Developing more capable or commercial usable LLMs is of great value. Besides LLaMA, the open-source community has developed several capable base LLMs such as OpenLLaMA [11], MPT [44] and Falcon [35], or released the training recipe [5].</li>
</ul>
<p><a href="https://github.com/lm-sys/FastChat/blob/main/fastchat/eval/table/question.jsonl">https://github.com/lm-sys/FastChat/blob/main/fastchat/eval/table/question.jsonl</a></p>
<p>P11</p>
<h2 id="4-instructed-tuned-large-multimodal-models"><a class="header" href="#4-instructed-tuned-large-multimodal-models">4 Instructed Tuned Large Multimodal Models</a></h2>
<p>In this tutorial, we illustrate how to <strong>build the minimum prototype of multimodal GPT4 with open-source resources.</strong> Specially, we use LLaVA [24] as the running example, a similar idea is also
proposed in its co-current work miniGPT-4 [66].</p>
<h3 id="41-open-source-prototypes-llava--minigpt4"><a class="header" href="#41-open-source-prototypes-llava--minigpt4">4.1 Open-Source Prototypes: LLaVA / MiniGPT4</a></h3>
<p>The research in the multimodal space has often been inspired by the latest advances in NLP in recent years. One successful recipe is to keep asking what would happen if the most intriguing and successful NLP ideas are borrowed for the vision-and-language community. We are leveraging the self-instruct idea from the language domain. The unique challenge with self-instruct is that <strong>there is no strong multimodal teacher available yet. How can we use language model such as language-only GPT-4 to create multimodal instruction following data.</strong></p>
<p><img src="./assets/N-11-1.png" alt="" /></p>
<p><img src="./assets/N-11-2.png" alt="" /></p>
<h3 id="411-data-creation"><a class="header" href="#411-data-creation">4.1.1 Data Creation</a></h3>
<p>Instead of directly feed images into OpenAI GPT, we use their <strong>symbolic sequence representations</strong> shown in Figure 12 (a). In LLaVA, the caption and boxes are considered, due to the following</p>
<p>P12<br />
<img src="./assets/N-12-1.png" alt="" /></p>
<p>reasons: (1) it is empirically found that GPT-4 can understand them well, in contrast that ChatGPT has a difficult time in understanding the box data. (2) they are important to <strong>represent the image as informative as possible.</strong></p>
<blockquote>
<p>âœ… å›¾åƒ â†’ ç»“æ„åŒ–æ–‡æœ¬ â†’ æ–‡æœ¬è¾“å‡ºã€‚<br />
âœ… ç»“æ„åŒ–æ–‡æœ¬ç§°ä¸º text representation.</p>
</blockquote>
<p>As exemplified in Figure 12 (b), three types of instruction-following data are considered: <strong>multi-turn conversations</strong> so that users can chat with bot, <strong>detailed description</strong> so that long response can be generated from the bot; Lastly, <strong>complex reasoning</strong>, this is more about the implication of the image, rather than the image content. For example, â€œwhat challenge do these people faceâ€ in this image? The image is about a SUV in the parking area, while the challenge is how the luggage can be packed into the SUV due to the tight space in the car. In total, 158K samples are collected.</p>
<p>To summarize, the trick is that whatever tasks one wants to the model to perform in the serving stage, it is important to <strong>create the corresponding instruction-following for the training</strong>.</p>
<blockquote>
<p>â“ æ€æ ·è®©æ¨¡å‹ä¸åªè¯†åˆ«å›¾ç‰‡ä¿¡æ¯ï¼Œè¿˜è¦æ ¹æ®å›¾ç‰‡åšå¤æ‚æ¨æ–­ï¼Ÿ</p>
</blockquote>
<h3 id="412-network-architecture-and-training"><a class="header" href="#412-network-architecture-and-training">4.1.2 Network Architecture and Training</a></h3>
<p>As illustrated in Figure 13, the LLaVA network architecture is an instantiation of the general image-to-text generative model framework introduced in Section 2 and Figure 3. Specifically, <strong>LLaVa connects
pre-trained CLIP ViT-L/14 visual encoder [37] and large language model Vicuna [47], using a simple
projection matrix.</strong> A two-stage instruction-tuning procedure is considered:</p>
<ul>
<li><em>Stage 1: Pre-training for Feature Alignment.</em> Only the projection matrix is updated, based on a subset of CC3M [40]. The only task is <strong>image captioning</strong>.</li>
<li><em>Stage 2: Fine-tuning End-to-End.</em> Both the projection matrix and LLM are updated for two different use scenarios.</li>
</ul>
<blockquote>
<p>âœ… å³ä½¿æ¯ä¸ªæ¨¡å—åˆ†å·¥æ˜ç¡®ä¸”å•ç‹¬è®­å¥½ï¼ŒE2E çš„ finetune è¿˜æ˜¯å¿…ä¸å¯å°‘çš„ã€‚</p>
</blockquote>
<h3 id="413-performance"><a class="header" href="#413-performance">4.1.3 Performance</a></h3>
<p><strong>Performance on Visual Chat: Towards building multimodal GPT-4 level chatbot.</strong> . LLaVA is fine-tuned on the generated multimodal instruction-following data, which contains a diverse set of task instruction and response for daily user-oriented applications. It is empirically found that <strong>fine-tuning the linear projection layer only is sufficient for the chat demo/scenarios, though it requires longer training time.</strong></p>
<p><img src="./assets/N-12-2.png" alt="" /></p>
<p>An evaluation dataset with 30 unseen images is constructed: each image is associated with three types of instructions: conversation, detailed description and complex reasoning. This leads to 90 new language-image instructions, on which we test LLaVA and GPT-4, and use GPT-4 to rate their responses from score 1 to 10. The summed score and relative score per type is reported in Figure 14. Overall, LLaVA achieves 85.1% relative score compared with GPT-4, <strong>indicating the effectiveness of the proposed self-instruct method in multimodal settings.</strong></p>
<p>P13<br />
<strong>Performance on Science QA: New SoTA with the synergy of LLaVA with GPT-4.</strong> LLaVA is fine-tuned on a multimodal reaï¿¾soning dataset in the science domain [26]. In Figure 15, LLaVA alone achieves 90.92%. We use the language-only GPT-4 as the judge, to predict the final answer based on its own previous answers and the LLaVA answers. This â€œGPT-4 as judgeâ€ scheme yields a new SoTA 92.53%.</p>
<p><img src="./assets/N-13.png" alt="" /></p>
<p>P14<br />
<strong>Performance on OCR in the wild: An emerging property.</strong> LLaVA has never been explicitly trained on OCR data, i.e., images that contains text from the corresponding caption. Surprisingly, <strong>the model show strong zero-shot OCR task transfer ability in the wild.</strong> Some examples are shown in Figure 16.</p>
<p>P16</p>
<h3 id="42-emerging-topics"><a class="header" href="#42-emerging-topics">4.2 Emerging Topics</a></h3>
<p><img src="./assets/N-16-3.png" alt="" /></p>
<p><img src="./assets/N-16-2.png" alt="" /></p>
<p>The history of recent instructed tuned LMM are illustrated in Figure 17 (a). Due to the popularity of ChatGPT and GPT-4, <strong>instructed tuned LMM</strong> appears as an emerging line of research in the past three months after GPT-4 was proposed. Alpaca and Vicuna were proposed to make LLaMA more instruction-following in the language domain in March. In two weeks, MiniGPT-4 and LLaVA were proposed to make Vicuna to see and chat about the visual world. In ten days, Llama-Adpter v2 and mPlug-OWL started to compare performance with MiniGPT-4/LLaVA, indicating the beginning of model evolution. The data points in April are relatively sparse. In May, a large number of LMM papers appeared on arXiv, which improve this line of research from many different aspects. The momentum is till going in June.</p>
<p>P17<br />
It is easy to lose track of all the recent papers for the readers, so as well in our literature review. To better organize the literature, we group them based on specific research topics in this tutorial, shown in Figure 17 (b). The early LMMs with billions of parameters include GPT-4 [33], Flamingo [1], PaLM-E [9] and KOSMOS-1 [15]. In constrast to these proprietary LMMs, LLaVA/MiniGPT-4 open the opportunities to build LMMs with open-source resource. We will discuss the several topics as below, in addition to dense prediction [49, 60], video [62, 28, 21], image generation [16] and embodied agent [31].</p>
<h4 id="421-more-modalities-beyond-vl"><a class="header" href="#421-more-modalities-beyond-vl">4.2.1 More Modalities (Beyond VL)</a></h4>
<p>ğŸ” <em>ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst [65]</em><br />
ğŸ” <em>PandaGPT: One Model To Instruction-Follow Them All [41]</em><br />
ğŸ” <em>SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities [61]</em><br />
ğŸ” <em>X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages [4]</em></p>
<p>While LMM extends LLM by adding the vision modality into language, it is natural to further extend the framework to include more modalities beyond vision and language. Following this spirit, several attempts have been made. In Figure 18, PandaGPT leverages ImageBind to add more modalities into LMMs. The ImageBind model [12] <strong>learns a single, shared representation space for text, image/video, audio, sensors that record depth (3D), thermal (infrared radiation), and inertial measurement units (IMU), which calculate motion and position.</strong> ImageBind provides a holistic understanding of the visual world that connects objects in a photo with how they will sound, their 3D shape, how warm or cold they are, and how they move. <strong>By training a projection layer for one modality in LMM, the model can zero-shot transfer to infer over other modalities due to the shared multimodal embedding space.</strong> Another representative model is SpeechGPT, where language and speech modalities are enabled for both input and output ends. Despite of rich model variations, the idea to connect diverse modalities is similar to LMM that adds images into LLMs.</p>
<blockquote>
<p>â“ æŠŠå¤šç§æ¨¡æ€ä¿¡æ¯èåˆåˆ°åŒä¸€ç©ºé—´ï¼Œé‚£å¤šç§éª¨éª¼åŠ¨ä½œä¹Ÿå¯ä»¥ï¼Œå“ªæ¥çš„ pairdataå‘¢ï¼Ÿ<br />
â“ åªè®­ä¸€ä¸ªæ¨¡æ€ï¼Œå…¶å®ƒæ¨¡æ€èƒ½è‡ªåŠ¨è¿ç§»ï¼Œè¿™äº›æ¨¡æ€æ˜¯æ€ä¹ˆå¯¹é½çš„ï¼Ÿ<br />
â“ ä¸åŒéª¨éª¨åŠ¨ä½œçš„è¿ç§»ï¼ŒBVH èƒ½å¦ä½œä¸ºä¸­é—´çš„ç»“æ„åŒ–æ–‡æœ¬ï¼Ÿ</p>
</blockquote>
<p><img src="./assets/N-17.png" alt="" /></p>
<p>P18<br />
<img src="./assets/N-18.png" alt="" /></p>
<h4 id="422-multitask-instruct-with-established-academic-datasetstasks"><a class="header" href="#422-multitask-instruct-with-established-academic-datasetstasks">4.2.2 Multitask Instruct with Established Academic Datasets/Tasks</a></h4>
<p>ğŸ” <em>MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning [57]</em><br />
ğŸ” <em>mPlug-OWL: Modularization empowers large language models with multimodality [58]</em><br />
ğŸ” <em>InstructBLIP: Towards general-purpose vision-language models with instruction tuning [6]</em><br />
ğŸ” <em>Multimodal-GPT: A vision and language model for dialogue with humans [13]</em><br />
ğŸ” <em>Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT [54]</em></p>
<p>As discussed earlier in Section 3, instruction tuning in the language domains is implemented in two different ways: <strong>fine-tuning the model on a wide range of tasks using human-annotated prompts and feedback</strong>[34], or <strong>supervised fine-tuning using public benchmarks and datasets augmented with manually or automatically generated instructions [52]</strong>. The former is good at user-oriented daily life tasks, and the latter is good at achieving good numbers on established benchmarks. LLaVA/MiniGPT-4 can be categorized as the former class. Several other works either target for the latter class or combine both classes.</p>
<blockquote>
<p>âœ… ç”¨ prompt ä½¿ç”¨æ›´å‹å¥½ï¼Œä½†ç”¨æ•°æ® finetue èƒ½å¾—åˆ°æ›´å¥½çš„æ•ˆæœã€‚<br />
âœ… å‰è€…æ•°æ®æ¥è‡ª daily conversationï¼Œå› æ­¤æ²¡æœ‰æ˜ç¡®çš„ä»»åŠ¡ç±»å‹ï¼Œå±äºé€šæ‰ã€‚<br />
âœ… åè€…æ•°æ®æ¥ä¸“ç”¨æ•°æ®é›†ï¼Œæœ‰æ˜ç¡®çš„ä»»åŠ¡ç±»å‹ï¼Œå±äºä¸“æ‰ã€‚</p>
</blockquote>
<h4 id="423-multimodal-in-context-learning"><a class="header" href="#423-multimodal-in-context-learning">4.2.3 Multimodal In-Context-Learning</a></h4>
<p>ğŸ” <em>OpenFlamingo [2]</em><br />
ğŸ” <em>Otter: A Multi-Modal Model with In-Context Instruction Tuning [18]</em><br />
ğŸ” \(M^3\)<em>IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning [22]</em><br />
ğŸ” <em>MetaVL: Transferring In-Context Learning Ability From Language Models to Vision-Language Models [30]</em></p>
<p>Similar to the behaviour of LLMs, which can address a language task by processing examples of the task in their text prompt, <strong>multimodal in-context-learning refers to an visual and text interface can steer the model towards solving a multimodal task.</strong> Given a few example pairs of visual inputs and expected text responses composed in the multimodal prompt, the model can be asked a question with a new image or video, and then generate an answer.</p>
<p>P19<br />
OpenFlamingo [2] is an open source version of DeepMindâ€™s Flamingo model, trained on Multimodal C4 dataset [67], which is a billions-scale corpus of image interleaved with text. To explicit enhance the multimodal in-context-learning ability of LMMs, MIMIC-IT [17] dataset is constructed, which is 2.4M multimodal instruction instances with in-context examples. By tuning OpenFlamingo on MIMIC-IT, a new model Otter is obtained with a stronger instruction-following ability. The model life cycle is summarized in Figure 20. Using two image-text pairs as the context, Otter learns the concise answering style demonstrated by the examples, otherwise a tedious response is generated.</p>
<blockquote>
<p>âœ… æå‡ in-context-learning ä¸»è¦é å¢åŠ æ•°æ®é›†ã€‚</p>
</blockquote>
<p><img src="./assets/N-19-1.png" alt="" /></p>
<h4 id="424-parameter-efficient-training"><a class="header" href="#424-parameter-efficient-training">4.2.4 Parameter-Efficient Training</a></h4>
<p>ğŸ”  <em>LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model [10]</em><br />
ğŸ”  <em>Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models [27]</em></p>
<p>ğŸ”  <em>QLoRA: Efficient Finetuning of Quantized LLMs [7]</em></p>
<p>While fine-tuning very large models often leads to high performance, it is prohibitively expensive; For example, regular 16-bit fine-tuning of a LLaMA 65B parameter model [45] requires more than 780 GB of GPU memory [7]. Therefore, it is critical to reduce the memory footprint of LLMs/LMMs, especially when it comes to improve the accessibility of large models to a wider community. Parameter-efficient training is an effective approach for LMM adaptation. Two representative methods are illustrated in Figure 21. <strong>They freeze most of the model parameters, and only allow a small of trainable parameter to update with domain specific data.</strong> For example, LLaMA Adapter v2 and LAVIN only has 14M and 3.8M trainable parameters, compared with 7B/13B LLM parameters. <strong>Another efficient training method is quantization.</strong> The recent QLoRA finetunes 65B LLaMA for 24 hours on a single GPU, reaching 99.3% of the performance level of ChatGPT. Since instruction tuning typically involves a small amount of data, it makes parameter-efficient training or model quantization feasible with limited GPU resources.</p>
<blockquote>
<p>âœ… quantization æ˜¯ä»€ä¹ˆæŠ€æœ¯ï¼Ÿ</p>
</blockquote>
<p><img src="./assets/N-19-2.png" alt="" /></p>
<blockquote>
<p>âœ… å¯ä»¥åœ¨ä¸¤ä¸ªæ¨¡æ€çš„ä¸­é—´åŠ  adapterï¼Œå­¦ä¹ æ¨¡æ€é—´çš„ alignment.<br />
âœ… å¯ä»¥åœ¨ä¸¤ä¸ªæ¨¡æ€ä¸Šå¢åŠ  adapterï¼Œå¢åŠ æ¨¡æ€çš„æ³›åŒ–æ€§ã€‚</p>
</blockquote>
<p>P20<br />
<img src="./assets/N-20-1.png" alt="" /></p>
<h4 id="425-benchmarks"><a class="header" href="#425-benchmarks">4.2.5 Benchmarks</a></h4>
<p>ğŸ” <em>On the Hidden Mystery of OCR in Large Multimodal Models [25]</em><br />
ğŸ” <em>Evaluating Object Hallucination in Large Vision-Language Models [23]</em><br />
ğŸ” <em>On Evaluating Adversarial Robustness of Large Vision-Language Models [64]</em><br />
ğŸ” <em>LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark [59]</em><br />
ğŸ” <em>LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models [56]</em></p>
<p>While LMMs have shown excellent visual recognition and reasoning in an open-set manner with free-form text in many scenarios, the evaluation of LMMs is becoming an urgent and challenging problem. Several related benchmarks have been developed to evaluate various aspects of LMMs, ranging from their specific abilities including <strong>OCR[25], object hallucination [23] and adversarial robustness [64], to comprehensive evaluation [59, 56].</strong></p>
<blockquote>
<p>â“ è¿™å››ä¸ªèƒ½åŠ›æ˜¯æ€ä¹ˆè¯„ä»·çš„ï¼Ÿ<br />
âœ… OCRï¼šä»å›¾ç‰‡ä¸­è¯†åˆ«æ–‡æœ¬ã€‚LMM ä¸éœ€è¦å­¦ä¹ å°±å…·æœ‰çš„èƒ½åŠ›ï¼Œå…¶ä¸­ BLIP2 ç”šè‡³ä¼˜äºä¸“é—¨è®­ç»ƒçš„ OCR ä»»åŠ¡ SOTAï¼</p>
</blockquote>
<p>It is surprising that LMMs shows strong zero-shot OCR performance in the wild, without explicitly training on text recognition data. To shed light on the hidden mystery of OCR in LMMs, a compre-hensive empirical study is conducted in [25] to compare open-source LMMs on 24 academic text recognition datasets, shown in Figure 22. Three observations are highlighted: (1) LLaVA consistently outperforms miniGPT-4 on 21 out of 24 datasets, despite LLaVA being trained with an order of magnitude smaller training data. (2) Training with significantly larger training data leads to higher OCR performance, as demonstrated by BLIP2 [20] and mPLUG-Owl. (3) In most cases, supervised SoTA results significantly outperform zero-shot LMM. However, it is worth noting that in the WordArt dataset [55], which primarily features challenging artistic text, BLIP2 surpasses supervised SoTA. This reveals the potential of LMM in recognizing more complex text types.</p>
<p><img src="./assets/N-20-2.png" alt="" /></p>
<p>P21</p>
<h4 id="426-applications"><a class="header" href="#426-applications">4.2.6 Applications</a></h4>
<p>ğŸ” <em>PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology [42]</em><br />
ğŸ” <em>PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering [63]</em><br />
ğŸ” <em>LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day [19]</em></p>
<p>The success of ChatGPT/GPT-4 in the general domain has inspired the interests in building assistants in the vertical domains such as medicine, gaming and education. Such <strong>domain-specific assistants</strong> can have the several advantages over the general domain counterpart: (1) training high-quality domain knowledge makes the assistants more helpful, (2) the model size can be smaller, and thus severing cost is low, (3) the sensitive user prompt data can be maintained internally by serving the model at local, and the privacy issue can be avoided.</p>
<blockquote>
<p>â“ ä¸ºä»€ä¹ˆ domain-specific assistants ä¼šæ›´å°ï¼Ÿ</p>
</blockquote>
<p>LMMs have been recently explored in the biomedical domain [42, 63, 19], where conversational gener-ative AI has demonstrated remarkable promise for empowering biomedical practitioners. LLaVA-Med is a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model LLaVA using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. In Figure 23, we provide examples on the biomed visual conversations of different chatbots. LLaVA-Med precisely answers the questions with biomedical knowledge, while LLaVA behaves like a layperson, who hallucinate based on commonsense.</p>
<p>P22</p>
<h2 id="5-how-close-we-are-with-openai-multimodal-gpt-4"><a class="header" href="#5-how-close-we-are-with-openai-multimodal-gpt-4">5 How Close We Are with OpenAI Multimodal GPT-4?</a></h2>
<p>With all these new works, are we close or even surpassing OpenAI Multimodal GPT-4? It is encouraging to see that the open-source community has quickly developed a variety of models and prototypes for various new capabilities. For example, LLaVA/Mini-GPT4 paves the way towards building multimodal chatbots, with some examples that reproduce the results in OpenAI GPT-4 technique report; GILL [16] extends LMMs for end-to-end image generation, to our best knowledge, this is a capability that the current GPT-4 does not exhibit. From the perspective of enabling new multimodal capabilities with the minimum prototypes, the open-source community seems close to OpenAI Multimodal GPT-4, by exploring the baby steps towards building the general-purpose multimodal assistant.</p>
<p><img src="./assets/N-22.png" alt="" /></p>
<p>However, there is a large gap in terms of scaling a given capability, for example, even the for visual reasoning capability that we have observed in LLaVA. Figure 24 shows two more visual examples from OpenAI technique report. To correctly answer the questions, it requires models to understand multiple high-resolution images and long sequence, as well we responding with domain knowledge. It requires much larger compute and more powerful language models, which are not available for most people.</p>
<p>In summary, we have presented the background and strong capabilities of large multimodal models, reviewed instruction tuning in LLMs, and showed how we can build a prototype such as LLaVA and minigpt4 using open-sourced resources. We also summarize and cateorized the most recent papers merged on this line of research to help thoese who are interested to gain the momentum to start the journey of LMM research.</p>
<p>To discuss the next steps to work on as a community, one sustainable suggestion can be that <strong>those with resource can continue focusing on the scaling success and study new emerging properties, while others focus on prototypes for new functionalities and evaluation,</strong> as well as developing techniques to reduce the compute barriers and thus allow more accessibility for larger model compute.</p>
<p>P23<br />
<strong>Acknowledgments</strong></p>
<p>We thank all authors who have contributed to the related papers in LLM/LMM, which makes the tutorial possible. We have tried to track related papers for the CVPR tutorial before June 19, 2023, but may not cover all the papers on the topic, due to the fast research pace in LMMs. Apologies in advance.</p>
<p><strong>References</strong></p>
<p>[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. <em>arXiv preprint arXiv:2204.14198</em>, 2022. 5, 6, 17</p>
<p>[2] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo, March 2023. 13, 18, 19</p>
<p>[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. <em>Advances in neural information processing systems</em>, 33:1877â€“1901, 2020. 5,6</p>
<p>[4] Feilong Chen, Minglun Han, Haozhi Zhao, Qingyang Zhang, Jing Shi, Shuang Xu, and Bo Xu. X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages. <em>arXiv preprint arXiv:2305.04160</em>, 2023. 17</p>
<p>[5] Together Computer. Redpajama-data: An open source recipe to reproduce llama training dataset, 2023. 10</p>
<p>[6] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. <em>arXiv preprint arXiv:2305.06500</em>, 2023. 18</p>
<p>[7] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. <em>arXiv preprint arXiv:2305.14314</em>, 2023. 19</p>
<p>[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. <em>arXiv preprint arXiv:1810.04805</em>, 2018. 6</p>
<p>[9] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. PaLM-E: An embodied multimodal language model. <em>arXiv preprint arXiv:2303.03378</em>, 2023. 17</p>
<p>[10] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. <em>arXiv preprint arXiv:2304.15010</em>, 2023. 19</p>
<p>[11] Xinyang Geng and Hao Liu. Openllama: An open reproduction of llama, May 2023. 10</p>
<p>[12] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,</em> pages 15180â€“15190, 2023. 17</p>
<p>[13] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans. <em>arXiv preprint arXiv:2305.04790</em>, 2023. 18</p>
<p>[14] Arnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey Levine, and Dawn Song. The false promise of imitating proprietary llms. <em>arXiv preprint arXiv:2305.15717</em>, 2023. 10</p>
<p>P23<br />
[15] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning perception with language models. <em>arXiv preprint arXiv:2302.14045</em>, 2023. 17</p>
<p>[16] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models. <em>arXiv preprint arXiv:2305.17216</em>, 2023. 17, 22</p>
<p>[17] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-it: Multi-modal in-context instruction tuning. <em>arXiv preprint arXiv:2306.05425</em>, 2023. 19</p>
<p>[18] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal model with in-context instruction tuning. <em>arXiv preprint arXiv:2305</em>.03726, 2023. 18</p>
<p>[19] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training a large language-and-vision assistant for biomedicine in one day. <em>arXiv preprint arXiv:2306.00890</em>, 2023. 20, 21</p>
<p>[20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. <em>arXiv preprint arXiv:2301.12597</em>, 2023. 4, 5, 13, 20</p>
<p>[21] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. <em>arXiv preprint arXiv:2305.06355</em>, 2023. 17</p>
<p>[22] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. M3it: A large-scale dataset towards multi-modal multilingual instruction tuning. <em>arXiv preprint arXiv:2306.04387</em>, 2023. 18</p>
<p>[23] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. <em>arXiv preprint arXiv:2305.10355</em>, 2023. 20</p>
<p>[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. <em>arXiv preprint arXiv:2304.08485</em>, 2023. 11, 12, 13, 14</p>
<p>[25] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. <em>arXiv preprint arXiv:2305.07895</em>, 2023. 20</p>
<p>[26] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. <em>Advances in Neural Information Processing Systems</em>, 2022. 13</p>
<p>[27] Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, and Rongrong Ji. Cheap and quick: Efficient vision-language instruction tuning for large language models. <em>arXiv preprint arXiv:2305.15023</em>, 2023. 19</p>
<p>[28] Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Minghui Qiu, Pengcheng Lu, Tao Wang, and Zhongyu Wei. Valley: Video assistant with large language model enhanced ability. <em>arXiv preprint arXiv:2306.07207</em>, 2023. 17</p>
<p>[29] Mark Mazumder, Colby Banbury, Xiaozhe Yao, Bojan KarlaÅ¡, William Gaviria Rojas, Sudnya Diamos, Greg Diamos, Lynn He, Douwe Kiela, David Jurado, et al. Dataperf: Benchmarks for data-centric ai development. <em>arXiv preprint arXiv:2207.10062</em>, 2022. 10</p>
<p>[30] Masoud Monajatipoor, Liunian Harold Li, Mozhdeh Rouhsedaghat, Lin F Yang, and Kai-Wei Chang. Metavl: Transferring in-context learning ability from language models to vision-language models. <em>arXiv preprint arXiv:2306.01311</em>, 2023. 18</p>
<p>P25<br />
[31] Yao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng Dai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of thought. <em>arXiv preprint arXiv:2305.15021</em>, 2023. 17</p>
<p>[32] OpenAI. ChatGPT. <a href="https://openai.com/blog/chatgpt/,">https://openai.com/blog/chatgpt/,</a> 2022. 3, 9</p>
<p>[33] OpenAI. GPT-4 technical report. <a href="https://arxiv.org/abs/2303.08774,">https://arxiv.org/abs/2303.08774,</a> 2023. 3, 6, 9, 13, 14, 17, 22</p>
<p>[34] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. <em>Advances in Neural Information Processing Systems</em>, 35:27730â€“27744, 2022. 7, 9, 18</p>
<p>[35] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. <em>arXiv preprint arXiv:2306.01116</em>, 2023. 10</p>
<p>[36] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with GPT-4. <em>arXiv preprint arXiv:2304.03277</em>, 2023. 9</p>
<p>[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. <em>arXiv preprint arXiv:2103.00020</em>, 2021. 12</p>
<p>[38] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. <em>OpenAI blog</em>, 2019. 6</p>
<p>[39] ShareGPT. y<a href="https://sharegpt.com/,">https://sharegpt.com/,</a> 2023. 9</p>
<p>[40] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. <em>In ACL</em>, 2018. 12</p>
<p>[41] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to instruction-follow them all. <em>arXiv preprint arXiv:2305.16355</em>, 2023. 17</p>
<p>[42] Yuxuan Sun, Chenglu Zhu, Sunyi Zheng, Kai Zhang, Zhongyi Shui, Xiaoxuan Yu, Yizhi Zhao, Honglin Li, Yunlong Zhang, Ruojia Zhao, et al. Pathasst: Redefining pathology through generative foundation ai assistant for pathology. <em>arXiv preprint arXiv:2305.15072</em>, 2023. 21</p>
<p>[43] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. <a href="https://github.com/tatsu-lab/stanford_alpaca,">https://github.com/tatsu-lab/stanford_alpaca,</a> 2023. 9</p>
<p>[44] MosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, ly usable llms, 2023. Accessed: 2023-03-28. 10</p>
<p>[45] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoï¿¾thÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. <em>arXiv preprint arXiv:2302.13971</em>, 2023. 9, 19</p>
<p>[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In <em>NeurIPS</em>, 2017. 4</p>
<p>[47] Vicuna. Vicuna: An open-source chatbot impressing GPT-4 with 90%* chatgpt quality. <a href="https://vicuna.lmsys.org/,">https://vicuna.lmsys.org/,</a> 2023. 9, 10, 12</p>
<p>[48] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text transformer for vision and language. <em>arXiv preprint arXiv:2205.14100</em>, 2022. 4, 5</p>
<p>[49] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, et al. VisionLLM: Large language model is also an open-ended decoder for vision-centric tasks. <em>arXiv preprint arXiv:2305.11175</em>, 2023. 17</p>
<p>P26<br />
[50] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels go? exploring the state of instruction tuning on open resources. <em>arXiv preprint arXiv:2306.04751</em>, 2023. 9</p>
<p>[51] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instruc-tions. <em>arXiv preprint arXiv:2212.10560</em>, 2022. 9</p>
<p>[52] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, et al. Benchmarking generalization via in-context instructions on 1,600+ language tasks. <em>arXiv preprint arXiv:2204.07705</em>, 2022. 9, 18</p>
<p>[53] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. <em>arXiv preprint arXiv:2201.11903</em>, 2022. 6</p>
<p>[54] Zhenxiang Xiao, Yuzhong Chen, Lu Zhang, Junjie Yao, Zihao Wu, Xiaowei Yu, Yi Pan, Lin Zhao, Chong Ma, Xinyu Liu, et al. Instruction-vit: Multi-modal prompts for instruction learning in vit. <em>arXiv preprint arXiv:2305.00201</em>, 2023. 18</p>
<p>[55] Xudong Xie, Ling Fu, Zhifei Zhang, Zhaowen Wang, and Xiang Bai. Toward understanding wordart: Corner-guided transformer for scene text recognition, 2022. 20</p>
<p>[56] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language models. <em>arXiv preprint arXiv:2306.09265</em>, 2023. 20</p>
<p>[57] Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning. <em>arXiv preprint arXiv:2212.10773</em>, 2022. 18</p>
<p>[58] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. <em>arXiv preprint arXiv:2304.14178</em>, 2023. 18</p>
<p>[59] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu Sheng, Lei Bai, Xiaoshui Huang, Zhiyong Wang, et al. Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark. <em>arXiv preprint arXiv:2306.06687</em>, 2023. 20</p>
<p>[60] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection with multimodal large language models. <em>arXiv preprint arXiv:2305.18279</em>, 2023. 17</p>
<p>[61] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu. Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities. <em>arXiv preprint arXiv:2305.11000</em>, 2023. 17</p>
<p>[62] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding. <em>arXiv preprint arXiv:2306.02858</em>, 2023. 17</p>
<p>[63] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Weixiong Lin, Ya Zhang, Yanfeng Wang, and Weidi Xie. Pmc-vqa: Visual instruction tuning for medical visual question answering. <em>arXiv preprint arXiv:2305.10415</em>, 2023. 21</p>
<p>[64] Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man Cheung, and Min Lin. On evaluating adversarial robustness of large vision-language models. <em>arXiv preprint arXiv:2305.16934</em>, 2023. 20</p>
<p>[65] Zijia Zhao, Longteng Guo, Tongtian Yue, Sihan Chen, Shuai Shao, Xinxin Zhu, Zehuan Yuan, and Jing Liu. Chatbridge: Bridging modalities with large language model as a language catalyst. <em>arXiv preprint arXiv:2305.16103</em>, 2023. 17</p>
<p>[66] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models, 2023. 11</p>
<p>P27<br />
[67] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of images interleaved with text. <em>arXiv preprint arXiv:2304.06939</em>, 2023. 19</p>
<hr />
<blockquote>
<p>æœ¬æ–‡å‡ºè‡ªCaterpillarStudyGroupï¼Œè½¬è½½è¯·æ³¨æ˜å‡ºå¤„ã€‚</p>
<p>https://caterpillarstudygroup.github.io/ImportantArticles/</p>
</blockquote>
<div style="break-before: page; page-break-before: always;"></div><h1 id="human-pose-estimation"><a class="header" href="#human-pose-estimation">Human Pose Estimation</a></h1>
<blockquote>
<p>è¾“å‡ºå…³èŠ‚ä½ç½®ã€æ—‹è½¬ã€è¿æ¥å…³ç³»</p>
</blockquote>
<h2 id="å•äººhpe"><a class="header" href="#å•äººhpe">å•äººHPE</a></h2>
<h3 id="å›¾åƒå•äººhpe"><a class="header" href="#å›¾åƒå•äººhpe">å›¾åƒå•äººHPE</a></h3>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>31</td><td></td><td>SMPLer: Taming Transformers for Monocular 3D Human Shape and Pose Estimation</td><td>åŸºäº SMPL çš„ Transformer æ¡†æ¶çš„HMR</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/31.html">link</a></td></tr>
</tbody></table>
<h4 id="solving-depth-ambiguity"><a class="header" href="#solving-depth-ambiguity">Solving Depth Ambiguity</a></h4>
<h4 id="solving-body-structure-understanding"><a class="header" href="#solving-body-structure-understanding">Solving Body Structure Understanding</a></h4>
<h4 id="solving-occlusion-problems"><a class="header" href="#solving-occlusion-problems">Solving Occlusion Problems</a></h4>
<h4 id="solving-data-lacking"><a class="header" href="#solving-data-lacking">Solving Data Lacking</a></h4>
<h3 id="è§†é¢‘å•äººhpe"><a class="header" href="#è§†é¢‘å•äººhpe">è§†é¢‘å•äººHPE</a></h3>
<h4 id="solving-single-frame-limitation"><a class="header" href="#solving-single-frame-limitation">Solving Single-frame Limitation</a></h4>
<h4 id="solving-real-time-problems"><a class="header" href="#solving-real-time-problems">Solving Real-time Problems</a></h4>
<h4 id="solving-body-structure-understanding-1"><a class="header" href="#solving-body-structure-understanding-1">Solving Body Structure Understanding</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>26</td><td></td><td>PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</td><td>åˆ©ç”¨ç‰©ç†åˆç†åŒ–äººç‰©åŠ¨ä½œ</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/26.html">link</a></td></tr>
</tbody></table>
<h4 id="solving-occlusion-problems-1"><a class="header" href="#solving-occlusion-problems-1">Solving Occlusion Problems</a></h4>
<h4 id="solving-data-lacking-1"><a class="header" href="#solving-data-lacking-1">Solving Data Lacking</a></h4>
<h2 id="å¤šäººhpe"><a class="header" href="#å¤šäººhpe">å¤šäººHPE</a></h2>
<h1 id="human-mesh-recovery"><a class="header" href="#human-mesh-recovery">Human Mesh Recovery</a></h1>
<h2 id="template-based-human-mesh-recovery"><a class="header" href="#template-based-human-mesh-recovery">Template-based human mesh recovery</a></h2>
<h3 id="naked-human-body-recovery"><a class="header" href="#naked-human-body-recovery">Naked human body recovery</a></h3>
<h4 id="multimodal-methods"><a class="header" href="#multimodal-methods"><strong>Multimodal Methods</strong></a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>[123]</td><td>2019</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[124]</td><td>2022</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[125]</td><td>2022</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[126]</td><td>2022</td><td></td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>WHAM: Reconstructing World-grounded Humans with Accurate 3D Motion</td><td></td><td>å•äººï¼Œç§»åŠ¨ç›¸æœº</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/11.html">link</a></td></tr>
<tr><td></td><td>2024</td><td>Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment</td><td></td><td>2D to 3D lifting</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/27.html">link</a></td></tr>
<tr><td>Moritz Einfalt, Katja Ludwig, and Rainer Lienhart. Uplift and upsample: Efficient 3d human pose estimation with uplifting transformers. In IEEE Winter Conf. Appl. Comput. Vis., pages 2903â€“2913, 2023.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Wenhao Li, Hong Liu, Runwei Ding, Mengyuan Liu, Pichao Wang, and Wenming Yang. Exploiting temporal contexts with strided transformer for 3d human pose estimation. IEEE Trans. Multimedia, 25:1282â€“1293, 2022a.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Shanshe Wang, Siwei Ma, and Wen Gao. P-stmo: Pre-trained spatial temporal many-to-one model for 3d human pose estimation. In Eur. Conf. Comput. Vis., pages 461â€“478. Springer, 2022.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Jinlu Zhang, Zhigang Tu, Jianyu Yang, Yujin Chen, and Junsong Yuan. Mixste: Seq2seq mixed spatio-temporal encoder for 3d human pose estimation in video. In IEEE Conf. Comput. Vis. Pattern Recog., pages 13232â€“ 13242, 2022.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Zhenhua Tang, Zhaofan Qiu, Yanbin Hao, Richang Hong, and Ting Yao. 3d human pose estimation with spatio-temporal criss-cross attention. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4790â€“4799, 2023.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>Qitao Zhao, Ce Zheng, Mengyuan Liu, Pichao Wang, and Chen Chen. Poseformerv2: Exploring frequency domain for efficient and robust 3d human pose estimation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8877â€“8886, 2023.</td><td></td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<h4 id="utilizing-attention-mechanism"><a class="header" href="#utilizing-attention-mechanism">Utilizing Attention Mechanism</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2023</td><td>Humans in 4D: Reconstructing and Tracking Humans with Transformers</td><td></td><td>å›¾åƒï¼Œå¼€æº</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/28.html">link</a></td></tr>
</tbody></table>
<h4 id="exploiting-temporal-information"><a class="header" href="#exploiting-temporal-information"><strong>Exploiting Temporal Information</strong></a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>[134]</td><td>2019</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[135]</td><td>2021</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[136]</td><td>2021</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[137]</td><td>2021</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[138]</td><td>2022</td><td></td><td></td><td></td><td></td></tr>
<tr><td>[139]</td><td>2023</td><td>Global-to-local modeling for video-based 3d human pose and shape estimation</td><td>To effec-tively balance the learning of short-term and long-term temporal correlations, Global-to-Local Transformer (GLoT) [139] structurally decouples the modeling of long-term and short-term correlations.</td><td>è§†é¢‘ï¼Œå•äººï¼ŒSMPLï¼Œéæµå¼ï¼Œtransformer</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/12.html">link</a></td></tr>
<tr><td></td><td>2024</td><td>TRAM: Global Trajectory and Motion of 3D Humans from in-the-wild Video</td><td>ä»…å›¾åƒç‰¹å¾æ¢å¤3DåŠ¨ä½œ</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/18.html">link</a></td></tr>
</tbody></table>
<h4 id="multi-view-methods"><a class="header" href="#multi-view-methods">Multi-view Methods.</a></h4>
<h4 id="boosting-efficiency"><a class="header" href="#boosting-efficiency">Boosting Efficiency</a></h4>
<h4 id="developing-various-representations"><a class="header" href="#developing-various-representations">Developing Various Representations</a></h4>
<h4 id="utilizing-structural-information"><a class="header" href="#utilizing-structural-information">Utilizing Structural Information</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2024</td><td>PhysPT: Physics-aware Pretrained Transformer for Estimating Human Dynamics from Monocular Videos</td><td></td><td></td><td></td></tr>
</tbody></table>
<h4 id="choosing-appropriate-learning-strategies"><a class="header" href="#choosing-appropriate-learning-strategies">Choosing Appropriate Learning Strategies</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>161</td><td>2019</td><td></td><td></td><td></td><td></td></tr>
<tr><td>44</td><td>2020</td><td></td><td></td><td></td><td></td></tr>
<tr><td>163</td><td>2020</td><td>Coherent reconstruction of multiple humans from a single image</td><td></td><td>å›¾åƒï¼Œå¤šäºº</td><td></td></tr>
<tr><td>164</td><td>2021</td><td></td><td></td><td></td><td></td></tr>
<tr><td>46</td><td>2021</td><td></td><td></td><td></td><td></td></tr>
<tr><td>214</td><td>2021</td><td></td><td></td><td></td><td></td></tr>
<tr><td>165</td><td>2022</td><td></td><td></td><td></td><td></td></tr>
<tr><td>166</td><td>2022</td><td></td><td></td><td></td><td></td></tr>
<tr><td>167</td><td>2023</td><td>Jotr: 3d joint con-trastive learning with transformers for occluded human mesh recovery</td><td>èåˆ 2D å’Œ 3D ç‰¹å¾ï¼Œå¹¶é€šè¿‡åŸºäº Transformer çš„å¯¹æ¯”å­¦ä¹ æ¡†æ¶ç»“åˆå¯¹ 3D ç‰¹å¾çš„ç›‘ç£</td><td></td><td></td></tr>
<tr><td>162</td><td>2023</td><td>Refit: Recurrent fitting network for 3d human recovery</td><td>é€šè¿‡åé¦ˆ-æ›´æ–°å¾ªç¯æœºåˆ¶é‡æ–°æŠ•å½±å…³é”®ç‚¹å¹¶å®Œå–„äººä½“æ¨¡å‹</td><td></td><td></td></tr>
<tr><td>4</td><td>2023</td><td>Co-evolution of pose and mesh for 3d human body estimation from video</td><td>å¼•å…¥äº†ä¸€ç§åˆ©ç”¨ 3D å§¿åŠ¿ä½œä¸ºä¸­ä»‹çš„äººä½“meshæ¢å¤çš„å…±åŒè¿›åŒ–æ–¹æ³•ã€‚è¯¥æ–¹æ³•å°†è¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªä¸åŒçš„é˜¶æ®µï¼šé¦–å…ˆï¼Œå®ƒä»è§†é¢‘ä¸­ä¼°è®¡ 3D äººä½“å§¿åŠ¿ï¼Œéšåï¼Œæ ¹æ®ä¼°è®¡çš„ 3D å§¿åŠ¿å¹¶ç»“åˆæ—¶é—´å›¾åƒç‰¹å¾å¯¹meshé¡¶ç‚¹è¿›è¡Œå›å½’</td><td>å¼€æºã€å•äººã€è§†é¢‘ã€mesh</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/13.html">link</a></td></tr>
<tr><td>168</td><td>2023</td><td>Cyclic test-time adaptation on monocular video for 3d human mesh reconstruction</td><td>ä¸ºäº†å¼¥åˆè®­ç»ƒå’Œæµ‹è¯•æ•°æ®ä¹‹é—´çš„å·®è·ï¼ŒCycleAdapt [168]æå‡ºäº†ä¸€ç§åŸŸè‡ªé€‚åº”æ–¹æ³•ï¼ŒåŒ…æ‹¬meshé‡å»ºç½‘ç»œå’Œè¿åŠ¨é™å™ªç½‘ç»œï¼Œèƒ½å¤Ÿå®ç°æ›´æœ‰æ•ˆçš„è‡ªé€‚åº”ã€‚</td><td></td><td></td></tr>
</tbody></table>
<h3 id="detailed-human-body-recovery"><a class="header" href="#detailed-human-body-recovery">Detailed human body recovery</a></h3>
<h4 id="with-clothes"><a class="header" href="#with-clothes">With Clothes</a></h4>
<h4 id="with-hands"><a class="header" href="#with-hands">With Hands</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>173</td><td>2023</td><td>SGNify, a model that captures hand pose, facial expression, and body movement from sign language videos. It employs linguistic priors and constraints on 3D hand pose to effectively address the ambiguities in isolated signs.</td><td></td><td></td><td></td></tr>
<tr><td>174</td><td>2021</td><td>the relationship between Two- Hands</td><td></td><td></td><td></td></tr>
<tr><td>175</td><td>2021</td><td>the relationship between Hand-Object</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>HMP: Hand Motion Priors for Pose and Shape Estimation from Video</td><td>å…ˆç”¨æ— è§†é¢‘ä¿¡æ¯çš„æ‰‹åŠ¿æ•°æ®åšæ‰‹åŠ¿åŠ¨ä½œå…ˆéªŒã€‚åŸºäºå…ˆéªŒå†åšæ‰‹åŠ¿è¯†åˆ«</td><td>æ‰‹ã€å¼€æº</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/15.html">link</a></td></tr>
</tbody></table>
<h4 id="whole-body"><a class="header" href="#whole-body">Whole Body</a></h4>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>176</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>177</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>178</td><td>2021</td><td>independently running 3D mesh recovery regression for face, hands, and body and subsequently combining the outputs through an integration module</td><td></td><td></td><td></td></tr>
<tr><td>179</td><td>2021</td><td>integrates independent es- timates from the body, face, and hands using the shared shape space of SMPL-X across all body parts</td><td></td><td></td><td></td></tr>
<tr><td>180</td><td>2022</td><td>Accurate 3d hand pose estimation for whole-body 3d human mesh estimation</td><td>end-to-end framework for whole-body human mesh recovery named Hand4Whole, which employs joint features for 3D joint rotations to enhance the accuracy of 3D hand predictions</td><td></td><td></td></tr>
<tr><td>181</td><td>2023</td><td>Pymaf-x: Towards well-aligned full-body model regression from monocular images</td><td>to resolve the misalignment issues in regression-based, one-stage human mesh recovery methods by employing a feature pyramid approach and refining the mesh-image alignment parameters.</td><td></td><td></td></tr>
<tr><td>215</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>182</td><td>2023</td><td>One-stage 3d whole-body mesh recovery with component aware transformer</td><td>a simple yet effective component-aware transformer that includes a global body encoder and a lo- cal face/hand decoder instead of separate networks for each part</td><td></td><td></td></tr>
<tr><td>183</td><td></td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
<h2 id="template-free-human-body-recovery"><a class="header" href="#template-free-human-body-recovery">Template-free human body recovery</a></h2>
<h1 id="è¿åŠ¨ç›¸æœºåœºæ™¯"><a class="header" href="#è¿åŠ¨ç›¸æœºåœºæ™¯">è¿åŠ¨ç›¸æœºåœºæ™¯</a></h1>
<h2 id="æå–ç›¸æœºè½¨è¿¹"><a class="header" href="#æå–ç›¸æœºè½¨è¿¹">æå–ç›¸æœºè½¨è¿¹</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2022</td><td>BodySLAM: Joint Camera Localisation, Mapping, and Human Motion Tracking</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>Decoupling Human and Camera Motion from Videos in the Wild</td><td>è”åˆä¼˜åŒ–äººä½“å§¿åŠ¿å’Œç›¸æœºscaleï¼Œä½¿äººä½“ä½ç§»ä¸å­¦ä¹ çš„è¿åŠ¨æ¨¡å‹ç›¸åŒ¹é…</td><td>å¤šäºº</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/16.html">link</a></td></tr>
</tbody></table>
<h1 id="evaluation"><a class="header" href="#evaluation">Evaluation</a></h1>
<h2 id="evaluation-metrics"><a class="header" href="#evaluation-metrics">Evaluation metrics</a></h2>
<h3 id="for-pose-and-shape-reconstruction"><a class="header" href="#for-pose-and-shape-reconstruction">For pose and shape reconstruction</a></h3>
<p>mean per-joint error (MPJPE),   Procrustes-aligned perjoint error (PA-MPJPE),<br />
per-vertex error (PVE)</p>
<h3 id="to-evaluate-the-motion-smoothness"><a class="header" href="#to-evaluate-the-motion-smoothness">To evaluate the motion smoothness</a></h3>
<p>acceleration error (ACCEL) against the ground truth acceleration</p>
<h3 id="for-human-trajectory-evaluation"><a class="header" href="#for-human-trajectory-evaluation">For human trajectory evaluation,</a></h3>
<p>we slice a sequence into 100-frame segments and evaluate 3D joint error after aligning the first two frames (W-MPJPE100) or the entire segment (WA-MPJPE100) [93].<br />
evaluate the error of the entire trajectory after aligning the first frame, with root translation error (RTE), root orientation error (ROE), and egocentric root velocity error (ERVE).</p>
<h3 id="for-camera-trajectory-evaluation"><a class="header" href="#for-camera-trajectory-evaluation">For camera trajectory evaluation</a></h3>
<p>absolute trajectory error (ATE) [75], which performs Procrustes with scaling to align the estimation with ground truth before computing error.</p>
<h3 id="to-evaluate-the-accuracy-of-our-scale-estimation"><a class="header" href="#to-evaluate-the-accuracy-of-our-scale-estimation">To evaluate the accuracy of our scale estimation</a></h3>
<p>evaluate ATE using our estimated scale (ATE-S) [35].</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="./assets/7b6ce0e0b104d36e8331f86fd4c9ab5a_1_Figure_2_1330436394.png" alt="" /></p>
<h1 id="introduction"><a class="header" href="#introduction">INTRODUCTION</a></h1>
<p>éœ€æ±‚ï¼šåŸºäºå›¾åƒçš„3Dåœºæ™¯é‡å»º</p>
<p>å‘å±•å²ï¼š</p>
<table><thead><tr><th></th><th></th><th></th></tr></thead><tbody>
<tr><td>å…‰åœºå’ŒåŸºæœ¬åœºæ™¯å’Œé‡å»º</td><td>1-3</td><td>å—åˆ°å¯¹å¯†é›†é‡‡æ ·å’Œç»“æ„åŒ–æ•æ‰çš„ä¾èµ–çš„é™åˆ¶ï¼Œå¯¼è‡´åœ¨å¤„ç†å¤æ‚åœºæ™¯å’Œç…§æ˜æ¡ä»¶æ–¹é¢é¢ä¸´é‡å¤§æŒ‘æˆ˜</td></tr>
<tr><td>structure-frommotion [4]ï¼Œ multi-view stereo [5] algorithms</td><td>4ï¼Œ 5</td><td>éš¾ä»¥è¿›è¡Œæ–°è§†è§’åˆæˆï¼Œå¹¶ä¸”ç¼ºä¹ä¸æ·±åº¦åœºæ™¯ç†è§£æ¨¡å‹çš„å…¼å®¹æ€§</td></tr>
<tr><td>NeRFï¼šå®ç°ç©ºé—´åæ ‡åˆ°é¢œè‰²å’Œå¯†åº¦çš„ç›´æ¥æ˜ å°„</td><td>6-11ï¼Œ</td><td>NeRF çš„æˆåŠŸå–å†³äºå…¶åˆ›å»ºè¿ç»­çš„ä½“ç§¯åœºæ™¯å‡½æ•°çš„èƒ½åŠ›ï¼Œäº§ç”Ÿå…·æœ‰å‰æ‰€æœªæœ‰çš„ç»†èŠ‚å’ŒçœŸå®æ„Ÿçš„ç»“æœã€‚<br>1. è®¡ç®—å¼ºåº¦ã€‚åŸºäº NeRF çš„æ–¹æ³•æ˜¯è®¡ç®—å¯†é›†å‹çš„ [6]-[11]ï¼Œé€šå¸¸éœ€è¦å¤§é‡çš„è®­ç»ƒæ—¶é—´å’Œå¤§é‡çš„æ¸²æŸ“èµ„æºï¼Œç‰¹åˆ«æ˜¯å¯¹äºé«˜åˆ†è¾¨ç‡è¾“å‡ºã€‚ <br>2. å¯ç¼–è¾‘æ€§ã€‚æ“çºµéšå¼è¡¨ç¤ºçš„åœºæ™¯å¯èƒ½å…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå› ä¸ºå¯¹ç¥ç»ç½‘ç»œæƒé‡çš„ç›´æ¥ä¿®æ”¹ä¸åœºæ™¯çš„å‡ ä½•æˆ–å¤–è§‚å±æ€§çš„å˜åŒ–å¹¶ä¸ç›´è§‚ç›¸å…³ã€‚</td></tr>
<tr><td>3D Gaussian splatting (GS) [12]</td><td>12</td><td>å¼•å…¥å…ˆè¿›çš„ã€æ˜ç¡®çš„åœºæ™¯è¡¨ç¤ºï¼Œä½¿ç”¨ç©ºé—´ä¸­æ•°ç™¾ä¸‡ä¸ªå¯å­¦ä¹ çš„ 3D é«˜æ–¯æ¨¡å‹å¯¹åœºæ™¯è¿›è¡Œå»ºæ¨¡ã€‚<br>é‡‡ç”¨æ˜¾å¼è¡¨ç¤ºå’Œé«˜åº¦å¹¶è¡ŒåŒ–çš„å·¥ä½œæµç¨‹ï¼Œä¿ƒè¿›æ›´é«˜æ•ˆçš„è®¡ç®—å’Œæ¸²æŸ“</td></tr>
</tbody></table>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>12</td><td>2023</td><td>3D Gaussian Splatting for Real-Time Radiance Field Rendering</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/17.html">link</a></td></tr>
</tbody></table>
<h1 id="background"><a class="header" href="#background">BACKGROUND</a></h1>
<h2 id="problem-formulation"><a class="header" href="#problem-formulation">Problem Formulation</a></h2>
<h3 id="radiance-field"><a class="header" href="#radiance-field">Radiance Field</a></h3>
<p><a href="https://caterpillarstudygroup.github.io/GAMES101_mdbook/Color/LightField.html">GAMES101è¯¾ç¨‹å…³äºå…‰åœºçš„ä»‹ç»</a></p>
<p>å…‰åœºæ˜¯ä¸‰ç»´ç©ºé—´ä¸­å…‰åˆ†å¸ƒçš„è¡¨ç¤ºï¼Œå®ƒæ•è·å…‰å¦‚ä½•ä¸ç¯å¢ƒä¸­çš„è¡¨é¢å’Œææ–™ç›¸äº’ä½œç”¨[30]ã€‚åœ¨æ•°å­¦ä¸Šï¼Œå…‰åœºå¯ä»¥æè¿°ä¸ºå‡½æ•°</p>
<p>$$
L : (x, y, z, Î¸, Ï†) \in R^5 â†’ R^+
$$</p>
<p>å…¶ä¸­(x, y, z)ä¸ºæ˜ å°„ç©ºé—´ä¸­çš„ä¸€ä¸ªç‚¹ï¼Œ(Î¸, Ï†)ä¸ºçƒåæ ‡æŒ‡å®šçš„æ–¹å‘ã€‚<br />
radiance valueä¸ºéå€¼ã€‚<br />
Lå¯ä»¥é€šè¿‡éšå¼æˆ–æ˜¾å¼è¡¨ç¤ºæ¥å°è£…ï¼Œæ¯ç§è¡¨ç¤ºå¯¹äºåœºæ™¯è¡¨ç¤ºå’Œæ¸²æŸ“éƒ½æœ‰ç‰¹å®šçš„ä¼˜åŠ¿ã€‚</p>
<h3 id="implicit-radiance-field"><a class="header" href="#implicit-radiance-field">Implicit Radiance Field</a></h3>
<p>éšå¼è¾å°„åœºè¡¨ç¤ºåœºæ™¯ä¸­çš„å…‰åˆ†å¸ƒï¼Œè€Œæ— éœ€æ˜¾å¼å®šä¹‰åœºæ™¯çš„å‡ ä½•å½¢çŠ¶ã€‚</p>
<p>Lç»å¸¸ä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å­¦ä¹ è¿ç»­çš„ä½“ç§¯åœºæ™¯è¡¨ç¤º[35]ï¼Œ[36]ã€‚æœ€çªå‡ºçš„ä¾‹å­æ˜¯ NeRF [15]ã€‚</p>
<p>åœ¨ NeRF ä¸­ï¼Œç¥ç»ç½‘ç»œï¼ˆé€šå¸¸æ˜¯MLPï¼‰å°†ä¸€ç»„ç©ºé—´åæ ‡ (x, y, z) å’Œè§‚å¯Ÿæ–¹å‘ (Î¸, Ï†) æ˜ å°„åˆ°é¢œè‰²å’Œå¯†åº¦å€¼ã€‚ä»»ä½•ç‚¹çš„è¾å°„åº¦éƒ½ä¸ä¼šæ˜¾å¼å­˜å‚¨ï¼Œè€Œæ˜¯é€šè¿‡æŸ¥è¯¢ MLP å³æ—¶è®¡ç®—ã€‚å› æ­¤ï¼Œè¯¥å‡½æ•°å¯ä»¥å†™ä¸ºï¼š</p>
<p>$$
L = MLP(x, y, z, Î¸, Ï†)
$$</p>
<p>Goodï¼šè¿™ç§æ ¼å¼å…è®¸å¯¹å¤æ‚åœºæ™¯è¿›è¡Œå¯å¾®åˆ†å’Œç´§å‡‘çš„è¡¨ç¤º<br />
Badï¼šç„¶è€Œç”±äºvolumetric ray marching [12]ï¼Œæ¸²æŸ“çš„è®¡ç®—è´Ÿè½½é«˜ã€‚</p>
<h3 id="explicit-radiance-field"><a class="header" href="#explicit-radiance-field">Explicit Radiance Field</a></h3>
<p>æ˜¾å¼è¾å°„åœºç›´æ¥è¡¨ç¤ºç¦»æ•£ç©ºé—´ç»“æ„ä¸­çš„å…‰åˆ†å¸ƒï¼Œä¾‹å¦‚ä½“ç´ ã€ç½‘æ ¼ã€ç‚¹äº‘ã€‚<strong>è¯¥ç»“æ„ä¸­çš„æ¯ä¸ªå…ƒç´ å­˜å‚¨å…¶å„è‡ªç©ºé—´ä½ç½®çš„è¾å°„ä¿¡æ¯</strong>ã€‚</p>
<p>æ˜¾å¼è¾å°„åœºè¡¨ç¤ºçš„é€šç”¨å½¢å¼å¯ä»¥å†™ä¸ºï¼š</p>
<p>$$
L = \text {DataStructure}[(x,y,z)] \cdot f(Î¸, Ï†)
$$</p>
<p>Goodï¼šè¿™ç§æ–¹æ³•å…è®¸æ›´ç›´æ¥ä¸”é€šå¸¸æ›´å¿«åœ°è®¿é—®radiance valueã€‚<br />
Badï¼šä»£ä»·æ˜¯æ›´é«˜çš„å†…å­˜ä½¿ç”¨é‡å’Œå¯èƒ½æ›´ä½çš„åˆ†è¾¨ç‡ã€‚</p>
<h3 id="3d-gaussian-splatting-ä¸¤å…¨å…¶ç¾"><a class="header" href="#3d-gaussian-splatting-ä¸¤å…¨å…¶ç¾">3D Gaussian Splatting: ä¸¤å…¨å…¶ç¾</a></h3>
<p>3D GS [12]æ˜¯æ˜¾å¼è¾å°„åœºï¼Œåˆå…·æœ‰éšå¼è¾å°„åœºçš„ä¼˜ç‚¹ã€‚å› ä¸ºå®ƒç»“åˆäº†åŸºäºç¥ç»ç½‘ç»œçš„ä¼˜åŒ–å’Œæ˜¾å¼ç»“æ„åŒ–æ•°æ®å­˜å‚¨çš„ä¼˜ç‚¹ã€‚å› æ­¤å¯ä»¥å®æ—¶ã€é«˜è´¨é‡æ¸²æŸ“ï¼Œå¹¶ä¸”éœ€è¦æ›´å°‘çš„è®­ç»ƒæ—¶é—´ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤æ‚åœºæ™¯å’Œé«˜åˆ†è¾¨ç‡è¾“å‡ºã€‚ 3D é«˜æ–¯è¡¨ç¤ºå½¢å¼ä¸ºï¼š</p>
<p>$$
L = \sum_i G(x,y,z,\mu_i, \sigma_i)\cdot c_i(Î¸, Ï†)
$$</p>
<blockquote>
<p>æ˜¾å¼çš„æ–¹æ³•ï¼Œåªèƒ½æŠŠradianceç»‘å®šåœ¨ç‚¹ä¸Šï¼Œå› æ­¤å—é™äºç‚¹çš„åˆ†è¾¨ç‡ï¼Œè€Œç‚¹çš„åˆ†è¾¨ç‡åˆå—é™äºå†…å­˜ã€‚<br />
3D GSæŠŠradianceç»‘å®šåœ¨æœ‰ä½“ç§¯çš„ç‚¹ï¼ˆçƒï¼‰ä¸Šï¼Œæ‰€ä»¥å¯¹ç‚¹çš„åˆ†è¾¨ç‡è¦æ±‚ä½ä¸€ç‚¹ã€‚çƒçš„ä½œç”¨æœ‰ç‚¹åƒç‚¹ä¹‹é—´çš„æ’å€¼ã€‚</p>
</blockquote>
<h2 id="èƒŒæ™¯å’Œæœ¯è¯­"><a class="header" href="#èƒŒæ™¯å’Œæœ¯è¯­">èƒŒæ™¯å’Œæœ¯è¯­</a></h2>
<h3 id="åœºæ™¯é‡å»ºä¸æ¸²æŸ“"><a class="header" href="#åœºæ™¯é‡å»ºä¸æ¸²æŸ“">åœºæ™¯é‡å»ºä¸æ¸²æŸ“</a></h3>
<p>3Dé‡å»ºï¼šå›¾åƒ-&gt;3Dæ¨¡å‹<br />
æ¸²æŸ“ï¼š3Dæ¨¡å‹-&gt;å›¾åƒ</p>
<h3 id="ç¥ç»æ¸²æŸ“å’Œè¾å°„åœº"><a class="header" href="#ç¥ç»æ¸²æŸ“å’Œè¾å°„åœº">ç¥ç»æ¸²æŸ“å’Œè¾å°„åœº</a></h3>
<h3 id="ä½“ç§¯è¡¨ç¤ºå’Œray-marching"><a class="header" href="#ä½“ç§¯è¡¨ç¤ºå’Œray-marching">ä½“ç§¯è¡¨ç¤ºå’Œray marching</a></h3>
<p>ä½“ç§¯è¡¨ç¤ºä¸ä»…å°†å¯¹è±¡å’Œåœºæ™¯å»ºæ¨¡ä¸ºè¡¨é¢ï¼Œè€Œä¸”å°†å…¶å»ºæ¨¡ä¸ºå……æ»¡ææ–™æˆ–ç©ºç™½ç©ºé—´çš„ä½“ç§¯[46]ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ›´å‡†ç¡®åœ°æ¸²æŸ“é›¾ã€çƒŸæˆ–åŠé€æ˜ææ–™ç­‰ç°è±¡ã€‚<br />
å…‰çº¿è¡Œè¿›æ˜¯ä¸€ç§ä¸ä½“ç§¯è¡¨ç¤ºä¸€èµ·ä½¿ç”¨çš„æŠ€æœ¯ï¼Œé€šè¿‡å¢é‡è·Ÿè¸ªç©¿è¿‡ä½“ç§¯çš„å…‰è·¯æ¥æ¸²æŸ“å›¾åƒ[13]ã€[14]ã€‚ </p>
<p><a href="https://caterpillarstudygroup.github.io/GAMES101_mdbook/AdvancedRendering/AdcancedAppearcing.html">éè¡¨é¢æ¨¡å‹çš„æ¸²æŸ“</a></p>
<p>NeRF [15] ä¸ä½“ç§¯å°„çº¿è¡Œè¿›æœ‰ç€ç›¸åŒçš„ç²¾ç¥ï¼Œå¹¶å¼•å…¥äº†é‡è¦æ€§é‡‡æ ·å’Œä½ç½®ç¼–ç æ¥æé«˜åˆæˆå›¾åƒçš„è´¨é‡ã€‚å› æ­¤é«˜è´¨é‡çš„ç»“æœï¼Œé«˜è®¡ç®—æˆæœ¬ã€‚</p>
<h3 id="åŸºäºç‚¹çš„æ¸²æŸ“"><a class="header" href="#åŸºäºç‚¹çš„æ¸²æŸ“">åŸºäºç‚¹çš„æ¸²æŸ“</a></h3>
<p>åŸºäºç‚¹çš„æ¸²æŸ“æ˜¯ä¸€ç§ä½¿ç”¨ç‚¹è€Œä¸æ˜¯ä¼ ç»Ÿå¤šè¾¹å½¢æ¥å¯è§†åŒ– 3D åœºæ™¯çš„æŠ€æœ¯ã€‚å¯ä»¥é€šè¿‡å¯å­¦ä¹ ç¥ç»ç½‘ç»œç­‰é™„åŠ å±æ€§æ¥å¢å¼ºç‚¹æè¿°ç¬¦[47]ã€[48]ï¼Œå¹¶æœ‰æ•ˆæ¸²æŸ“[49]ã€[50]ã€‚</p>
<p>Goodï¼šå¯¹äºæ¸²æŸ“å¤æ‚ã€éç»“æ„åŒ–æˆ–ç¨€ç–çš„å‡ ä½•æ•°æ®ç‰¹åˆ«æœ‰æ•ˆã€‚
Badï¼šå­˜åœ¨æ¸²æŸ“æ¼æ´æˆ–é”¯é½¿æ•ˆåº”ç­‰é—®é¢˜ã€‚ 3D GS [12] é€šè¿‡ä½¿ç”¨å„å‘å¼‚æ€§é«˜æ–¯å‡½æ•°æ‰©å±•äº†è¿™ä¸€æ¦‚å¿µï¼Œä»¥å®ç°æ›´è¿ç»­ã€æ›´æœ‰å‡èšåŠ›çš„åœºæ™¯è¡¨ç¤ºã€‚</p>
<h1 id="3d-gaussian-splatting-principles"><a class="header" href="#3d-gaussian-splatting-principles">3D GAUSSIAN SPLATTING: PRINCIPLES</a></h1>
<h2 id="ä½¿ç”¨å­¦ä¹ çš„-3d-é«˜æ–¯å‡½æ•°è¿›è¡Œæ–°é¢–è§†å›¾åˆæˆ"><a class="header" href="#ä½¿ç”¨å­¦ä¹ çš„-3d-é«˜æ–¯å‡½æ•°è¿›è¡Œæ–°é¢–è§†å›¾åˆæˆ">ä½¿ç”¨å­¦ä¹ çš„ 3D é«˜æ–¯å‡½æ•°è¿›è¡Œæ–°é¢–è§†å›¾åˆæˆ</a></h2>
<p>3D GS å¦‚ä½•åœ¨ç»™å®šç»“æ„è‰¯å¥½çš„ 3D é«˜æ–¯çš„æƒ…å†µä¸‹åˆæˆå›¾åƒï¼Œå³3D GSçš„å‰å‘è¿‡ç¨‹ã€‚</p>
<p>$$
L = \sum_i G(x,y,z,\mu_i, \Sigma_i)\cdot c_i(Î¸, Ï†)
$$</p>
<h3 id="è¾“å…¥"><a class="header" href="#è¾“å…¥">è¾“å…¥</a></h3>
<p>ä¸€ç»„é«˜æ–¯çƒï¼Œæ¯ä¸ªé«˜æ–¯çƒåŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š</p>
<ul>
<li>ä½ç½®ï¼š\(\mu\)</li>
<li>ä¸é€æ˜åº¦ï¼š \(\alpha\)</li>
<li>åæ–¹å·®ï¼š\(\Sigma\)</li>
<li>é¢œè‰²ï¼šc</li>
</ul>
<p>ä¸€ä¸ªé«˜æ–¯çƒæ˜¯ 3D GS ä¸­åœºæ™¯è¡¨ç¤ºçš„æœ€å°å…ƒç´ ã€‚</p>
<p>æ‰€æœ‰å±æ€§éƒ½å¯ä»¥é€šè¿‡åå‘ä¼ æ’­æ¥å­¦ä¹ å’Œä¼˜åŒ–ã€‚ç°åœ¨å‡è®¾è¿™äº›é«˜æ–¯çƒéƒ½å·²ç»ä¼˜åŒ–å¥½äº†ã€‚</p>
<p><img src="./assets/7b6ce0e0b104d36e8331f86fd4c9ab5a_3_Figure_3_-1437298192.png" alt="" /></p>
<h3 id="splatting"><a class="header" href="#splatting">Splatting</a></h3>
<p>é¦–å…ˆå°†è¿™äº› 3D é«˜æ–¯æŠ•å½±åˆ°åŸºäºåƒç´ çš„å›¾åƒå¹³é¢ä¸Šï¼Œè¿™ä¸€è¿‡ç¨‹ç§°ä¸ºâ€œsplattingâ€ã€‚</p>
<p><img src="./assets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202024-05-05%2022.42.06.png" alt="" /></p>
<p><a href="https://caterpillarstudygroup.github.io/GAMES101_mdbook/MVP/PerspectiveProjection.html">Frustum</a></p>
<p>ç›¸æœºposeç¡®å®šä»¥åï¼Œæ ¹æ®frustumåˆ‡å‡ºèƒ½çœ‹è§çš„é«˜æ–¯çƒã€‚è®¡ç®—å‡ºç›¸æœºè§†è§’ä¸‹çš„é«˜æ–¯çƒçš„åæ–¹å·®ã€‚</p>
<h3 id="å¯å¾®åˆ†æ¸²æŸ“-by-pixels"><a class="header" href="#å¯å¾®åˆ†æ¸²æŸ“-by-pixels">å¯å¾®åˆ†æ¸²æŸ“ by pixels</a></h3>
<blockquote>
<p>æ­¤å¤„å…ˆåªä»‹ç»åŸºæœ¬è¿‡ç¨‹ï¼Œä¸è®²åŠ é€Ÿç®—æ³•</p>
</blockquote>
<p>ç»™å®šåƒç´  x çš„ä½ç½®ï¼Œå¯ä»¥é€šè¿‡æŠ•å½±å˜æ¢ W æ¥è®¡ç®—å…¶åˆ°æ‰€æœ‰é‡å é«˜æ–¯å‡½æ•°çš„è·ç¦»ï¼Œå³è¿™äº›é«˜æ–¯å‡½æ•°çš„æ·±åº¦ï¼Œå½¢æˆé«˜æ–¯å‡½æ•° N çš„æ’åºåˆ—è¡¨ã€‚</p>
<p>ç„¶åï¼Œé‡‡ç”¨alphaåˆæˆæ¥è®¡ç®—è¯¥åƒç´ çš„æœ€ç»ˆé¢œè‰²ï¼š</p>
<p><img src="./assets/7b6ce0e0b104d36e8331f86fd4c9ab5a_3_Figure_4_1136390164.png" alt="" /><br />
å¦‚å›¾æ‰€ç¤ºï¼ŒNeRFå’Œ3D GSçš„æ¸²æŸ“å¯ä»¥çœ‹ä½œæ˜¯å½¼æ­¤çš„é€†è¿‡ç¨‹ã€‚</p>
<h3 id="åŠ é€ŸæŠ€æœ¯"><a class="header" href="#åŠ é€ŸæŠ€æœ¯">åŠ é€ŸæŠ€æœ¯</a></h3>
<p>åƒç´ çº§è®¡ç®—çš„æˆæœ¬æ¯”è¾ƒé«˜ï¼Œå› æ­¤å°†ç²¾åº¦ä»åƒç´ çº§è½¬ç§»åˆ°å—çº§ã€‚</p>
<p>å…·ä½“æ¥è¯´ï¼Œ3D GS å…ˆå°†å›¾åƒåˆ’åˆ†ä¸ºå¤šä¸ªä¸é‡å çš„å›¾å—ï¼Œæ¯ä¸ªå›¾å—åŒ…å« 16Ã—16 åƒç´ ã€‚</p>
<p><img src="./assets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202024-05-07%2007.35.43.png" alt="" /></p>
<p>3D GS è¿›ä¸€æ­¥ç¡®å®šæ¯ä¸ªå›¾å—è¢«å“ªäº›æŠ•å½±é«˜æ–¯è¦†ç›–ã€‚å¦‚æœä¸€ä¸ªæŠ•å½±é«˜æ–¯è¦†ç›–å¤šä¸ªå›¾å—ï¼Œåˆ™éœ€è¦æŠŠé«˜æ–¯å¤åˆ¶å¤šä»½ã€‚</p>
<p><img src="./assets/%E5%B1%8F%E5%B9%95%E5%BF%AB%E7%85%A7%202024-05-07%2007.40.28.png" alt="" /></p>
<h2 id="3d-gsçš„ä¼˜åŒ–ä¸ºç»™å®šåœºæ™¯è·å–æ„é€ è‰¯å¥½çš„-3d-é«˜æ–¯"><a class="header" href="#3d-gsçš„ä¼˜åŒ–ä¸ºç»™å®šåœºæ™¯è·å–æ„é€ è‰¯å¥½çš„-3d-é«˜æ–¯">3D GSçš„ä¼˜åŒ–ï¼šä¸ºç»™å®šåœºæ™¯è·å–æ„é€ è‰¯å¥½çš„ 3D é«˜æ–¯</a></h2>
<p>3D GS çš„ä¼˜åŒ–ï¼Œæ—¨åœ¨æ„å»ºå¤§é‡ 3D é«˜æ–¯é›†åˆï¼Œå‡†ç¡®æ•æ‰åœºæ™¯çš„æœ¬è´¨ï¼Œä»è€Œä¿ƒè¿›è‡ªç”±è§†ç‚¹æ¸²æŸ“ã€‚</p>
<h3 id="å‚æ•°ä¼˜åŒ–"><a class="header" href="#å‚æ•°ä¼˜åŒ–">å‚æ•°ä¼˜åŒ–</a></h3>
<h4 id="loss"><a class="header" href="#loss">Loss</a></h4>
<blockquote>
<p>ç”±äºray marchingæˆæœ¬é«˜æ˜‚ï¼ŒNeRF é€šå¸¸åœ¨åƒç´ çº§åˆ«è€Œä¸æ˜¯å›¾åƒçº§åˆ«è®¡ç®—æŸå¤±ã€‚</p>
</blockquote>
<h1 id="3d-gaussian-splatting-directions"><a class="header" href="#3d-gaussian-splatting-directions">3D GAUSSIAN SPLATTING: DIRECTIONS</a></h1>
<h1 id="application-areas-and-tasks"><a class="header" href="#application-areas-and-tasks">APPLICATION AREAS AND TASKS</a></h1>
<h2 id="simultaneous-localization-and-mapping-slam"><a class="header" href="#simultaneous-localization-and-mapping-slam">Simultaneous Localization and Mapping (SLAM)</a></h2>
<h2 id="dynamic-scene-reconstruction"><a class="header" href="#dynamic-scene-reconstruction">Dynamic Scene Reconstruction</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>36</td><td>2024</td><td>GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians</td><td>è¾“å…¥ï¼šå•ä¸ªè§†é¢‘ è¾“å‡ºï¼šå…·æœ‰åŠ¨æ€ 3D å¤–è§‚çš„é€¼çœŸäººç±»å¤´åƒ ç›®çš„ï¼šå®ç°è‡ªç”±è§†è§’æ¸²æŸ“ï¼Œç”Ÿæˆé€¼çœŸäººç±»å¤´åƒåŠ¨ç”»</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/36.html">link</a></td></tr>
</tbody></table>
<h2 id="ai-generated-content-aigc"><a class="header" href="#ai-generated-content-aigc">AI-Generated Content (AIGC)</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>34</td><td>2024</td><td>Splatter a Video: Video Gaussian Representation for Versatile Processing</td><td>åˆ©ç”¨é«˜æ–¯è¿›è¡Œè§†é¢‘ç¼–è¾‘</td><td></td><td></td></tr>
</tbody></table>
<h2 id="autonomous-driving"><a class="header" href="#autonomous-driving">Autonomous Driving</a></h2>
<h2 id="endoscopic-scene-reconstruction"><a class="header" href="#endoscopic-scene-reconstruction">Endoscopic Scene Reconstruction</a></h2>
<h2 id="medical-image"><a class="header" href="#medical-image">Medical Image</a></h2>
<h1 id="reference-4"><a class="header" href="#reference-4">Reference</a></h1>
<p>A Survey on 3D Gaussian Splatting</p>
<div style="break-before: page; page-break-before: always;"></div><p><img src="./assets/d378e84bd11f484517ba2d687e8bb933_5_Table_1_-876463523.png" alt="" /></p>
<h1 id="preliminaries"><a class="header" href="#preliminaries">PRELIMINARIES</a></h1>
<h2 id="motion-data"><a class="header" href="#motion-data">Motion Data</a></h2>
<h2 id="motion-generation-methods"><a class="header" href="#motion-generation-methods">Motion Generation Methods</a></h2>
<h3 id="gan"><a class="header" href="#gan">GAN</a></h3>
<h3 id="vae"><a class="header" href="#vae">VAE</a></h3>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>158</td><td>2021</td><td>HuMoR: 3D Human Motion Model for Robust Pose Estimation</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/14.html">link</a></td></tr>
<tr><td></td><td>2024</td><td>WANDR: Intention-guided Human Motion Generation</td><td></td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/19.html">link</a></td></tr>
</tbody></table>
<h3 id="normalizing-flows"><a class="header" href="#normalizing-flows">Normalizing Flows.</a></h3>
<h3 id="diffusion-models"><a class="header" href="#diffusion-models">Diffusion Models</a></h3>
<h3 id="motion-graph"><a class="header" href="#motion-graph">Motion Graph</a></h3>
<h3 id="regression"><a class="header" href="#regression">Regression</a></h3>
<h1 id="text-conditioned-motion-generation"><a class="header" href="#text-conditioned-motion-generation">TEXT-CONDITIONED MOTION GENERATION</a></h1>
<h2 id="action-to-motion"><a class="header" href="#action-to-motion">Action to Motion</a></h2>
<h2 id="text-to-motion"><a class="header" href="#text-to-motion">Text to Motion</a></h2>
<h1 id="audio-conditioned-motion-generation"><a class="header" href="#audio-conditioned-motion-generation">AUDIO-CONDITIONED MOTION GENERATION</a></h1>
<h2 id="music-to-dance"><a class="header" href="#music-to-dance">Music to Dance</a></h2>
<h2 id="speech-to-gesture"><a class="header" href="#speech-to-gesture">Speech to Gesture</a></h2>
<h1 id="scene-conditioned-motion-generation"><a class="header" href="#scene-conditioned-motion-generation">SCENE-CONDITIONED MOTION GENERATION</a></h1>
<h2 id="scene-representation"><a class="header" href="#scene-representation">Scene representation</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>29</td><td>2024</td><td>PACER+: On-Demand Pedestrian Animation Controller in Driving Scenarios</td><td>åŸºäº2Dè½¨è¿¹æˆ–è§†é¢‘çš„è¡ŒäººåŠ¨ä½œç”Ÿæˆ</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/29.html">link</a></td></tr>
</tbody></table>
<h1 id="motion-conditioned-motion-generation"><a class="header" href="#motion-conditioned-motion-generation">Motion-Conditioned Motion Generation</a></h1>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>27</td><td></td><td>Learning Human Motion from Monocular Videos via Cross-Modal Manifold Alignment</td><td>2Dè½¨è¿¹ç”Ÿæˆ3D Motion</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/27.html">link</a></td></tr>
</tbody></table>
<h2 id="generation-pipeline"><a class="header" href="#generation-pipeline">Generation pipeline</a></h2>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2024</td><td>WANDR: Intention-guided Human Motion Generation</td><td>åŸºäºåˆå§‹ä¸ç»“æŸçŠ¶æ€æ§åˆ¶çš„åŠ¨ä½œç”Ÿæˆã€‚</td><td></td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/19.html">link</a></td></tr>
</tbody></table>
<h1 id="datasets-1"><a class="header" href="#datasets-1">Datasets</a></h1>
<div style="break-before: page; page-break-before: always;"></div><p><img src="./assets/c5094236dee05a597cc12eb2a5b13473_1_Figure_1_37170738.png" alt="" /></p>
<h1 id="äººç±»è§†é¢‘ç”Ÿæˆçš„åŸºç¡€çŸ¥"><a class="header" href="#äººç±»è§†é¢‘ç”Ÿæˆçš„åŸºç¡€çŸ¥">äººç±»è§†é¢‘ç”Ÿæˆçš„åŸºç¡€çŸ¥</a></h1>
<h1 id="å…³é”®å­ä»»åŠ¡"><a class="header" href="#å…³é”®å­ä»»åŠ¡">å…³é”®å­ä»»åŠ¡</a></h1>
<p>æ ¹æ®é©±åŠ¨ç”Ÿæˆè¿‡ç¨‹çš„æ¨¡æ€å°†ç°æœ‰æ–¹æ³•åˆ†ä¸ºä¸‰ç±»ï¼šæ–‡æœ¬é©±åŠ¨ã€éŸ³é¢‘é©±åŠ¨å’Œå§¿åŠ¿é©±åŠ¨</p>
<h2 id="æ–‡æœ¬é©±åŠ¨çš„äººç±»è§†é¢‘ç”Ÿæˆ"><a class="header" href="#æ–‡æœ¬é©±åŠ¨çš„äººç±»è§†é¢‘ç”Ÿæˆ">æ–‡æœ¬é©±åŠ¨çš„äººç±»è§†é¢‘ç”Ÿæˆ</a></h2>
<p>è®¨è®ºäº†å¦‚ä½•ä½¿ç”¨æ–‡æœ¬æè¿°æ¥æ§åˆ¶ç”Ÿæˆè§†é¢‘ä¸­çš„äººç±»å¤–è§‚å’ŒåŠ¨ä½œã€‚</p>
<p><img src="./assets/c5094236dee05a597cc12eb2a5b13473_4_Figure_3_-1248124106.png" alt="" /></p>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>1</td><td>2024</td><td>ID-Animator</td><td>To ensure the consistency of appearance in generated videos with the textual descriptions while preserving identity details during frames, ID-Animator [1] leverages a pre-trained textto-video (T2V) model with a lightweight face adapter to encode identity-relevant embeddings.</td><td>äººä½“å¤–è§‚æ§åˆ¶</td><td></td></tr>
<tr><td>2</td><td></td><td>Follow Your Pose</td><td>uses text descriptions to provide semantic information about the content of the characters, ensuring the generated videos align with the textual descriptions.</td><td>äººä½“å¤–è§‚æ§åˆ¶</td><td></td></tr>
<tr><td>83</td><td></td><td>HMTV</td><td>æ–‡æœ¬ç”ŸæˆåŠ¨ä½œå’Œç›¸æœºè¿åŠ¨ï¼Œå†ç”Ÿæˆå›¾åƒ</td><td>äººä½“åŠ¨ä½œæ§åˆ¶ï¼Œ2é˜¶æ®µæ–¹æ³•</td><td></td></tr>
<tr><td>84</td><td>2020</td><td>SignSynth</td><td>Gloss2Poseæ–‡ç”ŸåŠ¨ä½œï¼ŒGANåŠ¨ä½œç”Ÿè§†é¢‘</td><td>äººä½“åŠ¨ä½œæ§åˆ¶ï¼Œ2é˜¶æ®µæ–¹æ³•</td><td></td></tr>
<tr><td>85</td><td>2022</td><td>H-DNA</td><td></td><td>äººä½“åŠ¨ä½œæ§åˆ¶ï¼Œ2é˜¶æ®µæ–¹æ³•</td><td></td></tr>
<tr><td>86</td><td>2024</td><td>SignLLM</td><td>æ–‡æœ¬-&gt;GLoss-&gt;Pose-&gt;Video</td><td>äººä½“åŠ¨ä½œæ§åˆ¶ï¼Œ2é˜¶æ®µæ–¹æ³•</td><td></td></tr>
<tr><td>89</td><td>2024</td><td></td><td>æ–‡æœ¬-&gt;GLoss-&gt;Pose-&gt;Video</td><td>äººä½“åŠ¨ä½œæ§åˆ¶ï¼Œ2é˜¶æ®µæ–¹æ³•</td><td></td></tr>
<tr><td>53</td><td></td><td>Text2Performer</td><td>involves the motion text and a motion encoder. motion text describes the movement, such as &quot;She is swinging to the right.&quot; The model implicitly models these descriptions by separately representing appearance and motion, thereby generating high-quality videos with consistent appearance and actions.</td><td>textä½œä¸ºpromptç›´æ¥ç”Ÿæˆvideo</td><td></td></tr>
<tr><td></td><td>2024</td><td>Motion Avatar: Generate Human and Animal Avatars with Arbitrary Motion</td><td>ä½¿ç”¨äº¤äº’å¼æ–‡æœ¬ç”Ÿæˆ3DåŠ¨ä½œå’Œ3D Meshï¼Œå¹¶ç”¨3DåŠ¨ä½œé©±åŠ¨3DMesh</td><td>æ–‡æœ¬æ§åˆ¶ï¼Œ3Dé©±åŠ¨</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/39.html">link</a></td></tr>
<tr><td></td><td>2024</td><td>StoryDiffusion: Consistent Self-Attention for Long-Range Image and Video Generation</td><td>æ–‡ç”Ÿå›¾ï¼Œå›¾å¡«å……æˆè§†é¢‘</td><td>æ–‡æœ¬æ§åˆ¶ï¼Œé•¿è§†é¢‘ç”Ÿæˆ</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/41.html">link</a></td></tr>
</tbody></table>
<h2 id="éŸ³é¢‘é©±åŠ¨çš„äººç±»è§†é¢‘ç”Ÿæˆ"><a class="header" href="#éŸ³é¢‘é©±åŠ¨çš„äººç±»è§†é¢‘ç”Ÿæˆ">éŸ³é¢‘é©±åŠ¨çš„äººç±»è§†é¢‘ç”Ÿæˆ</a></h2>
<p>è¯­éŸ³é©±åŠ¨ï¼šè¦æ±‚ç”Ÿæˆçš„äººä½“åŠ¨ä½œåœ¨é«˜çº§è¯­ä¹‰æ–¹é¢åŠåœ¨æƒ…æ„Ÿå’ŒèŠ‚å¥æ–¹é¢ä¸éŸ³é¢‘å’Œè°ã€‚<br />
éŸ³ä¹é©±åŠ¨ï¼šåˆæˆä¸€ä¸ªäººåœ¨ç»™å®šçš„éŸ³ä¹ç‰‡æ®µå¼•å¯¼ä¸‹è·³èˆæˆ–æ¼”å¥æŸç§ä¹å™¨çš„è§†é¢‘ï¼Œå…³æ³¨äºä½çº§èŠ‚æ‹å¯¹é½ã€‚</p>
<p><img src="./assets/c5094236dee05a597cc12eb2a5b13473_5_Figure_4_581747205.png" alt="" /><br />
<img src="./assets/c5094236dee05a597cc12eb2a5b13473_5_Table_III_-1681426925.png" alt="" /></p>
<h2 id="å§¿åŠ¿é©±åŠ¨çš„äººç±»è§†é¢‘ç”Ÿæˆ"><a class="header" href="#å§¿åŠ¿é©±åŠ¨çš„äººç±»è§†é¢‘ç”Ÿæˆ">å§¿åŠ¿é©±åŠ¨çš„äººç±»è§†é¢‘ç”Ÿæˆ</a></h2>
<p>åŒ…æ‹¬å•æ¡ä»¶å§¿åŠ¿å¼•å¯¼æ–¹æ³•å’Œå¤šæ¡ä»¶å§¿åŠ¿å¼•å¯¼æ–¹æ³•ã€‚</p>
<p><img src="./assets/c5094236dee05a597cc12eb2a5b13473_6_Figure_5_853816112.png" alt="" /></p>
<p><img src="./assets/c5094236dee05a597cc12eb2a5b13473_7_Table_IV_-1432891952.png" alt="" /></p>
<h3 id="2dåŠ¨ä½œé©±åŠ¨"><a class="header" href="#2dåŠ¨ä½œé©±åŠ¨">2DåŠ¨ä½œé©±åŠ¨</a></h3>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>37</td><td>2024</td><td>TCAN: Animating Human Images with Temporally Consistent Pose Guidance using Diffusion Models</td><td>é€šè¿‡ä¿®æ­£attention mapå®ç°èƒŒæ™¯çš„æ—¶åºç¨³å®šæ€§</td><td>Diffusion</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/37.html">link</a></td></tr>
</tbody></table>
<h3 id="è§†é¢‘åŠ¨ä½œé©±åŠ¨"><a class="header" href="#è§†é¢‘åŠ¨ä½œé©±åŠ¨">è§†é¢‘åŠ¨ä½œé©±åŠ¨</a></h3>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>53</td><td>2024</td><td>Implicit Warping for Animation with Image Sets</td><td>ç”¨drivingè§†é¢‘ä¸­çš„äººå»é©±åŠ¨referenceå›¾åƒä¸­çš„äººï¼Œç”Ÿæˆreferenceåšä¸drivingä¸­ç›¸åŒåŠ¨ä½œçš„è§†é¢‘</td><td>äººç‰©è§†é¢‘ç”Ÿæˆï¼Œè§†é¢‘é©±åŠ¨ï¼ŒCross Attention</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/53.html">link</a></td></tr>
</tbody></table>
<h3 id="3dåŠ¨ä½œé©±åŠ¨"><a class="header" href="#3dåŠ¨ä½œé©±åŠ¨">3DåŠ¨ä½œé©±åŠ¨</a></h3>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td>42</td><td>2024</td><td>HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation</td><td>3Då»ºæ¨¡ + 3Dé‡å®šå‘ + æ¸²æŸ“ï¼ŒåŠ¨ä½œæ§åˆ¶+ç›¸æœºæ§åˆ¶</td><td>äººç‰©è§†é¢‘ç”Ÿæˆï¼Œ3Dç®¡çº¿</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/42.html">link</a></td></tr>
</tbody></table>
<h1 id="æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡"><a class="header" href="#æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡">æ•°æ®é›†å’Œè¯„ä¼°æŒ‡æ ‡</a></h1>
<h2 id="æ•°æ®é›†"><a class="header" href="#æ•°æ®é›†">æ•°æ®é›†</a></h2>
<p><img src="./assets/c5094236dee05a597cc12eb2a5b13473_3_Figure_2_-1187442696.png" alt="" /><br />
<img src="./assets/c5094236dee05a597cc12eb2a5b13473_3_Table_II_-1093823805.png" alt="" /></p>
<h2 id="è¯„ä¼°æŒ‡æ ‡"><a class="header" href="#è¯„ä¼°æŒ‡æ ‡">è¯„ä¼°æŒ‡æ ‡</a></h2>
<p><a href="./VideoDiffusionModels/EvaluationMetrics.html">link</a></p>
<h1 id="æŒ‘æˆ˜å’Œéš¾é¢˜"><a class="header" href="#æŒ‘æˆ˜å’Œéš¾é¢˜">æŒ‘æˆ˜å’Œéš¾é¢˜</a></h1>
<ul>
<li>é®æŒ¡é—®é¢˜ï¼šèº«ä½“éƒ¨ä½é‡å æˆ–å¤šäººé®æŒ¡å¾ˆå¸¸è§ï¼Œä½†å¤§å¤šæ•°æ¨¡å‹ä¸èƒ½å¾ˆå¥½åœ°å¤„ç†ç›¸äº’å½±å“çš„é—®é¢˜[98]ï¼Œ[138]ã€‚</li>
<li>Body Deformation</li>
<li>å¤–è§‚ä¸ä¸€è‡´</li>
<li>èƒŒæ™¯å½±å“</li>
<li>æ—¶åºä¸ä¸€è‡´</li>
<li>ä¸è‡ªç„¶çš„å§¿åŠ¿</li>
<li>æ–‡æœ¬é©±åŠ¨æˆ–è¯­éŸ³é©±åŠ¨ä¸­ï¼Œç”±äºæœ¬èº«æ˜¯ä¸€å¯¹å¤šé—®é¢˜ï¼Œå¯èƒ½å—é™äºæ•°æ®é›†è€Œå­˜åœ¨åå‘æ€§</li>
</ul>
<h1 id="å½±å“ç”Ÿæˆè´¨é‡çš„å› ç´ "><a class="header" href="#å½±å“ç”Ÿæˆè´¨é‡çš„å› ç´ ">å½±å“ç”Ÿæˆè´¨é‡çš„å› ç´ </a></h1>
<h2 id="ç”ŸæˆèŒƒå¼"><a class="header" href="#ç”ŸæˆèŒƒå¼">ç”ŸæˆèŒƒå¼ã€‚</a></h2>
<p>ä¸å§¿åŠ¿é©±åŠ¨æ–¹æ³•ï¼ˆå¯ä»¥è§†ä¸ºä¸€é˜¶æ®µæ–¹æ³•ï¼‰ç›¸æ¯”ï¼Œæ–‡æœ¬å’ŒéŸ³é¢‘é©±åŠ¨æ–¹æ³•å¯ä»¥åˆ†ä¸ºä¸€é˜¶æ®µå’Œä¸¤é˜¶æ®µæ–¹æ³•ã€‚å‰è€…ç›´æ¥ä½¿ç”¨è¾“å…¥æ–‡æœ¬æˆ–éŸ³é¢‘ä½œä¸ºæç¤ºæ¥æŒ‡å¯¼äººç±»è§†é¢‘ç”Ÿæˆï¼Œè€Œåè€…ä»è¾“å…¥æ–‡æœ¬æˆ–éŸ³é¢‘ç”Ÿæˆå§¿åŠ¿ï¼Œç„¶åä½¿ç”¨è¿™äº›ç”Ÿæˆçš„å§¿åŠ¿ä½œä¸ºä¿¡å·æ¥æŒ‡å¯¼äººç±»è§†é¢‘ç”Ÿæˆã€‚åœ¨ä¸¤é˜¶æ®µæ–¹æ³•ä¸­å¼•å…¥å„ç§å§¿åŠ¿ç±»å‹ï¼ˆä¾‹å¦‚éª¨æ¶å§¿åŠ¿ï¼‰æä¾›äº†é¢å¤–çš„å‡ ä½•å’Œè¯­ä¹‰ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†è§†é¢‘è¿åŠ¨çš„å‡†ç¡®æ€§å’ŒçœŸå®æ„Ÿã€‚è¿™ä½¿å¾—ä¸¤é˜¶æ®µæ–¹æ³•æ˜æ˜¾æ¯”ä¸€é˜¶æ®µæ–¹æ³•æ›´æœ‰æ•ˆï¼Œå°½ç®¡ä¼šç‰ºç‰²ä¸€äº›æ•ˆç‡ã€‚</p>
<h2 id="backbone"><a class="header" href="#backbone">backbone</a></h2>
<p>SD å’Œ SVD ç­‰æ‰©æ•£æ¨¡å‹å› å…¶å“è¶Šçš„æ€§èƒ½å’Œå¤šæ ·æ€§è€Œå¹¿æ³›åº”ç”¨äºå„ç§ç”Ÿæˆä»»åŠ¡ï¼ŒåŒ…æ‹¬äººç±»è§†é¢‘ç”Ÿæˆã€‚ç„¶è€Œï¼Œä¸åœ¨å•ä¸ªé‡‡æ ·æ­¥éª¤ä¸­ç”Ÿæˆæ ·æœ¬çš„ GAN ä¸åŒï¼Œæ‰©æ•£æ¨¡å‹éœ€è¦å¤šä¸ªé‡‡æ ·æ­¥éª¤ï¼Œä»è€Œå¢åŠ äº†è®­ç»ƒå’Œæ¨ç†çš„æ—¶é—´æˆæœ¬ã€‚</p>
<h2 id="poseæ§åˆ¶ä¿¡å·"><a class="header" href="#poseæ§åˆ¶ä¿¡å·">poseæ§åˆ¶ä¿¡å·</a></h2>
<p>ä¸åŒç±»å‹çš„æ¡ä»¶å§¿åŠ¿ä¹‹æ‰€ä»¥æœ‰æ•ˆï¼Œæ˜¯å› ä¸ºå®ƒä»¬æä¾›äº†è¡¥å……ä¿¡æ¯ã€‚</p>
<ul>
<li>éª¨éª¼å§¿åŠ¿å‡†ç¡®åœ°æè¿°äº†å¸§ä¸­äººä½“çš„ç©ºé—´ä¿¡æ¯ä»¥åŠèº«ä½“éƒ¨ä½çš„ç›¸å¯¹ä½ç½®ã€‚ç„¶è€Œï¼Œå®ƒæ•è·ç¦»æ•£çš„å§¿åŠ¿å˜åŒ–è€Œä¸æ˜¯è¿ç»­çš„è¿åŠ¨ç»†èŠ‚ï¼Œæä¾›æœ‰é™çš„æ—¶é—´è¿è´¯æ€§ã€‚</li>
<li>å…‰æµæœ¬è´¨ä¸ŠåŒ…æ‹¬æ—¶é—´ä¿¡æ¯ï¼Œæ•è·è¿ç»­å¸§ä¹‹é—´çš„å˜åŒ–å¹¶æä¾›ç‰¹å¾ç©ºé—´ä¸­çš„è¿ç»­è¿åŠ¨è½¨è¿¹ã€‚è¿™ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆå¸§ä¹‹é—´å¹³æ»‘è¿‡æ¸¡çš„è§†é¢‘ï¼Œé¿å…è·³è·ƒæˆ–ä¸è¿ç»­ã€‚</li>
<li>æ·±åº¦åœ°å›¾æ•æ‰äººä½“ä¸èƒŒæ™¯ä¹‹é—´çš„è·ç¦»ä¿¡æ¯ï¼Œä»¥åŠè¡¨é¢ç»†èŠ‚å’Œæ·±åº¦å˜åŒ–ã€‚</li>
<li>3D ç½‘æ ¼æä¾›äº†éª¨éª¼å§¿åŠ¿æ‰€ç¼ºä¹çš„ç‰©ä½“è¡¨é¢çš„è¯¦ç»†å‡ ä½•ç»“æ„ã€‚</li>
</ul>
<p>æ€»ä¹‹ï¼Œä¸åŒç±»å‹çš„å§¿åŠ¿æä¾›äº’è¡¥çš„æ—¶ç©ºä¿¡æ¯ï¼Œå¹¶ä¸”ä¸å­˜åœ¨æ»¡è¶³æ‰€æœ‰è¦æ±‚çš„ç»Ÿä¸€å§¿åŠ¿ç±»å‹ã€‚ä¸åŒçš„åœºæ™¯å’Œé—®é¢˜å¯èƒ½éœ€è¦ä¸åŒçš„å§¿åŠ¿ã€‚</p>
<h1 id="æœªæ¥ç ”ç©¶æ–¹å‘"><a class="header" href="#æœªæ¥ç ”ç©¶æ–¹å‘">æœªæ¥ç ”ç©¶æ–¹å‘</a></h1>
<ul>
<li>å¤§è§„æ¨¡é«˜è´¨é‡äººç±»è§†é¢‘æ•°æ®é›†</li>
<li>é•¿è§†é¢‘ç”Ÿæˆ</li>
<li>é«˜ä¿çœŸè§†é¢‘ç”Ÿæˆ</li>
<li>æé«˜äººç±»è§†é¢‘æ‰©æ•£æ¨¡å‹çš„æ•ˆç‡</li>
<li>ç»†ç²’åº¦å¯æ§æ€§</li>
<li>äº¤äº’æ€§ã€‚</li>
</ul>
<h1 id="reference-5"><a class="header" href="#reference-5">Reference</a></h1>
<ol>
<li>https://arxiv.org/pdf/2407.08428</li>
<li>https://github.com/wentaoL86/Awesome-Human-Video-Generation</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2024</td><td>PKU-DyMVHumans: A Multi-View Video Benchmark for High-Fidelity Dynamic Human Modeling</td><td>ä¸€ä¸ªä»¥äººä¸ºä¸­å¿ƒçš„å¤šåŠŸèƒ½æ•°æ®é›†ï¼Œç”¨äºä»å¯†é›†çš„å¤šè§†å›¾è§†é¢‘ä¸­é«˜ä¿çœŸé‡å»ºå’Œæ¸²æŸ“åŠ¨æ€äººç±»åœºæ™¯ã€‚è¶…è¿‡ 56 ä¸ªåŒæ­¥æ‘„åƒæœºï¼Œ 45 ä¸ªä¸åŒåœºæ™¯ï¼Œ 32 ä¸åŒçš„äººï¼Œ820ä¸‡å¸§ã€‚æ¯å¸§éƒ½æœ‰é«˜åº¦è¯¦ç»†çš„å¤–è§‚å’Œé€¼çœŸçš„äººä½“åŠ¨ä½œ</td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>BEDLAM: A Synthetic Dataset of Bodies Exhibiting Detailed Lifelike Animated Motion</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>CIRCLE: Capture In Rich Contextual Environments</td><td>å…·æœ‰ç›®æ ‡å¯¼å‘è¿åŠ¨çš„æ•°æ®é›†</td><td></td><td></td></tr>
<tr><td></td><td>2022</td><td>Artemis: Articulated Neural Pets with Appearance and Motion Synthesis</td><td>åŠ¨æ€æ¯›èŒ¸èŒ¸åŠ¨ç‰©ï¼ˆDFAï¼‰æ•°æ®é›†ï¼š<br> - æ¥è‡ªè‰ºæœ¯å®¶çš„å»ºæ¨¡ã€‚<br> - å«ä¹ç§é«˜è´¨é‡çš„ CGI åŠ¨ç‰©ï¼ŒåŒ…æ‹¬ç†ŠçŒ«ã€ç‹®å­ã€çŒ«ç­‰ã€‚<br> - å®ƒä»¬å…·æœ‰åŸºäºçº¤ç»´/çº¿çš„æ¯›çš®å’Œéª¨éª¼ <br> - ä½¿ç”¨å•†ä¸šæ¸²æŸ“å¼•æ“ï¼ˆä¾‹å¦‚ MAYAï¼‰å°†æ‰€æœ‰è¿™äº› CGI åŠ¨ç‰©è§’è‰²æ¸²æŸ“æˆå„ç§ä»£è¡¨æ€§éª¨éª¼è¿åŠ¨ä¸‹çš„é«˜è´¨é‡å¤šè§†å›¾ 1080 Ã— 1080 RGBA è§†é¢‘ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬é‡‡ç”¨äº† 36 ä¸ªæ‘„åƒæœºè§†å›¾ï¼Œè¿™äº›æ‘„åƒæœºè§†å›¾å‡åŒ€åœ°å›´ç»•æ•è·çš„åŠ¨ç‰©æ’åˆ—æˆä¸€ä¸ªåœ†åœˆï¼Œæ¯ä¸ªåŠ¨ç‰©çš„ä»£è¡¨æ€§å§¿åŠ¿æ•°é‡ä» 700 åˆ° 1000 ä¸ªä¸ç­‰ã€‚</td><td>å››è¶³åŠ¨ç‰©</td><td><a href="https://caterpillarstudygroup.github.io/ReadPapers/32.html">è®ºæ–‡</a>ï¼Œ<a href="https://caterpillarstudygroup.github.io/ReadPapers/33.html">æ•°æ®é›†</a></td></tr>
<tr><td></td><td>2019</td><td>AMASS: Archive of Motion Capture as Surface Shapes</td><td>AMASSæ•°æ®é›†æ„æˆäº†ä¸€ä¸ªå…¨é¢ä¸”å¤šæ ·åŒ–çš„äººä½“è¿åŠ¨æ•°æ®é›†ï¼ŒåŒ…å«æ¥è‡ª300åå—è¯•è€…çš„11,000å¤šä¸ªåŠ¨ä½œï¼Œæ€»è®¡è¶…è¿‡40ä¸ªå°æ—¶ã€‚<br> è¿åŠ¨æ•°æ®ä»¥åŠç”¨äºéª¨æ¶å’Œç½‘æ ¼è¡¨ç¤ºçš„ SMPL å‚æ•°æºè‡ªåˆ©ç”¨ 15 ä¸ªå…‰å­¦æ ‡è®°çš„åŸºäºæ ‡è®°çš„ MoCap ç³»ç»Ÿã€‚</td><td></td><td></td></tr>
<tr><td></td><td>2019</td><td>iMapper</td><td>i3DB [69] contains RGB videos of person-scene interactions involving medium to heavy occlusions. It provides annotated 3D joint positions and a primitive 3D scene reconstruction.</td><td></td><td></td></tr>
<tr><td></td><td>2019</td><td>Resolving 3D Human Pose Ambiguities With 3D Scene Constraints</td><td>PROX [34] contains RGB-D videos of people interacting with indoor environments.</td><td></td><td></td></tr>
<tr><td></td><td>2018</td><td>Recovering Accurate 3D Human Pose in the Wild Using IMUs and a Moving Camera</td><td><strong>3DPW æ•°æ®é›†</strong>æ•è· 51,000 ä¸ªå•è§†å›¾int the wildè§†é¢‘åºåˆ—ï¼Œå¹¶ç”± IMU æ•°æ®è¡¥å……ã€‚ è¿™äº›è§†é¢‘æ˜¯ä½¿ç”¨æ‰‹æŒå¼æ‘„åƒæœºå½•åˆ¶çš„ï¼ŒIMU æ•°æ®æœ‰åŠ©äºå°† 2D å§¿åŠ¿ä¸å…¶ 3D å¯¹åº”å§¿åŠ¿å…³è”èµ·æ¥ã€‚ 3DPW æ˜¯æœ€å¼ºå¤§çš„æ•°æ®é›†ä¹‹ä¸€ï¼Œå°†è‡ªèº«ç¡®ç«‹ä¸ºè¿‘æœŸå¤šäººé‡å¤–åœºæ™¯ä¸­ 3D å§¿æ€ä¼°è®¡çš„åŸºå‡†ã€‚</td><td></td><td></td></tr>
<tr><td></td><td>2014</td><td>Human3.6M: Large Scale Datasets and Predictive Methods for 3D Human Sensing in Natural Environments</td><td>ä½¿ç”¨ RGB å’Œ ToF ç›¸æœºä»ç°å®ä¸–ç•Œç¯å¢ƒä¸­çš„ä¸åŒè§†è§’æ•è·çš„ 360 ä¸‡ä¸ªå§¿åŠ¿çš„å¤§é‡é›†åˆã€‚ <br>èº«ä½“ç½‘æ ¼çš„é«˜åˆ†è¾¨ç‡ 3D æ‰«æä»ªæ•°æ®ã€‚</td><td></td><td></td></tr>
<tr><td>225</td><td></td><td>MPI-INF-3DPH</td><td>è¶…è¿‡ 2K çš„è§†é¢‘ï¼Œå…·æœ‰æˆ·å¤–åœºæ™¯ä¸­ 13 ä¸ªå…³é”®ç‚¹çš„è”åˆæ³¨é‡Šï¼Œé€‚ç”¨äº 2D å’Œ 3D äººä½“å§¿åŠ¿ä¼°è®¡ã€‚<br> GTæ˜¯é€šè¿‡å¤šæ‘„åƒå¤´å¸ƒç½®å’Œæ— æ ‡è®°åŠ¨æ•ç³»ç»Ÿè·å¾—çš„ï¼Œè¿™ä»£è¡¨äº†ä¸æ¶‰åŠçœŸå®ä¸ªä½“çš„ä¼ ç»ŸåŸºäºæ ‡è®°çš„åŠ¨æ•ç³»ç»Ÿçš„è½¬å˜ã€‚</td><td></td><td></td></tr>
<tr><td>226</td><td></td><td>HumanEva dataset</td><td>å¤šè§†å›¾ 3D äººä½“å§¿æ€ä¼°è®¡æ•°æ®é›†ã€‚åŒ…æ‹¬ä¸¤ä¸ªç‰ˆæœ¬ï¼šHumanEva-I å’Œ HumanEva-IIã€‚ <br> åœ¨ HumanEva-I ä¸­ï¼Œæ•°æ®é›†åŒ…æ‹¬ä»ä½äºå‰ã€å·¦ã€å³ (RGB) å’Œå››ä¸ªè§’ (Mono) çš„ä¸ƒä¸ªæ‘„åƒå¤´æ•è·çš„çº¦ 40,000 ä¸ªå¤šè§†å›¾è§†é¢‘å¸§ã€‚ <br>HumanEva-II å…·æœ‰å¤§çº¦ 2,460 å¸§ï¼Œç”±æ¯ä¸ªè§’è½çš„å››ä¸ªæ‘„åƒæœºè®°å½•ã€‚</td><td></td><td></td></tr>
<tr><td>227,248</td><td></td><td>CMU-Panoptic dataset</td><td>65 ä¸ªå¸§åºåˆ—ï¼Œå¤§çº¦ 5.5 å°æ—¶çš„é•œå¤´ï¼Œå¹¶å…·æœ‰ 150 ä¸‡ä¸ª 3D å¸¦æ³¨é‡Šçš„å§¿åŠ¿ã€‚<br> è¯¥æ•°æ®é›†é€šè¿‡é…å¤‡ 511 ä¸ªæ ¡å‡†ç›¸æœºå’Œ 10 ä¸ªå…·æœ‰åŸºäºç¡¬ä»¶åŒæ­¥åŠŸèƒ½çš„ RGB-D ä¼ æ„Ÿå™¨çš„å¤§å‹å¤šè§†å›¾ç³»ç»Ÿè®°å½•ï¼Œå¯¹äºé€šè¿‡å¤šè§†å›¾å‡ ä½•å¼€å‘å¼±ç›‘ç£æ–¹æ³•è‡³å…³é‡è¦ã€‚ è¿™äº›æ–¹æ³•è§£å†³äº†ä¼ ç»Ÿè®¡ç®—æœºè§†è§‰æŠ€æœ¯ä¸­å¸¸è§çš„é®æŒ¡é—®é¢˜ã€‚</td><td></td><td></td></tr>
<tr><td>115</td><td></td><td>Multiperson Composited 3D Human Pose (MuCo-3DHP) dataset</td><td>ç”¨ä½œ 3D äººä½“å§¿æ€ä¼°è®¡çš„å¤§è§„æ¨¡å¤šäººé®æŒ¡è®­ç»ƒé›†ã€‚<br> MuCo-3DHP ä¸­çš„å¸§æ˜¯é€šè¿‡åˆæˆå’Œå¢å¼ºæ–¹æ¡ˆä» MPI-INF-3DPH æ•°æ®é›†ç”Ÿæˆçš„ã€‚</td><td></td><td></td></tr>
<tr><td>SURREAL dataset [228] is a large synthetic human body dataset containing 6 million RGB video frames. It provides a range of accurate annotations, including depth, body parts, optical flow, 2D/3D poses, and surfaces. In the SURREAL dataset, images exhibit variations in texture, view, and pose, and the body models are based on the SMPL parameters, a widely-recognized mesh representation standard.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>3DOH50K dataset [150] offers a collection of 51,600 images obtained from six distinct viewpoints in real-world settings, predominantly featuring object oc- clusions. Each image is annotated with ground truth 2D and 3D poses, SMPL parameters, and a segmentation mask. Utilized for training human estimation and reconstruction models, the 3DOH50K dataset facilitates exceptional per- formance in occlusion scenarios.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>3DCP dataset [229] represents a 3D human mesh dataset, derived from AMASS [230]. It includes 190 self-contact meshes spanning six human subjects (three males and three females), each modeled with an SMPL-X parameterized template.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>DensePose dataset [231] features 50,000 manually annotated real images, comprising 5 million image-to-surface correspondence pairs extracted from the COCO [249] dataset. This dataset proves instrumental for training in dense human pose estimation, as well as in detection and segmentation tasks.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>UP-3D dataset [232] is a dedicated 3D human pose and shape estima- tion dataset featuring extensive annotations in sports scenarios. The UP-3D comprises approximately 8,000 images from the LSP and MPII datasets. Addi- tionally, each image in UP-3D is accompanied by a metadata file indicating the quality (medium or high) of the 3D fit.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td>THuman dataset [233] constitutes a 3D real-world human mesh dataset. It includes 7,000 RGBD images, each featuring a textured surface mesh obtained using a Kinect camera. Including surface mesh with detailed texture and the aligned SMPL model is anticipated to significantly enhance and stimulate future research in human mesh reconstruction.</td><td></td><td></td><td></td><td></td><td></td></tr>
<tr><td></td><td></td><td></td><td></td><td></td><td></td></tr>
</tbody></table>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="theme/pagetoc.js"></script>
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
    </body>
</html>
